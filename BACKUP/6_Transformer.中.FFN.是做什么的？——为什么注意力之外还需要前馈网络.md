# [Transformer 中 FFN 是做什么的？——为什么注意力之外还需要前馈网络](https://github.com/lihe/MyGitBlog/issues/6)





在 Transformer 架构中，自注意力机制（Self-Attention）往往更容易被关注，但真正决定模型表达能力上限的核心模块之一，是每一层中与注意力并列存在的前馈神经网络（Feed-Forward Network, FFN）。FFN 并非简单的附属组件，而是 Transformer 能够建模复杂语义与非线性关系的关键所在。

<img width="1225" height="384" alt="Image" src="https://github.com/user-attachments/assets/ebfb108f-344b-4773-977e-d0045407e3be" />

------





## **注意力与 FFN 的职责分工**





从信息流的角度来看，Transformer 的每一层可以拆分为两类互补的计算过程：



- **自注意力机制**

  负责在不同 token 之间建立依赖关系，对上下文信息进行加权聚合，解决“从哪里获取信息”的问题。

- **前馈网络（FFN）**

  负责在单个 token 内部对已聚合的信息进行非线性变换和特征重组，解决“如何理解和加工信息”的问题。





两者形成了清晰的分工：注意力负责信息路由，FFN 负责信息加工。



------





## **FFN 的标准结构**





在经典 Transformer 中，FFN 是一个逐位置（position-wise）的两层全连接网络，其基本形式为：



$\text{FFN}(x) = W_2 \, \sigma(W_1 x + b_1) + b_2$



其中：



- $x \in \mathbb{R}^{d_{\text{model}}}$ 是单个 token 的表示；
- $W_1$ 将特征维度从 $d_{\text{model}} $升维到 $d_{\text{ff}}$；
- $\sigma$ 是非线性激活函数（ReLU、GELU、SwiGLU 等）；
- $W_2$ 再将特征映射回模型维度。





这一网络对序列中每个位置独立作用，参数在所有位置共享，不引入 token 间的交互。



------





## **升维—非线性—降维的设计动机**





FFN 中“先升维、再降维”的结构并非偶然，而是 Transformer 表达能力的核心来源。



1. **扩展表示空间**

   中间层维度 $d_{\text{ff}}$ 通常是 $d_{\text{model}} $的 4 倍左右。高维空间提供了更强的函数拟合能力，使模型能够表示复杂的特征组合。

2. **引入非线性建模能力**

   自注意力本质上是线性加权求和，只能重组已有特征，无法产生新的语义结构。FFN 通过非线性激活，使模型能够学习条件关系和高阶特征。

3. **特征解耦与重组**

   升维后的空间允许不同语义子特征被分离、激活和重新组合，随后再压缩为更抽象、更有判别力的表示。





从这一角度看，FFN 更接近 Transformer 中真正的“计算单元”，而注意力更多承担结构性信息组织的角色。



------





## **FFN 对语义理解的作用**





在经过注意力计算后，每个 token 的表示已经融合了上下文信息，但这些信息仍然是线性叠加的结果。FFN 在此基础上完成语义层面的再解释。



例如，在上下文信息已被聚合后，FFN 能够根据特征组合激活特定语义模式，将模糊的上下文混合表示映射为更明确的高层语义状态。这种能力并非来自 token 间交互，而是来自对单个 token 表示的深度非线性变换。



------





## **为什么 FFN 采用逐位置设计**





FFN 以逐位置方式作用，原因在于 Transformer 的结构分层原则：



- token 之间的依赖关系已经由自注意力显式建模；
- FFN 专注于 token 内部的语义加工；
- 职责分离使模型结构更清晰、训练更稳定。





这种设计也使 FFN 在计算上高度并行，适合大规模模型训练。



------





## **缺失 FFN 的影响**





如果 Transformer 中仅保留注意力而移除 FFN，那么整个网络将退化为线性映射的堆叠。即便层数增加，模型仍无法表示复杂的非线性函数，表达能力会受到根本性限制。



在实际的大语言模型中，FFN 不仅承担主要的非线性建模任务，其参数量和计算量也通常占据模型总量的绝大部分。这一事实从工程层面进一步印证了 FFN 在 Transformer 中的核心地位。



------





## **FFN 的结构演进**





随着模型规模和任务复杂度的提升，FFN 的形式也在不断演化：



- **GELU FFN**：常见于 BERT、早期 GPT 系列
- **SwiGLU / GEGLU**：引入门控机制，提高表达效率（LLaMA、PaLM、T5）
- **MoE-FFN**：通过专家路由进一步提升模型容量（Mixtral、DeepSeek-MoE）





尽管实现形式不同，这些变体仍遵循同一基本思想：在高维空间中进行非线性特征加工，再映射回模型表示空间。



------





## **总结**





在 Transformer 架构中，前馈神经网络承担着对每个 token 表示进行深度语义加工的职责。自注意力机制负责跨 token 的信息聚合，而 FFN 通过升维、非线性激活与降维，使模型具备建模复杂语义关系的能力。



正是这种“注意力负责结构，FFN 负责计算”的分工，使 Transformer 成为一种既具全局建模能力、又具强表达能力的通用序列建模架构。
