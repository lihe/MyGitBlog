# [Transformer 在哪里做了权重共享？](https://github.com/lihe/MyGitBlog/issues/10)





权重共享（Weight Sharing）是 Transformer 架构中一个**看似基础、实则非常关键**的设计选择。它直接关系到模型的参数效率、泛化能力以及对任意长度序列的支持能力。



不同于“层与层之间共享参数”的直觉理解，Transformer 的权重共享主要发生在**位置维度和词表维度**，而不是深度维度。理解这一点，是理解 Transformer 架构设计哲学的重要一步。



------





## **一、什么是权重共享**





权重共享指的是：**在模型的不同计算位置或不同使用场景中复用同一组参数**，而不是为每个位置或情况单独学习一套权重。



其核心目的包括：



- 减少参数规模
- 强化模型的泛化能力
- 引入结构上的不变性假设





在 Transformer 中，权重共享并非“随意为之”，而是与序列建模的本质高度契合。



------





## **二、Transformer 中最核心的权重共享位置**







### **1. 前馈网络（FFN）在所有 token 位置之间共享参数**





在每一层 Transformer 中，前馈网络的形式为：



$\text{FFN}(x_i) = W_2 \,\sigma(W_1 x_i + b_1) + b_2$



关键点在于：



- 对序列中**每一个 token**，都使用**同一组** $W_1, W_2$
- 参数不随 token 在序列中的位置而变化





这是一种**位置级共享（position-wise sharing）**，其效果与 CNN 中“卷积核在空间位置上共享”高度类似。



这一设计保证了：



- 同一语义变换规则可作用于任意位置
- 模型不依赖固定序列长度
- 参数规模与序列长度无关





这是 Transformer 能够处理任意长度输入的基础之一。



------





### **2. 自注意力中的 Q / K / V 投影矩阵在所有位置共享**





在自注意力中，输入序列表示为 X，并通过线性变换得到：



$Q = XW_Q,\quad K = XW_K,\quad V = XW_V$



其中：



- $W_Q, W_K, W_V$ 在**所有 token 位置之间完全共享**
- 不同 token 的差异，来源于输入表示 X，而不是参数本身





这意味着：



- “如何将一个 token 映射到查询 / 键 / 值空间” 是一种**位置无关的通用语义规则**





这种共享保证了模型对序列中不同位置的 token 采取一致的语义处理方式。



------





### **3. 多头注意力中：位置共享，头之间不共享**





在多头注意力机制中：



- 每一个 attention head 都有自己独立的 $W_Q^h, W_K^h, W_V^h$
- **同一个 head 内部**，这些参数在所有 token 位置上共享
- **不同 head 之间**，参数不共享





因此可以总结为：



- 位置维度：共享
- head 维度：不共享





这种设计使得不同 head 可以学习不同的关系子空间，而每个子空间在序列位置上保持一致的变换规则。



------





### **4. 层内共享，层间不共享**





在标准 Transformer 中：



- 同一层内部，Attention 和 FFN 的参数对所有 token 共享
- 不同层之间，参数完全独立





每一层可以被理解为学习**不同层级的语义抽象**，因此不进行层间共享。



需要注意的是，只有在特殊模型（如 ALBERT）中，才会引入层间共享作为额外的参数压缩手段。



------





### **5. 输入 Embedding 与输出 Softmax 权重的共享（常见实践）**





在许多语言模型实现中（如 GPT、BERT）：



- 输入词嵌入矩阵
- 输出层 softmax 前的线性映射矩阵





会进行权重共享（或转置共享）：



$W_{\text{out}} = W_{\text{embed}}^\top$



这种做法的优势包括：



- 显著减少参数量
- 保证输入与输出使用同一语义空间
- 提升泛化能力和训练稳定性





这是一个非常常见但并非强制的工程选择。



------





## **三、为什么 Transformer 必须这样共享权重？**







### **1. 序列建模中的“平移不变性”假设**





在语言中：



> 一个词出现在句首或句尾，其基本语义处理规则应当一致。



通过在位置维度上共享参数，模型将“位置相关性”与“语义变换规则”解耦：



- 语义规则 → 由共享参数学习
- 位置信息 → 由位置编码提供





------





### **2. 参数效率与泛化能力**





如果不做共享：



- 参数量会随序列长度线性或指数增长
- 模型更容易过拟合位置模式
- 泛化到更长序列将变得不可行





共享参数使模型学到的是**可复用的语言结构规律**，而不是对某些固定位置的记忆。



------





### **3. 支持任意长度输入**





Transformer 能够在推理时处理比训练阶段更长的序列，其前提是：



- 参数不依赖于具体的绝对位置索引





位置级权重共享正是这一能力的结构保障。



------





## **四、哪些地方没有做权重共享**





为了避免误解，有必要明确：



- Transformer **不在不同层之间共享参数**
- 不同 attention head 之间也不共享参数
- 模型深度维度上的多样性是有意保留的





这种“共享与不共享的边界”，正是 Transformer 表达能力与稳定性之间的重要平衡。



------





## **五、权重共享位置一览**



| **维度**              | **是否共享** |
| --------------------- | ------------ |
| 不同 token 位置       | ✅ 共享       |
| 同一层内部            | ✅ 共享       |
| 不同层之间            | ❌ 不共享     |
| 不同 attention head   | ❌ 不共享     |
| 输入 / 输出 embedding | ✅（常见）    |



------





## **总结**





Transformer 的权重共享并不 hookup 在“层”这个维度上，而是集中体现在**位置维度和词表维度**。

通过在所有 token 位置上共享 Attention 和 FFN 的参数，Transformer 实现了类似卷积的平移不变性，并得以支持任意长度序列建模；而通过共享输入与输出 embedding，则进一步提升了参数效率与语义一致性。



这种共享策略并非工程折中，而是由序列建模与注意力机制的本质共同决定的结构选择。




