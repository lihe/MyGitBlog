# [从 SGD 到 Adam：深度学习优化算法的原理与工程实践](https://github.com/lihe/MyGitBlog/issues/16)



在科研与工程中，**大量问题都可以抽象为一个目标函数的最小化问题**。

深度学习也不例外——模型训练的本质，就是在高维参数空间中寻找一个使损失函数最小的点。



正如吴恩达老师常用的比喻：



> 梯度下降（Gradient Descent）就像一个人站在高山上，每一步都沿着当前最陡的方向，尽可能快地跑向山谷的最低点。



围绕这个直觉，过去十多年里，优化算法经历了一条清晰的演进路线：

**SGD → Momentum → AdaGrad / RMSProp → Adam → AdamW**



------





## **一、随机梯度下降（SGD）：一切的起点**







### **1. 基本思想**





设参数为 $\theta$，第 t 步的梯度为 $g_t = \nabla_\theta L(\theta_t)$，

SGD 的更新规则是：



$\theta_{t+1} = \theta_t - \alpha g_t$



其中 $\alpha$ 是学习率。



SGD 的优势在于：



- 形式简单
- 计算与内存开销低
- 在大规模数据上可扩展







### **2. Mini-batch SGD**





实际训练中通常采用 mini-batch：



- 用一小批样本估计梯度
- 在噪声中前进，反而有助于跳出局部极小值





但 **SGD 也有明显问题**：



1. 学习率难以选择
2. 所有参数共享同一个学习率
3. 梯度噪声大、收敛慢
4. 在高度非凸问题中易震荡或卡在鞍点附近





------





## **二、Momentum：给梯度“加惯性”**







### **1. 核心思想**





Momentum 引入“速度”的概念，让更新方向具有惯性：



$v_t = \mu v_{t-1} + g_t$

$\theta_{t+1} = \theta_t - \alpha v_t$



直觉上：



- 梯度方向一致 → 加速前进
- 梯度来回震荡 → 自动平滑





Momentum 能显著提升 SGD 的收敛速度和稳定性。



------





## **三、自适应学习率的尝试：AdaGrad 与 RMSProp**







### **1. AdaGrad：为每个参数分配学习率**





AdaGrad 的关键思想是：



> **频繁更新的参数，步子走小一点；**

> **稀疏更新的参数，步子走大一点。**



更新规则（按元素）：



$\theta_{t+1,i} = \theta_{t,i} - \frac{\alpha}{\sqrt{\sum_{k=1}^t g_{k,i}^2 + \epsilon}} g_{t,i}$



**优点**



- 对稀疏特征非常友好（如 NLP）





**致命缺点**



- 梯度平方不断累积
- 学习率单调衰减，后期几乎停止学习





------





### **2. RMSProp：修复 AdaGrad 的“学习率过早衰减”**





RMSProp 用 **指数移动平均** 替代累计和：



$v_t = \gamma v_{t-1} + (1-\gamma) g_t^2$



$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{v_t} + \epsilon} g_t$



**优势**



- 能适应非平稳目标函数
- 在实践中比 AdaGrad 稳定得多





RMSProp 是 Adam 的直接前身。



------





## **四、Adam：Momentum + RMSProp 的结合**







### **1. 一句话定义**





**Adam 是一种同时利用梯度的一阶矩（均值）和二阶矩（方差）进行自适应更新的优化器。**



------





### **2. 核心思想**





Adam 同时解决两个问题：



1. 梯度方向是否稳定？ → Momentum
2. 不同参数尺度差异大怎么办？ → 自适应学习率





------





### **3. 数学形式**





设第 t 步梯度为 $g_t$



**一阶矩（均值）**

$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$



**二阶矩（方差）**

$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$



由于初始化为 0，需要偏置校正：



$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$



最终更新：



$\theta_{t+1} = \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$



------





### **4. 直觉理解**





- $m_t$：最近梯度的大方向

- $v_t$：这个方向抖不抖

- 更新规则：

  

  - 方向稳定 → 步子大
  - 波动剧烈 → 步子小

  





------





## **五、为什么 Adam 如此流行？**





**工程优势非常明显：**



- 收敛快
- 对学习率不敏感
- 适合稀疏梯度
- 训练初期非常稳定





因此在实践中：



> **Adam 是深度学习的默认优化器。**



------





## **六、Adam 的问题：为什么还不够？**







### **1. 泛化性能问题**





多项研究发现：



- Adam 更容易收敛到 sharp minima
- 在一些视觉 / 结构化任务上泛化不如 SGD + Momentum







### **2. 权重衰减实现不合理（关键问题）**





原始 Adam 将 L2 正则直接加到梯度上：



$g_t \leftarrow g_t + \lambda \theta$



这会与 Adam 的自适应缩放发生耦合，**破坏优化行为**。



------





## **七、AdamW：工程上的事实标准**





AdamW 将 **权重衰减与梯度更新解耦**：



$\theta \leftarrow \theta - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t}} - \alpha \lambda \theta$



优势：



- 更清晰的超参数语义
- 更好的泛化表现





 **现代大模型几乎清一色使用 AdamW。**



------





## **八、常见超参数**





默认值（可以直接说）：



- learning rate：1e-3（大模型常用 1e-4 / 1e-5）
- $\beta_1$ = 0.9
- $\beta_2$ = 0.999
- $\epsilon$ = 1e-8





工程经验：



- 大模型中常适当降低 $\beta_2$ 提升响应速度





------





## **九、一句话总结**





Adam 是一种通过估计梯度一阶矩和二阶矩来自适应调整学习率的优化器，结合了 Momentum 和 RMSProp 的优点，具有收敛快、稳定性好的特点。在实际工程中通常使用 AdamW，以正确实现权重衰减并获得更好的泛化性能。

