# [什么是 KV Cache？](https://github.com/lihe/MyGitBlog/issues/11)





在大语言模型的推理阶段，**KV Cache（Key-Value Cache）**几乎是不可或缺的基础机制。无论是 GPT、LLaMA、Qwen，还是各类推理框架（Transformers、vLLM、TensorRT-LLM），KV Cache 都直接决定了生成速度、显存占用以及系统吞吐能力。



它并不是一个“模型结构创新”，而是一个**严格由自回归推理范式推导出来的工程必然**。



------





## **一、KV Cache 要解决的根本问题**





在 Transformer Decoder 或 GPT 类模型中，文本生成采用**自回归（autoregressive）**方式：



- 每一步生成一个新 token
- 新 token 需要对 **所有历史 token** 做一次注意力计算





在第 t 步生成时，注意力形式为：



$\text{Attention}(Q_t, K_{1:t}, V_{1:t})$



如果不做任何缓存，那么在第 t 步：



- 前 t-1 个 token 的 Key / Value
- 会被**重复计算一次**





随着生成长度增加，这种重复会迅速成为性能瓶颈。



------





## **二、KV Cache 的核心思想**





KV Cache 的思想非常直接：



> **历史 token 的表示一旦算过，就永远不再变化，不需要重复计算。**



在注意力中：



- **Query（Q）**：来自当前 token，每一步都会变化
- **Key（K）、Value（V）**：来自历史 token，一旦生成就固定





因此：



- 只缓存 **K / V**
- 不缓存 **Q**





这就是 KV Cache 的全部动机。



------





## **三、KV Cache 的具体工作机制**







### **1. 首次计算（Prefill 阶段）**





当输入 prompt 时：



- 对 prompt 中所有 token
- 在每一层 Transformer 中
- 计算完整的 Q / K / V





随后：



- 将所有层、所有 head 的 **K / V 存入缓存**
- 这一步计算量大，属于 **compute-bound**





------





### **2. 自回归生成（Decode 阶段）**





从生成第一个新 token 开始，每一步：



- 只对 **新 token** 计算：

  

  - $Q_{\text{new}}$
  - $K_{\text{new}}$
  - $V_{\text{new}}$

  

- 从缓存中取出：

  

  - $K_{\text{old}}$
  - $V_{\text{old}}$

  





并进行注意力计算：



$\text{Attention}\bigl( Q_{\text{new}}, [K_{\text{old}}, K_{\text{new}}], [V_{\text{old}}, V_{\text{new}}] \bigr)$



随后：



- 将 $K_{\text{new}}, V_{\text{new}}$ 追加到 KV Cache





从这一刻起，**历史 token 的 K / V 永远只计算一次**。



------





## **四、为什么 KV Cache 能显著加速推理**





如果不使用 KV Cache：



- 第 t 步需要重新计算 $1 \sim t-1$ 的 K / V
- 总体计算复杂度接近 $O(t^2)$





使用 KV Cache 后：



- 历史部分不再重复计算
- 每一步只新增常数级计算





整体复杂度变为：



- **Prefill：一次性高开销**
- **Decode：近似** O(t)





这在长文本生成（几千 token）时，会带来**数量级的速度差异**。



------





## **五、KV Cache 的真实代价：显存**





KV Cache 的本质是“用空间换时间”。



缓存内容包括：



- 每一层
- 每一个 attention head
- 每一个历史 token
- 的 Key 和 Value





显存占用规模近似为：



$\text{Layers} \times \text{Heads} \times \text{SeqLen} \times \text{HeadDim} \times 2$



因此：



- 上下文越长，KV Cache 越大

- 在长上下文推理中：

  

  - **KV Cache 往往比模型权重更吃显存**

  





这也是为什么很多系统在长上下文场景下会首先“显存吃满”。



------





## **六、推理阶段的两个典型特征**







### **1. Prefill vs Decode**





KV Cache 将推理过程自然分成两个阶段：



- **Prefill 阶段**

  

  - 计算密集型（GEMM 多）
  - 吞吐受算力限制

  

- **Decode 阶段**

  

  - 内存密集型
  - 性能受显存带宽和缓存调度限制

  





这也是很多推理优化（如 vLLM）重点针对 decode 阶段的原因。



------





### **2. 为什么训练阶段不用 KV Cache**





在训练中：



- 整段序列并行计算
- 每个 token 都参与反向传播
- 需要完整计算图





KV Cache 会破坏梯度流与并行结构，因此：



> **KV Cache 只用于推理，不用于训练**



------





## **七、KV Cache 的常见优化方向**







### **1. Multi-Query / Grouped-Query Attention（MQA / GQA）**





- 多个 attention head **共享 K / V**
- 显著减少 KV Cache 的体积





这是许多现代模型的默认设计选择。



------





### **2. Paged KV Cache（vLLM）**





- 将 KV Cache 切分成固定大小的 page
- 类似操作系统的虚拟内存管理
- 减少碎片，提升并发请求能力





这是 vLLM 在工程上的关键创新之一。



------





### **3. FlashAttention + KV Cache**





- FlashAttention：优化 attention 计算本身
- KV Cache：避免重复计算历史





二者解决的是**不同层面的性能问题**，可以同时使用。



------





### **4. KV Cache Offloading**





- 将部分 KV Cache 放到 CPU 或其他设备

- GPU 只保留活跃窗口

- 常用于：

  

  - 超长上下文
  - 显存受限场景

  





------





## **八、一个直观类比**





KV Cache 就像做阅读理解时的笔记：



- 已经读过、理解过的内容记下来
- 后面答题时，不需要从头再读





它并不会改变理解方式，但会极大提升效率。



------





## **总结**





KV Cache 是自回归 Transformer 推理中的核心优化机制，通过缓存历史 token 的 Key 和 Value，避免在每一步生成中重复计算注意力，使推理复杂度从近似 $O(t^2)$ 降为 $O(t)$。

其代价是显存占用随上下文长度线性增长，因此在实际系统中通常会结合 MQA/GQA、Paged Cache、Offloading 等技术进行优化。



KV Cache 并不是“可选项”，而是**现代大模型推理能够落地的前提条件之一**。


