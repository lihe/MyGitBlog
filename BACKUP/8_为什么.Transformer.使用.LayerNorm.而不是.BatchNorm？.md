# [为什么 Transformer 使用 LayerNorm 而不是 BatchNorm？](https://github.com/lihe/MyGitBlog/issues/8)




在 Transformer 的设计中，**归一化方式并不是一个可随意替换的工程细节，而是由模型结构与训练范式共同决定的核心选择**。理解这一点，有助于从根本上把握 Transformer 与传统卷积网络在建模假设上的差异。





## **一句话结论**





Transformer 使用 **LayerNorm（层归一化）而不是 BatchNorm（批归一化）**，是因为 Transformer 的计算是逐 token、支持变长序列，并高度依赖自注意力与自回归生成；而 BatchNorm 对 batch 维度和样本分布有强假设，在这些场景下不成立，甚至会破坏模型行为。



------


<img width="368" height="401" alt="Image" src="https://github.com/user-attachments/assets/65e23d8d-3ce4-4d23-8f5b-7791573a1b91" />


## **一、BatchNorm 的设计前提**





BatchNorm 在提出时，隐含了几个关键假设：



**1. 样本是同构的**

同一层中，不同样本在同一维度上具有一致语义含义；第 *i* 个通道在 batch 内代表“同一种特征”。



**2. batch size 足够大**

BatchNorm 的均值和方差依赖 batch 内统计，小 batch 会导致估计噪声显著。



**3. 样本顺序不重要**

BatchNorm 在 batch 维度上聚合统计信息，不区分样本先后或结构角色。



这些假设在 CNN 中通常成立，但在 Transformer 中几乎全部失效。



------





## **二、Transformer 的计算范式与 BatchNorm 的冲突**







### **1. Transformer 是逐 token（token-wise）建模**





在 Transformer 中：



- 每个 token 是一个独立的语义单元
- 不同位置的 token 具有不同语义角色与分布特性





如果对 batch 内所有 token 使用 BatchNorm，相当于将：



> 句首词、句中词、句尾词

> 当作同一分布进行归一化



这在语言建模的语义层面并不成立。



------





### **2. 序列是变长的**





Transformer 天然支持变长序列，并通过 padding + mask 处理不等长输入。



但 BatchNorm 并不知道哪些位置是 padding：



- padding 会参与 batch 统计
- 均值与方差会随 padding 比例变化





结果是：**同一句话在不同 batch 中的归一化结果可能不同**，破坏模型一致性。



------





### **3. 自回归生成场景下 BatchNorm 几乎不可用**





在 GPT、LLaMA 等自回归模型中：



- 推理时 batch size 通常为 1
- token 逐步生成





此时 BatchNorm 的 batch 统计毫无意义，且训练与推理行为不一致，这对语言模型是致命问题。



------





## **三、LayerNorm 与 Transformer 的天然匹配**





LayerNorm 的归一化方式是：



$\text{LN}(x) = \frac{x - \mu_{\text{feature}}}{\sigma_{\text{feature}}}$



其统计方式具有以下特性：



- 在**单个样本内部**计算
- 沿**特征维度**归一化
- 不依赖 batch







### **LayerNorm 的关键优势**





**1. 与 batch size 无关**

batch size = 1 时仍然稳定，训练与推理行为一致。



**2. 逐 token 独立**

每个 token 仅基于自身特征归一化，不受其他 token 或样本干扰。



**3. 天然支持变长序列**

mask 与 padding 不会污染统计结果。



这正是 Transformer 所需要的归一化方式。



------





## **四、注意力机制决定了不能使用 BatchNorm**





自注意力的核心计算为：



$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V$



注意力机制对**数值尺度极其敏感**，而 softmax 会进一步放大分布变化。



如果使用 BatchNorm：



- 不同 batch 的统计不同
- 同一 token 在不同 batch 中的 attention 权重可能不同





这会导致模型行为不可控、训练不稳定。



LayerNorm 则保证：

**一个 token 的注意力行为仅由其自身和上下文决定，而不受 batch 中其他样本影响。**



------





## **五、工程视角：BatchNorm 会破坏可复现性**





BatchNorm 引入了：



- batch 依赖
- 运行时统计漂移
- train / eval 模式切换





而 Transformer 的设计目标是：



- 稳定
- 可扩展
- 易并行
- 高可复现性





LayerNorm 不依赖 running statistics，在分布式训练和超大模型场景下更可靠。这也是为什么几乎所有成功的 Transformer 变体都采用 LayerNorm 或其改进形式。



------





## **六、LayerNorm 在大模型中的进一步演化**





在大语言模型中，LayerNorm 甚至被进一步简化或重构：



- **RMSNorm（LLaMA、GPT-NeoX）**

  

  - 不减均值，仅做方差归一
  - 更快、更稳定

  

- **Pre-LN Transformer**

  

  - 将 LayerNorm 放在子层之前
  - 显著缓解深层网络的训练不稳定问题

  





这表明：**归一化方式在 Transformer 中是结构核心，而非附属组件。**



------





## **七、一个直观的类比**





- BatchNorm：

  “你这道题做得好不好，要参考全班平均水平”

- LayerNorm：

  “你这道题做得好不好，只取决于你自己的解题结构”





语言建模显然需要后者。



------





## **总结**





Transformer 使用 LayerNorm 而不是 BatchNorm，并非经验选择，而是由其逐 token、自回归、变长序列和注意力驱动的计算范式所决定。LayerNorm 在特征维度上对单个样本独立归一化，保证了训练与推理的一致性、数值稳定性和模型行为的可控性，因此成为 Transformer 架构中的标准归一化方案。




