# [混合精度训练时，哪些参数应该使用高精度，哪些该使用低精度？](https://github.com/lihe/MyGitBlog/issues/9)






## **一句话总原则**





> **参与梯度累积、数值范围敏感、长期状态的量 → 高精度**

> **只参与前向计算、短期使用、可容忍近似误差的量 → 低精度**



------





## **一、为什么混合精度不能“全 FP16 / BF16”？**





低精度的问题不是“不准”，而是：



- **动态范围小**
- **舍入误差大**
- **累积误差不可逆**





所以凡是**会反复累积、或作为基准的量**，一旦精度不够，训练就会：



- loss 抖动
- 梯度爆炸 / 消失
- 或者悄悄退化（最危险）





------





## **二、哪些参数 / 张量必须用高精度（FP32 或 BF16）**







### 1️⃣ Optimizer 状态






**必须 FP32**



包括：



- Adam / AdamW 的

  

  - 一阶动量 m
  - 二阶动量 v

  

- RMSProp 的 moving average

- SGD + momentum 的 momentum buffer





**原因**：



- 这是**长期累积状态**
- 数值会跨越大量 step
- FP16 累积误差极大





> 实际上：

> **99% 的训练不稳定，根因都在 optimizer state 精度不够**



------





### **2️⃣ Master Weights（权重的 FP32 副本）**





**推荐 FP32**



常见做法：



- 显存中用 FP16 / BF16 权重参与前向
- 同时维护一份 **FP32 master copy**
- 反向更新 master copy
- 再 cast 回低精度





**原因**：



- 梯度更新是“微小增量”
- FP16 很容易被直接 round 掉





------





### **3️⃣ Loss 计算与归一化相关量**





**必须高精度**



包括：



- loss 本身
- loss scaling（动态 loss scale）
- BatchNorm 的 running mean / var
- LayerNorm 的统计计算（常内部 FP32）





**原因**：



- loss 是所有梯度的源头
- 数值不稳会放大到整个网络





------





### **4️⃣ 梯度累积相关变量**





**推荐 FP32**



包括：



- 梯度累积 buffer
- 多 step 累加的 grad





**原因**：



- 梯度本身已经是高噪声信号
- 再低精度累加极易失真





------





## **三、哪些可以安全使用低精度（FP16 / BF16）**







### **1️⃣ 模型权重（参与前向）**





**可以低精度**



- Linear / Conv / Attention 权重
- FFN 权重
- Embedding 权重





前提：



- 有 FP32 master copy 或 BF16 宽动态范围





------





### **2️⃣ 前向激活（Activations）**





**可以低精度（最主要节省显存来源）**



- Q / K / V
- attention output
- FFN 中间层激活





原因：



- 激活是**短生命周期**
- 用完即丢
- 误差不会长期积累





------





### **3️⃣ Attention / FFN 中的矩阵乘法**





**低精度是主流**



- Tensor Core 对 FP16 / BF16 高度优化
- 性能提升巨大





------





### **4️⃣ LoRA / Adapter 参数（通常 BF16 / FP16）**





- 参数量小
- 梯度相对稳定
- 实践中 BF16 非常稳





------





## **四、FP16 vs BF16 的关键区别**



| **项**                | **FP16** | **BF16**   |
| --------------------- | -------- | ---------- |
| Mantissa（精度）      | 高       | 低         |
| Exponent（范围）      | 小       | 大         |
| 易溢出                | 是       | 否         |
| 是否需要 loss scaling | 必须     | 通常不需要 |
| LLM 训练主流          | 旧       | **新主流** |

**结论**：



- **能用 BF16 就别用 FP16**
- BF16 天生更适合大模型训练





------





## **五、典型框架里的默认做法**







### **PyTorch AMP（autocast）**





- 前向：

  

  - Linear / MatMul → FP16 / BF16

  

- 归一化 / softmax：

  

  - 自动回退 FP32

  

- Optimizer：

  

  - state 永远 FP32

  





------





### **DeepSpeed / FSDP**





- 参数分片：低精度
- Optimizer state：FP32
- 梯度 reduce：FP32 或混合





------





### **QLoRA 特例**





- Base 权重：**NF4（存储）**
- 计算时：反量化为 BF16
- LoRA 参数：BF16
- Optimizer state：FP32





------





## **六、一个清晰的分类表**



| **类型**             | **精度**    | **原因**    |
| -------------------- | ----------- | ----------- |
| Optimizer state      | FP32        | 长期累积    |
| Master weights       | FP32        | 微小更新    |
| Loss / norm 统计     | FP32        | 稳定性      |
| 梯度累积             | FP32        | 防止失真    |
| 前向权重             | FP16 / BF16 | 吞吐        |
| Activations          | FP16 / BF16 | 短生命周期  |
| Attention / FFN 计算 | FP16 / BF16 | Tensor Core |



------





## **最终一句话总结**





> 混合精度训练的核心不是“能不能用低精度”，而是**绝不让任何会被长期累积、或作为数值基准的变量使用低精度**；前向计算尽量低精度，反向更新与状态一律高精度。


