# [QLoRA 深度解析：用 4bit 量化，把 70B 大模型拉进单卡时代](https://github.com/lihe/MyGitBlog/issues/3)




## **前言：QLoRA 为什么值得被认真理解？**





QLoRA 的作者 **Tim Dettmers**，是模型量化领域的核心人物之一，同时也深度参与了 **BLOOM** 等大模型的工程化落地。



模型量化（Quantization）和参数高效微调（PEFT）看似属于两个方向，但它们背后有一个共同目标：



> **让大模型的训练与推理更快、更省、更可落地。**



QLoRA 正是将“**量化**”与“**LoRA 微调**”这两条路线真正融合到了一起，首次实现了：



> **在 4bit 量化模型上稳定训练 LoRA，且效果几乎不损失。**



这让 **70B 级别模型的单卡微调** 成为现实。



------





## **一、为什么需要 QLoRA？**







### **1. 传统微调的显存瓶颈**



| **方法**     | **显存需求**       | **问题**           |
| ------------ | ------------------ | ------------------ |
| 全参数微调   | 极高（65B >300GB） | 完全不可落地       |
| LoRA（FP16） | 较高（65B >120GB） | 模型仍需 FP16 加载 |
| GPTQ / AWQ   | 低                 | 只能推理，难以训练 |

结论非常明确：



> **我们需要一种：既能量化、又能训练的方案。**



QLoRA 就是这个答案。



------





## **二、QLoRA 的一句话定义**





> **QLoRA（Quantized LoRA）**

> 是一种在 **4bit 量化模型上冻结主权重，仅训练 LoRA 低秩参数** 的高效微调方法，在显存降低约 75% 的同时，训练效果接近甚至优于全参微调。



------





## **三、QLoRA 的整体技术框架**





QLoRA 并不是单一技术，而是**四项关键工程与算法设计的组合**：



1. **NF4（4bit Normal Float）量化**
2. **分块 + 分位数量化**
3. **双重量化（Double Quantization）**
4. **分页优化（Paged Optimizer / Paged Attention）**
5. **LoRA Adapter 训练**





下面逐一拆解。



------





## **四、模型量化基础（理解 QLoRA 的前提）**







### **4.1 什么是模型量化？**





模型量化的目标是：



> **用更低精度表示权重，在可接受的误差范围内减少模型体积和显存占用。**



本质上，这是一个**有损压缩问题**。



常见量化方式包括：



- 线性量化（absmax）
- 零点量化（zero-point）
- 非线性量化（分位数量化）





------





### **4.2 分位数量化（Quantile Quantization）**





QLoRA 的关键观察是：



> **大模型权重通常近似服从正态分布。**



因此，与其用线性区间均匀切分，不如：



- 按 **CDF 等概率切分**
- 让每个量化 bin 出现频率尽可能一致





这就是 **分位数量化** 的核心思想。



------







## **五、NF4：QLoRA 能在 4bit 下不掉点的根本原因**






### **5.1 一句话理解 NF4**





> **NF4 是一种专为大模型权重分布设计的 4bit 非均匀量化格式，通过将有限的 4bit 表示能力集中在高概率区域，从而在极低精度下最大化表达能力。**



它不是“更激进的 INT4”，而是**完全不同的量化思路**。



------





### **5.2 为什么传统 INT4 不适合大模型？**





INT4 采用的是**均匀量化**：



- 4bit → 16 个等距离散值
- 默认假设数值分布是均匀的





但大模型（LLM）的权重分布具有非常明确的统计特征：



- 近似 **零均值**
- 近似 **正态分布**
- **大量权重集中在 0 附近**
- 极端大值数量极少，但幅度大





这会直接导致 INT4 的问题：



- 中心区域（最重要的权重区间）精度不足
- 量化桶大量浪费在几乎不会出现的区间
- 少量误差被放大为性能下降





**换句话说：INT4 在“统计意义上”并不匹配 LLM。**



------





### **5.3 NF4 的核心思想：非均匀量化**





NF4 的设计目标非常明确：



> **把有限的 16 个量化值，用在最“值得用”的地方。**



具体来说：



- 在 **0 附近分布更密**
- 在 **两端极值区域分布更稀**





这使得：



- 常见的小权重 → 高精度表示
- 罕见的大权重 → 容忍更大误差





这种设计与 LLM 权重的真实分布高度匹配。



------





### **5.4 NF4 是怎么实现的？**







#### **1️⃣ 预定义 16 个“浮点量化值”**



NF4 并不是简单的整数映射，而是**预先定义好的 16 个浮点值**，这些值近似来自：



- 标准正态分布的分位点（quantiles）
- 经过归一化与截断处理





你可以把它理解为：

```
[-a, ..., -small, 0, +small, ..., +a]
```

但它们**不是等距的**，而是统计意义上更合理的分布。



------





#### **2️⃣ 权重映射到最近的 NF4 值**



量化过程本质上是一个最近邻映射：



$q = \arg\min_{v \in \text{NF4 values}} |w - v|$



反量化时：



$\hat{w} = s \cdot q$



其中：



- q：4bit index
- s：FP16（或经双重量化后的）scale





**scale 的精度非常关键**，它决定了整个 block 的动态范围。



------





### **5.5 为什么 NF4 比 INT4 精度高？**





#### **（1）分布匹配**





- LLM 权重 ≈ 正态分布
- NF4 量化点 ≈ 正态分布分位点





→ 在统计意义上是近似最优的 4bit 表示。



------





#### **（2）误差集中在“不重要区域”**





- 小权重：数量多、贡献大 → 高精度
- 大权重：数量少 → 容忍误差





整体误差被有效控制。



------





#### **（3）论文实验结论**



QLoRA 原论文明确指出：



> **NF4 在 4bit 条件下的量化误差，接近 FP16，显著优于 INT4。**



这正是 QLoRA 能在 4bit 下“几乎不掉点”的关键原因。



------





### **5.6 NF4 在 QLoRA 中的角色定位**





在 QLoRA 框架中，各部分分工非常清晰：

| **组件**  | **精度**      | **是否训练** | **作用**             |
| --------- | ------------- | ------------ | -------------------- |
| Base 权重 | NF4           | ❌ 冻结       | 压缩模型、降低显存   |
| Scale     | FP16 / 双量化 | ❌            | 恢复动态范围         |
| LoRA A/B  | FP16 / BF16   | ✅            | 学习新任务、补偿误差 |

可以这样理解：



> **NF4 负责“把模型装进显存”，**

> **LoRA 负责“把能力补回来”。**



------





### 5.7 NF4 vs INT4



| **对比项**   | **INT4** | **NF4**  |
| ------------ | -------- | -------- |
| 量化方式     | 均匀     | 非均匀   |
| 感知权重分布 | ❌        | ✅        |
| 中心区域精度 | 低       | 高       |
| 是否适合 LLM | 一般     | 非常适合 |
| QLoRA 默认   | ❌        | ✅        |

一句话总结：



> **INT4 是通用压缩方案，NF4 是为大模型定制的 4bit 浮点格式。**



------





### **5.8 一个常被追问的点**





**NF4 是浮点还是整数？**



- **逻辑上**：4bit 浮点格式
- **实现上**：4bit index + FP16 scale





它不是传统 IEEE 浮点，而是**为神经网络权重定制的表示形式**。



------





## **小结**





如果没有 NF4：



- 4bit 量化 ≈ 性能大幅下降
- LoRA 无法完全补偿量化误差





而正是 NF4 的存在，才让下面这件事成立：



> **在 4bit 条件下冻结主权重，只训练少量 LoRA 参数，依然可以达到接近全参微调的效果。**



这也是为什么——

**几乎所有严肃的 QLoRA 实现，都会默认使用 NF4。**



------



## **六、双重量化：进一步压缩显存**





在 4bit 量化中：



- 权重 → 4bit
- 量化常数 c → FP32（显存杀手）





QLoRA 的解决方案是：



> **对量化常数本身再做一次 8bit 量化。**



即：



- 第一层：权重量化
- 第二层：量化常数量化





结果：



- 量化常数显存占用从 **~1.6% → ~0.37%**
- 几乎没有额外精度损失





------





## **七、Paged Optimizer：避免训练 OOM 的关键工程技巧**





即使权重量化，训练中仍存在一个风险：



> **显存峰值（activation + gradient checkpoint）导致 OOM。**



QLoRA 引入 **分页优化（Paged Optimizer / Paged Attention）**：



- 将部分 KV cache / 激活页换出到 CPU 内存
- 类似操作系统的虚拟内存分页机制
- 在显存紧张时动态调度





这使得：



> **13B–70B 模型在单卡训练成为可能。**



------





## **八、QLoRA 如何与 LoRA 结合？**





核心公式仍然是 LoRA 的形式：



$W' = W + \Delta W,\quad \Delta W = BA$



区别在于：



- W：4bit NF4 量化权重（冻结）
- A, B：FP16 / BF16 LoRA 参数（可训练）





训练时：



- 主权重仅反量化参与前向
- **梯度只回传到 LoRA**





这也是 QLoRA 能“量化 + 训练”共存的根本原因。



------





## **九、QLoRA 的能力边界**







### **支持的任务**



| **能力**   | **是否支持** |
| ---------- | ------------ |
| SFT        | ✅            |
| DPO / ORPO | ✅            |
| 多轮对话   | ✅            |
| 单卡 70B   | ✅            |



------





### **不适合的场景**





- 模型结构级改动（attention 重构）
- LoRA rank 太小（易欠拟合）
- 极端高精度任务（数学 / 代码）





------





## **十、总结：QLoRA 的真正价值**





QLoRA 的本质不是“LoRA 的改进”，而是：



> **把大模型训练，从“算力竞赛”拉回到“工程理性”。**



它证明了一件事：



- 量化 ≠ 只能推理
- 低精度 ≠ 低性能





在今天，QLoRA 已经成为：



> **企业 SFT / DPO / 垂域微调的事实标准方案。**



------





## **写在最后**





这篇文章既是对 QLoRA 的系统性梳理，也是一种个人补课记录。

如果其中存在理解偏差，欢迎讨论与指正。



------





### **参考**





- [1] QLoRA: Efficient Finetuning of Quantized LLMs
- [2] LoRA: Low-Rank Adaptation of Large Language Models



