# [WordPiece Tokenization：从概率视角理解子词分词算法](https://github.com/lihe/MyGitBlog/issues/14)





在大语言模型出现之前，**Tokenizer 的设计**就已经深刻影响着模型的上限。

WordPiece，正是子词分词算法发展史中一个**非常关键、但常被简化理解的节点**。



它不仅仅是“BERT 用的分词器”，而是一种**明确以语言模型概率为目标函数的子词学习方法**。



------





## **一、为什么需要 WordPiece？**





早期 NLP 分词方案存在明显矛盾：



- **按词分词（word-level）**

  

  - 词表巨大
  - OOV（未登录词）问题严重

  

- **按字分词（char-level）**

  

  - 序列极长
  - 语义信息稀薄

  





于是诞生了折中方案：



> **子词分词（Subword Tokenization）**



WordPiece、BPE、Unigram 都属于这一范式，但它们的**优化目标并不相同**。



------





## **二、WordPiece 的核心思想**





WordPiece 的核心不是“哪个片段最常见”，而是：



> **哪个子词合并，能最大化训练语料的语言模型似然。**



也就是说，它从一开始就站在了**概率建模**而不是**频率统计**的角度。



这是它与 BPE 的根本区别。



------





## **三、WordPiece 的概率视角：在优化什么？**





WordPiece 假设一个 **Unigram 语言模型**：



- 每个子词独立出现
- 语料的整体概率 = 所有子词概率的乘积





对应的对数似然为：



$\mathcal{L} = \sum_{s \in S} f(s)\log P(s)$



其中：



- S 是当前子词集合
- f(s) 是子词在语料中的出现次数





WordPiece 的训练目标是：



> **通过合并子词，使整体对数似然增加得最多。**



------





## **四、WordPiece 的训练流程**







### **Step 1：初始化词表**





- 从字符级开始（a, b, c, …）







### **Step 2：统计子词频率并估计概率**





$P(s) = \frac{f(s)}{N}$





### **Step 3：评估所有可能的合并候选**





对任意相邻子词 a, b，计算合并收益：



$\Delta \mathcal{L} = f(ab)\log P(ab) - f(a)\log P(a) - f(b)\log P(b)$





### **Step 4：选择** 

### **ΔL 最大**

###  **的合并**





- 加入新子词
- 更新频率和概率







### **Step 5：重复**





直到达到预设词表大小（如 30k）



> 这一步解释了：

> **WordPiece 训练慢，但“合并更有语义意义”。**



------





## **五、一个典型示例（非常好理解）**





语料包含：



- unaffable
- unacceptable
- unaccountable





在训练过程中：



- u + n → un 会频繁提升整体似然
- un + able 比随机字符组合更“划算”





最终学到的子词往往是：

```
un, able, account, ##able
```

这些片段**具有稳定的语言学意义**，而不仅仅是高频。



------





## **六、WordPiece 的一个标志性设计：##**





在实际分词阶段，WordPiece 使用：



- **最长匹配优先**
- 用 ## 标记“非词首子词”





例如：

```
playing → play + ##ing
```

\##ing 表示：



> “这个子词不能独立作为词首出现”



这让模型能够区分：



- play
- display
- playing





在语义空间中的关系。



------





## **七、WordPiece vs BPE vs Unigram**



| **特性**   | **WordPiece** | **BPE**       | **Unigram**     |
| ---------- | ------------- | ------------- | --------------- |
| 优化目标   | 最大化似然    | 合并最高频对  | 最小化整体 loss |
| 核心策略   | 概率增益      | 贪心频率      | 从大词表删减    |
| 是否有 ##  | ✅             | ❌             | ❌               |
| 训练复杂度 | 中            | 低            | 高              |
| 代表模型   | BERT          | GPT / RoBERTa | SentencePiece   |

一句话区分：



- **BPE**：谁最常一起出现就合谁
- **WordPiece**：谁让语言模型“更合理”就合谁
- **Unigram**：先给你所有可能，再慢慢删到最优





------





## **八、WordPiece 的优势与局限**







### **优势**





- 显著减少 OOV
- 子词语义更稳定
- 英文等拼写语言表现好
- 非常适合 BERT 这类 Masked LM







### **局限**





- 训练复杂、计算量大
- 对中文等无空格语言不友好
- 工程灵活性不如 SentencePiece
- 已不适合超大规模 LLM





这也是为什么：



- **BERT 系列** → WordPiece
- **GPT / LLaMA / Qwen** → BPE / SentencePiece





------





## **九、为什么 WordPiece 逐渐“退居二线”？**





不是因为它不好，而是因为：



- SentencePiece 语言无关
- Unigram 全局最优
- BPE 工程实现更简单、可扩展





但在理解 **Tokenizer 与语言模型之间的关系**时，

WordPiece 依然是**最好的教学级算法之一**。



------





## **最终总结**





WordPiece 是一种基于语言模型概率的子词分词算法，通过在训练阶段选择能最大化语料整体似然的子词合并方式，在词表规模与 OOV 问题之间取得平衡。它生成的子词通常具有更稳定的语言学意义，被广泛应用于 BERT 等模型中。




