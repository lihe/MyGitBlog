## MyGitBlog
My personal blog using issues and GitHub Actions (参考[yihong](https://github.com/yihong0618/gitblog))



### 提醒自己：

#### 1.专注不是排除杂念，而是对重要之事的持续选择。

#### 2.行动会解决大多数焦虑。


[RSS Feed](https://raw.githubusercontent.com/lihe/MyGitBlog/master/feed.xml)

## 最近更新
- [混合精度训练时，哪些参数应该使用高精度，哪些该使用低精度？](https://github.com/lihe/MyGitBlog/issues/9)--2025-12-14
- [为什么 Transformer 使用 LayerNorm 而不是 BatchNorm？](https://github.com/lihe/MyGitBlog/issues/8)--2025-12-14
- [Prefix-Tuning：在注意力机制中引入可学习前缀的参数高效微调方法](https://github.com/lihe/MyGitBlog/issues/7)--2025-12-14
- [Transformer 中 FFN 是做什么的？——为什么注意力之外还需要前馈网络](https://github.com/lihe/MyGitBlog/issues/6)--2025-12-14
- [P-Tuning 与 P-Tuning v2：参数高效 Prompt 微调方法解析](https://github.com/lihe/MyGitBlog/issues/5)--2025-12-13
## LoRA
- [QLoRA 深度解析：用 4bit 量化，把 70B 大模型拉进单卡时代](https://github.com/lihe/MyGitBlog/issues/3)--2025-12-13
## MCP
- [从 Function Calling 到 MCP：大模型如何真正接入真实世界？](https://github.com/lihe/MyGitBlog/issues/2)--2025-12-13
## PPO
- [GRPO 详解：DeepSeek-R1/DeepSeekMath 为什么用它替代 PPO？](https://github.com/lihe/MyGitBlog/issues/4)--2025-12-13
## RAG
- [从 RAG 到 GraphRAG，再到 LightRAG](https://github.com/lihe/MyGitBlog/issues/1)--2025-12-13
## Transformer
- [为什么 Transformer 使用 LayerNorm 而不是 BatchNorm？](https://github.com/lihe/MyGitBlog/issues/8)--2025-12-14
- [Transformer 中 FFN 是做什么的？——为什么注意力之外还需要前馈网络](https://github.com/lihe/MyGitBlog/issues/6)--2025-12-14
## 微调
- [混合精度训练时，哪些参数应该使用高精度，哪些该使用低精度？](https://github.com/lihe/MyGitBlog/issues/9)--2025-12-14
- [Prefix-Tuning：在注意力机制中引入可学习前缀的参数高效微调方法](https://github.com/lihe/MyGitBlog/issues/7)--2025-12-14
- [P-Tuning 与 P-Tuning v2：参数高效 Prompt 微调方法解析](https://github.com/lihe/MyGitBlog/issues/5)--2025-12-13
