<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>https://github.com/lihe/MyGitBlog</id><title>RSS feed of lihe's MyGitBlog</title><updated>2025-12-13T06:58:07.624756+00:00</updated><link href="https://github.com/lihe/MyGitBlog"/><link href="https://raw.githubusercontent.com/lihe/MyGitBlog/master/feed.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><entry><id>https://github.com/lihe/MyGitBlog/issues/3</id><title>QLoRA 深度解析：用 4bit 量化，把 70B 大模型拉进单卡时代</title><updated>2025-12-13T06:58:07.995932+00:00</updated><content type="html"><![CDATA[<h2><strong>前言：QLoRA 为什么值得被认真理解？</strong></h2>
<p>QLoRA 的作者 <strong>Tim Dettmers</strong>，是模型量化领域的核心人物之一，同时也深度参与了 <strong>BLOOM</strong> 等大模型的工程化落地。</p>
<p>模型量化（Quantization）和参数高效微调（PEFT）看似属于两个方向，但它们背后有一个共同目标：</p>
<blockquote>
<p><strong>让大模型的训练与推理更快、更省、更可落地。</strong></p>
</blockquote>
<p>QLoRA 正是将“<strong>量化</strong>”与“<strong>LoRA 微调</strong>”这两条路线真正融合到了一起，首次实现了：</p>
<blockquote>
<p><strong>在 4bit 量化模型上稳定训练 LoRA，且效果几乎不损失。</strong></p>
</blockquote>
<p>这让 <strong>70B 级别模型的单卡微调</strong> 成为现实。</p>
<hr />
<h2><strong>一、为什么需要 QLoRA？</strong></h2>
<h3><strong>1. 传统微调的显存瓶颈</strong></h3>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>显存需求</strong></th>
<th><strong>问题</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>全参数微调</td>
<td>极高（65B &gt;300GB）</td>
<td>完全不可落地</td>
</tr>
<tr>
<td>LoRA（FP16）</td>
<td>较高（65B &gt;120GB）</td>
<td>模型仍需 FP16 加载</td>
</tr>
<tr>
<td>GPTQ / AWQ</td>
<td>低</td>
<td>只能推理，难以训练</td>
</tr>
</tbody></table><p>结论非常明确：</p>
<blockquote>
<p><strong>我们需要一种：既能量化、又能训练的方案。</strong></p>
</blockquote>
<p>QLoRA 就是这个答案。</p>
<hr />
<h2><strong>二、QLoRA 的一句话定义</strong></h2>
<blockquote>
<p><strong>QLoRA（Quantized LoRA）</strong></p>
</blockquote>
<blockquote>
<p>是一种在 <strong>4bit 量化模型上冻结主权重，仅训练 LoRA 低秩参数</strong> 的高效微调方法，在显存降低约 75% 的同时，训练效果接近甚至优于全参微调。</p>
</blockquote>
<hr />
<h2><strong>三、QLoRA 的整体技术框架</strong></h2>
<p>QLoRA 并不是单一技术，而是<strong>四项关键工程与算法设计的组合</strong>：</p>
<ol>
<li><strong>NF4（4bit Normal Float）量化</strong></li>
<li><strong>分块 + 分位数量化</strong></li>
<li><strong>双重量化（Double Quantization）</strong></li>
<li><strong>分页优化（Paged Optimizer / Paged Attention）</strong></li>
<li><strong>LoRA Adapter 训练</strong></li>
</ol>
<p>下面逐一拆解。</p>
<hr />
<h2><strong>四、模型量化基础（理解 QLoRA 的前提）</strong></h2>
<h3><strong>4.1 什么是模型量化？</strong></h3>
<p>模型量化的目标是：</p>
<blockquote>
<p><strong>用更低精度表示权重，在可接受的误差范围内减少模型体积和显存占用。</strong></p>
</blockquote>
<p>本质上，这是一个<strong>有损压缩问题</strong>。</p>
<p>常见量化方式包括：</p>
<ul>
<li>线性量化（absmax）</li>
<li>零点量化（zero-point）</li>
<li>非线性量化（分位数量化）</li>
</ul>
<hr />
<h3><strong>4.2 分位数量化（Quantile Quantization）</strong></h3>
<p>QLoRA 的关键观察是：</p>
<blockquote>
<p><strong>大模型权重通常近似服从正态分布。</strong></p>
</blockquote>
<p>因此，与其用线性区间均匀切分，不如：</p>
<ul>
<li>按 <strong>CDF 等概率切分</strong></li>
<li>让每个量化 bin 出现频率尽可能一致</li>
</ul>
<p>这就是 <strong>分位数量化</strong> 的核心思想。</p>
<hr />
<h2><strong>五、NF4：QLoRA 能在 4bit 下不掉点的根本原因</strong></h2>
<h3><strong>5.1 一句话理解 NF4</strong></h3>
<blockquote>
<p><strong>NF4 是一种专为大模型权重分布设计的 4bit 非均匀量化格式，通过将有限的 4bit 表示能力集中在高概率区域，从而在极低精度下最大化表达能力。</strong></p>
</blockquote>
<p>它不是“更激进的 INT4”，而是<strong>完全不同的量化思路</strong>。</p>
<hr />
<h3><strong>5.2 为什么传统 INT4 不适合大模型？</strong></h3>
<p>INT4 采用的是<strong>均匀量化</strong>：</p>
<ul>
<li>4bit → 16 个等距离散值</li>
<li>默认假设数值分布是均匀的</li>
</ul>
<p>但大模型（LLM）的权重分布具有非常明确的统计特征：</p>
<ul>
<li>近似 <strong>零均值</strong></li>
<li>近似 <strong>正态分布</strong></li>
<li><strong>大量权重集中在 0 附近</strong></li>
<li>极端大值数量极少，但幅度大</li>
</ul>
<p>这会直接导致 INT4 的问题：</p>
<ul>
<li>中心区域（最重要的权重区间）精度不足</li>
<li>量化桶大量浪费在几乎不会出现的区间</li>
<li>少量误差被放大为性能下降</li>
</ul>
<p><strong>换句话说：INT4 在“统计意义上”并不匹配 LLM。</strong></p>
<hr />
<h3><strong>5.3 NF4 的核心思想：非均匀量化</strong></h3>
<p>NF4 的设计目标非常明确：</p>
<blockquote>
<p><strong>把有限的 16 个量化值，用在最“值得用”的地方。</strong></p>
</blockquote>
<p>具体来说：</p>
<ul>
<li>在 <strong>0 附近分布更密</strong></li>
<li>在 <strong>两端极值区域分布更稀</strong></li>
</ul>
<p>这使得：</p>
<ul>
<li>常见的小权重 → 高精度表示</li>
<li>罕见的大权重 → 容忍更大误差</li>
</ul>
<p>这种设计与 LLM 权重的真实分布高度匹配。</p>
<hr />
<h3><strong>5.4 NF4 是怎么实现的？</strong></h3>
<h4><strong>1️⃣ 预定义 16 个“浮点量化值”</strong></h4>
<p>NF4 并不是简单的整数映射，而是<strong>预先定义好的 16 个浮点值</strong>，这些值近似来自：</p>
<ul>
<li>标准正态分布的分位点（quantiles）</li>
<li>经过归一化与截断处理</li>
</ul>
<p>你可以把它理解为：</p>
<pre><code>[-a, ..., -small, 0, +small, ..., +a]
</code></pre>
<p>但它们<strong>不是等距的</strong>，而是统计意义上更合理的分布。</p>
<hr />
<h4><strong>2️⃣ 权重映射到最近的 NF4 值</strong></h4>
<p>量化过程本质上是一个最近邻映射：</p>
<p>$q = \arg\min_{v \in \text{NF4 values}} |w - v|$</p>
<p>反量化时：</p>
<p>$\hat{w} = s \cdot q$</p>
<p>其中：</p>
<ul>
<li>q：4bit index</li>
<li>s：FP16（或经双重量化后的）scale</li>
</ul>
<p><strong>scale 的精度非常关键</strong>，它决定了整个 block 的动态范围。</p>
<hr />
<h3><strong>5.5 为什么 NF4 比 INT4 精度高？</strong></h3>
<h4><strong>（1）分布匹配</strong></h4>
<ul>
<li>LLM 权重 ≈ 正态分布</li>
<li>NF4 量化点 ≈ 正态分布分位点</li>
</ul>
<p>→ 在统计意义上是近似最优的 4bit 表示。</p>
<hr />
<h4><strong>（2）误差集中在“不重要区域”</strong></h4>
<ul>
<li>小权重：数量多、贡献大 → 高精度</li>
<li>大权重：数量少 → 容忍误差</li>
</ul>
<p>整体误差被有效控制。</p>
<hr />
<h4><strong>（3）论文实验结论</strong></h4>
<p>QLoRA 原论文明确指出：</p>
<blockquote>
<p><strong>NF4 在 4bit 条件下的量化误差，接近 FP16，显著优于 INT4。</strong></p>
</blockquote>
<p>这正是 QLoRA 能在 4bit 下“几乎不掉点”的关键原因。</p>
<hr />
<h3><strong>5.6 NF4 在 QLoRA 中的角色定位</strong></h3>
<p>在 QLoRA 框架中，各部分分工非常清晰：</p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>精度</strong></th>
<th><strong>是否训练</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Base 权重</td>
<td>NF4</td>
<td>❌ 冻结</td>
<td>压缩模型、降低显存</td>
</tr>
<tr>
<td>Scale</td>
<td>FP16 / 双量化</td>
<td>❌</td>
<td>恢复动态范围</td>
</tr>
<tr>
<td>LoRA A/B</td>
<td>FP16 / BF16</td>
<td>✅</td>
<td>学习新任务、补偿误差</td>
</tr>
</tbody></table><p>可以这样理解：</p>
<blockquote>
<p><strong>NF4 负责“把模型装进显存”，</strong></p>
</blockquote>
<blockquote>
<p><strong>LoRA 负责“把能力补回来”。</strong></p>
</blockquote>
<hr />
<h3>5.7 NF4 vs INT4</h3>
<table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>INT4</strong></th>
<th><strong>NF4</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>量化方式</td>
<td>均匀</td>
<td>非均匀</td>
</tr>
<tr>
<td>感知权重分布</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>中心区域精度</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>是否适合 LLM</td>
<td>一般</td>
<td>非常适合</td>
</tr>
<tr>
<td>QLoRA 默认</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody></table><p>一句话总结：</p>
<blockquote>
<p><strong>INT4 是通用压缩方案，NF4 是为大模型定制的 4bit 浮点格式。</strong></p>
</blockquote>
<hr />
<h3><strong>5.8 一个常被追问的点</strong></h3>
<p><strong>NF4 是浮点还是整数？</strong></p>
<ul>
<li><strong>逻辑上</strong>：4bit 浮点格式</li>
<li><strong>实现上</strong>：4bit index + FP16 scale</li>
</ul>
<p>它不是传统 IEEE 浮点，而是<strong>为神经网络权重定制的表示形式</strong>。</p>
<hr />
<h2><strong>小结</strong></h2>
<p>如果没有 NF4：</p>
<ul>
<li>4bit 量化 ≈ 性能大幅下降</li>
<li>LoRA 无法完全补偿量化误差</li>
</ul>
<p>而正是 NF4 的存在，才让下面这件事成立：</p>
<blockquote>
<p><strong>在 4bit 条件下冻结主权重，只训练少量 LoRA 参数，依然可以达到接近全参微调的效果。</strong></p>
</blockquote>
<p>这也是为什么——</p>
<p><strong>几乎所有严肃的 QLoRA 实现，都会默认使用 NF4。</strong></p>
<hr />
<h2><strong>六、双重量化：进一步压缩显存</strong></h2>
<p>在 4bit 量化中：</p>
<ul>
<li>权重 → 4bit</li>
<li>量化常数 c → FP32（显存杀手）</li>
</ul>
<p>QLoRA 的解决方案是：</p>
<blockquote>
<p><strong>对量化常数本身再做一次 8bit 量化。</strong></p>
</blockquote>
<p>即：</p>
<ul>
<li>第一层：权重量化</li>
<li>第二层：量化常数量化</li>
</ul>
<p>结果：</p>
<ul>
<li>量化常数显存占用从 <strong><del>1.6% → </del>0.37%</strong></li>
<li>几乎没有额外精度损失</li>
</ul>
<hr />
<h2><strong>七、Paged Optimizer：避免训练 OOM 的关键工程技巧</strong></h2>
<p>即使权重量化，训练中仍存在一个风险：</p>
<blockquote>
<p><strong>显存峰值（activation + gradient checkpoint）导致 OOM。</strong></p>
</blockquote>
<p>QLoRA 引入 <strong>分页优化（Paged Optimizer / Paged Attention）</strong>：</p>
<ul>
<li>将部分 KV cache / 激活页换出到 CPU 内存</li>
<li>类似操作系统的虚拟内存分页机制</li>
<li>在显存紧张时动态调度</li>
</ul>
<p>这使得：</p>
<blockquote>
<p><strong>13B–70B 模型在单卡训练成为可能。</strong></p>
</blockquote>
<hr />
<h2><strong>八、QLoRA 如何与 LoRA 结合？</strong></h2>
<p>核心公式仍然是 LoRA 的形式：</p>
<p>$W' = W + \Delta W,\quad \Delta W = BA$</p>
<p>区别在于：</p>
<ul>
<li>W：4bit NF4 量化权重（冻结）</li>
<li>A, B：FP16 / BF16 LoRA 参数（可训练）</li>
</ul>
<p>训练时：</p>
<ul>
<li>主权重仅反量化参与前向</li>
<li><strong>梯度只回传到 LoRA</strong></li>
</ul>
<p>这也是 QLoRA 能“量化 + 训练”共存的根本原因。</p>
<hr />
<h2><strong>九、QLoRA 的能力边界</strong></h2>
<h3><strong>支持的任务</strong></h3>
<table>
<thead>
<tr>
<th><strong>能力</strong></th>
<th><strong>是否支持</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>SFT</td>
<td>✅</td>
</tr>
<tr>
<td>DPO / ORPO</td>
<td>✅</td>
</tr>
<tr>
<td>多轮对话</td>
<td>✅</td>
</tr>
<tr>
<td>单卡 70B</td>
<td>✅</td>
</tr>
</tbody></table><hr />
<h3><strong>不适合的场景</strong></h3>
<ul>
<li>模型结构级改动（attention 重构）</li>
<li>LoRA rank 太小（易欠拟合）</li>
<li>极端高精度任务（数学 / 代码）</li>
</ul>
<hr />
<h2><strong>十、总结：QLoRA 的真正价值</strong></h2>
<p>QLoRA 的本质不是“LoRA 的改进”，而是：</p>
<blockquote>
<p><strong>把大模型训练，从“算力竞赛”拉回到“工程理性”。</strong></p>
</blockquote>
<p>它证明了一件事：</p>
<ul>
<li>量化 ≠ 只能推理</li>
<li>低精度 ≠ 低性能</li>
</ul>
<p>在今天，QLoRA 已经成为：</p>
<blockquote>
<p><strong>企业 SFT / DPO / 垂域微调的事实标准方案。</strong></p>
</blockquote>
<hr />
<h2><strong>写在最后</strong></h2>
<p>这篇文章既是对 QLoRA 的系统性梳理，也是一种个人补课记录。</p>
<p>如果其中存在理解偏差，欢迎讨论与指正。</p>
<hr />
<h3><strong>参考</strong></h3>
<ul>
<li>[1] QLoRA: Efficient Finetuning of Quantized LLMs</li>
<li>[2] LoRA: Low-Rank Adaptation of Large Language Models</li>
</ul>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/3"/><category term="LoRA"/><published>2025-12-13T06:57:38+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/2</id><title>从 Function Calling 到 MCP：大模型如何真正接入真实世界？</title><updated>2025-12-13T06:58:08.175921+00:00</updated><content type="html"><![CDATA[<blockquote>
<p>当我们谈论 AI Agent、Copilot、企业级智能系统时，真正的分水岭早已不在模型参数量，而在一个更底层的问题上：</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>大模型，究竟是“会说话的百科全书”，还是“能动手做事的系统”？</strong></p>
</blockquote>
<p>Function Calling 和 MCP（Model Context Protocol），正是围绕这个问题展开的两代关键答案。</p>
<hr />
<h2><strong>一、没有工具的大模型，本质上是“被困在房间里的人”</strong></h2>
<p>早期的大语言模型，即便知识极其丰富，本质上仍然是一个<strong>封闭系统</strong>：</p>
<ul>
<li>无法访问实时数据</li>
<li>不能查询数据库</li>
<li>不能操作文件系统</li>
<li>不能调用外部 API</li>
</ul>
<p>它只能基于<strong>训练时学到的静态知识</strong>进行回答。</p>
<p>这在“解释概念”“生成文本”时问题不大，但一旦进入真实应用场景，就会立刻失效：</p>
<ul>
<li>查最新数据</li>
<li>进行精确计算</li>
<li>与业务系统交互</li>
<li>执行确定性任务</li>
</ul>
<p><strong>模型需要“手”和“工具”。</strong></p>
<hr />
<h2><strong>二、Function Calling：第一次让模型“会用工具”</strong></h2>
<h3><strong>1. Function Calling 是什么？</strong></h3>
<p>Function Calling 是 OpenAI 在 2023 年提出的一种能力扩展机制，其核心目标只有一个：</p>
<blockquote>
<p><strong>让模型在推理过程中，决定是否调用外部函数，并生成结构化参数。</strong></p>
</blockquote>
<p>需要特别强调的是：</p>
<ul>
<li>模型 <strong>不执行函数</strong></li>
<li>模型 <strong>只输出调用意图与参数</strong></li>
<li>真正的执行发生在宿主程序（Backend / Server）中</li>
</ul>
<p>换句话说，模型负责“想”，系统负责“做”。</p>
<hr />
<h3><strong>2. 典型工作流程</strong></h3>
<pre><code>用户输入
↓
LLM 推理
↓
生成 function_name + arguments（JSON）
↓
宿主系统执行函数
↓
结果返回给 LLM
↓
最终自然语言回答
</code></pre>
<p>示例（简化）：</p>
<pre><code>{
  &quot;name&quot;: &quot;get_weather&quot;,
  &quot;arguments&quot;: {
    &quot;city&quot;: &quot;北京&quot;,
    &quot;date&quot;: &quot;今天&quot;
  }
}
</code></pre>
<hr />
<h3><strong>3. 为什么说 Function Calling 是一次重要跃迁？</strong></h3>
<p>因为它第一次让大模型具备了：</p>
<ul>
<li>与真实系统交互的能力</li>
<li>执行确定性任务的可能性</li>
<li>构建 Agent 的基础动作接口</li>
</ul>
<p>我们今天在 Coze、Dify 等低代码 Agent 平台中看到的各种“插件”，本质上都是 <strong>Function Calling 的工程封装</strong>。</p>
<hr />
<h2><strong>三、Function Calling 的工程代价与瓶颈</strong></h2>
<p>Function Calling 虽然重要，但它从一开始就<strong>不是为大规模系统设计的标准</strong>。</p>
<h3><strong>1. 模型强依赖</strong></h3>
<ul>
<li>模型必须 <strong>原生支持 Function Calling</strong></li>
<li>甚至需要 <strong>专门的 Function Call 微调</strong></li>
</ul>
<p>例如在 ShareGPT 风格的数据集中，会出现专门的结构字段：</p>
<pre><code>{
  &quot;from&quot;: &quot;function_call&quot;,
  &quot;value&quot;: &quot;工具参数&quot;
}
</code></pre>
<p>这意味着：</p>
<blockquote>
<p><strong>不是所有模型天然就“会用工具”。</strong></p>
</blockquote>
<hr />
<h3><strong>2. 协议碎片化问题</strong></h3>
<p>不同模型、不同厂商的 Function Calling：</p>
<ul>
<li>Schema 不统一</li>
<li>调用格式不同</li>
<li>错误处理方式各异</li>
</ul>
<p>工程上，几乎不可避免地需要为每个模型维护一套适配层。</p>
<hr />
<h3><strong>3. 能力边界清晰但狭窄</strong></h3>
<p>Function Calling 更像是：</p>
<ul>
<li>单次</li>
<li>无状态</li>
<li>函数级别</li>
</ul>
<p>它非常适合：</p>
<ul>
<li>查天气</li>
<li>查订单</li>
<li>简单计算</li>
</ul>
<p>但一旦进入：</p>
<ul>
<li>多工具协作</li>
<li>状态管理</li>
<li>复杂资源访问</li>
</ul>
<p>就会迅速变得笨重。</p>
<hr />
<h2><strong>四、MCP 出现的背景：Function Calling 不够用了</strong></h2>
<h3><strong>1. MCP 是什么？</strong></h3>
<p><strong>MCP（Model Context Protocol）</strong> 是 Anthropic 提出的一种开放标准协议，其目标不是“再发明一种函数调用”，而是：</p>
<blockquote>
<p><strong>为大模型与外部世界之间，建立一个标准化、可控、可扩展的通信层。</strong></p>
</blockquote>
<p>一句话定义：</p>
<blockquote>
<p>MCP 是模型接入工具、数据和能力的“通用协议层”。</p>
</blockquote>
<hr />
<h3><strong>2. MCP 要解决的核心问题</strong></h3>
<p>Function Calling 无法优雅解决的问题包括：</p>
<ul>
<li>工具数量爆炸</li>
<li>数据源类型复杂（DB / 文件 / API / Git）</li>
<li>不同语言、不同部署环境</li>
<li>权限与安全边界不清晰</li>
<li>状态与上下文难以管理</li>
</ul>
<p>MCP 的目标非常明确：</p>
<blockquote>
<p><strong>把“外部能力”系统性地包装成模型可理解、可控制的上下文接口。</strong></p>
</blockquote>
<hr />
<h2><strong>五、MCP 的标准架构（关键）</strong></h2>
<p>MCP 采用典型的 <strong>Client–Server 架构</strong>：</p>
<pre><code>LLM（Client）
   ↕ MCP Protocol
MCP Server
   ├─ Tools
   ├─ Resources
   └─ Prompts
</code></pre>
<h3><strong>MCP Server 提供三类能力</strong></h3>
<h4><strong>1️⃣ Tools（可执行能力）</strong></h4>
<ul>
<li>标准化工具接口</li>
<li>明确输入 / 输出 schema</li>
<li>受控权限</li>
</ul>
<h4><strong>2️⃣ Resources（数据资源）</strong></h4>
<ul>
<li>文件系统</li>
<li>数据库</li>
<li>API 结果</li>
<li>Git 仓库</li>
<li>本地或远程状态</li>
</ul>
<p>模型可以“读资源”，而不是每次都靠 prompt 注入。</p>
<h4><strong>3️⃣ Prompts（上下文模板）</strong></h4>
<ul>
<li>系统级规范</li>
<li>业务约束</li>
<li>任务模板</li>
</ul>
<hr />
<h2><strong>六、Function Calling vs MCP：本质对比</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>Function Calling</strong></th>
<th><strong>MCP</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>抽象层级</td>
<td>函数级</td>
<td>系统级</td>
</tr>
<tr>
<td>本质</td>
<td>模型生成调用参数</td>
<td>模型接入外部世界的协议</td>
</tr>
<tr>
<td>状态管理</td>
<td>无</td>
<td>支持（Server 维护）</td>
</tr>
<tr>
<td>工具规模</td>
<td>少量</td>
<td>大规模</td>
</tr>
<tr>
<td>协议标准</td>
<td>私有实现</td>
<td>开放标准</td>
</tr>
<tr>
<td>扩展性</td>
<td>差</td>
<td>极强</td>
</tr>
<tr>
<td>适用场景</td>
<td>Demo / 小工具</td>
<td>Agent / 企业系统</td>
</tr>
</tbody></table><hr />
<h2><strong>七、一个非常直观的类比</strong></h2>
<ul>
<li>
<p><strong>Function Calling</strong>：</p>
<p>像是你打电话让秘书临时帮你办一件事</p>
</li>
<li>
<p><strong>MCP</strong>：</p>
<p>像是你被授予了一整套办公系统的受控访问权限</p>
</li>
</ul>
<p>前者是“临时动作”，后者是“长期协作”。</p>
<hr />
<h2><strong>八、工程选型建议（非常重要）</strong></h2>
<h3><strong>什么时候用 Function Calling？</strong></h3>
<ul>
<li>工具数量少（&lt;10）</li>
<li>功能简单</li>
<li>单体应用</li>
<li>快速验证</li>
</ul>
<p>👉 <strong>90% 的聊天机器人 Demo</strong></p>
<hr />
<h3><strong>什么时候必须用 MCP？</strong></h3>
<ul>
<li>多工具、多数据源</li>
<li>AI Agent / Copilot</li>
<li>企业级系统</li>
<li>本地文件 / 内网服务</li>
<li>强安全与可扩展要求</li>
</ul>
<p>👉 <strong>真正可落地的 Agent 系统</strong></p>
<hr />
<h2><strong>九、总结一句话</strong></h2>
<blockquote>
<p>Function Calling 解决的是“模型怎么调用一个函数”，</p>
</blockquote>
<blockquote>
<p>MCP 解决的是“模型如何安全、可扩展地接入真实世界”。</p>
</blockquote>
<p>在 Agent 体系中：</p>
<ul>
<li>Function Calling 更像是“动作接口”</li>
<li>MCP 更接近“模型的操作系统层”</li>
</ul>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/2"/><category term="MCP"/><published>2025-12-13T06:24:38+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/1</id><title>从 RAG 到 GraphRAG，再到 LightRAG</title><updated>2025-12-13T06:58:08.363855+00:00</updated><content type="html"><![CDATA[<h2><strong>引言：当 RAG 不再只是“找得到”</strong></h2>
<p>检索增强生成（Retrieval-Augmented Generation, RAG）已经成为 LLM 应用的事实标准。</p>
<p>它通过“<strong>先检索、再生成</strong>”的方式，在一定程度上缓解了大模型的幻觉问题。</p>
<p>但随着应用场景从<strong>局部事实问答</strong>走向<strong>跨文档推理、全局总结与复杂关系理解</strong>，一个问题逐渐显现：</p>
<blockquote>
<p><strong>RAG 能检索到信息，但并不真正“理解结构”。</strong></p>
</blockquote>
<p>GraphRAG 与 LightRAG，正是在这一背景下被提出的两种结构化检索方案。</p>
<hr />
<h2><strong>一、传统 RAG 的结构性瓶颈</strong></h2>
<p>传统 RAG 的系统抽象非常清晰：</p>
<ul>
<li>文档 → Chunk</li>
<li>Chunk → 向量</li>
<li>Query → 相似度检索 → 上下文拼接</li>
</ul>
<p>这种设计在以下任务中表现良好：</p>
<ul>
<li>明确事实查询</li>
<li>定位具体段落</li>
<li>局部知识补全</li>
</ul>
<p>但它存在一个<strong>无法通过调参或模型升级解决的系统性限制</strong>：</p>
<blockquote>
<p><strong>Chunk 是扁平的，而知识本身是结构化的。</strong></p>
</blockquote>
<h3><strong>结构缺失带来的直接后果</strong></h3>
<ul>
<li>文档之间的关联关系被切断</li>
<li>实体之间的因果、从属、演化关系无法显式建模</li>
<li>模型只能“拼上下文”，而非“基于结构推理”</li>
</ul>
<p>这意味着，当问题本身涉及<strong>整体脉络、跨文档关系或宏观总结</strong>时，传统 RAG 的能力上限会迅速暴露。</p>
<hr />
<h2><strong>二、GraphRAG 是什么：第一次把“图结构”带入 RAG</strong></h2>
<p><strong>GraphRAG</strong> 是微软提出的一种 RAG 扩展框架，其核心目标并不是“检索更准的文本”，</p>
<p>而是：</p>
<blockquote>
<p><strong>让 RAG 具备显式建模和利用“知识结构”的能力。</strong></p>
</blockquote>
<p>在 GraphRAG 中，知识不再被视为一堆独立的文本片段，而是被重构为一个<strong>由实体与关系组成的图结构（Knowledge Graph）</strong>。</p>
<p>RAG 的检索对象，也从“文本 Chunk”升级为“图中的结构单元”。</p>
<hr />
<h2><strong>三、GraphRAG：显式引入“全局结构”的系统设计</strong></h2>
<h3><strong>1. GraphRAG 的核心设计思路</strong></h3>
<p>GraphRAG 的关键思想可以概括为一句话：</p>
<blockquote>
<p><strong>既然知识天然是图结构的，那就直接构建图，并在图的层面进行检索与摘要。</strong></p>
</blockquote>
<p>为此，GraphRAG 在 indexing 阶段引入了一套完整但代价不菲的流程：</p>
<ol>
<li>
<p><strong>实体与关系抽取</strong></p>
<p>使用 LLM 从原始文本中抽取实体（人物、概念、事件）及其关系。</p>
</li>
<li>
<p><strong>知识图谱构建</strong></p>
<p>将实体作为节点，关系作为边，形成全局图结构。</p>
</li>
<li>
<p><strong>社区发现（Community Detection）</strong></p>
<p>使用 Leiden 等图算法，将高度相关的子图聚合为“社区”。</p>
</li>
<li>
<p><strong>社区级摘要生成</strong></p>
<p>为每个社区预先生成结构化摘要，作为后续检索的高层语义单元。</p>
</li>
</ol>
<hr />
<h3><strong>2. GraphRAG 解决了什么本质问题？</strong></h3>
<p>GraphRAG 首次在 RAG 框架中引入了<strong>全局结构感知能力</strong>：</p>
<ul>
<li>不再只关注“相关段落”</li>
<li>而是能够回答“整体结构上发生了什么”</li>
</ul>
<p>例如：</p>
<ul>
<li>“整个语料中反复出现的核心主题是什么？”</li>
<li>“多个事件之间是否存在系统性关联？”</li>
</ul>
<p>这类问题，本质上需要<strong>跨文本、跨实体、跨关系的全局推理</strong>，</p>
<p>而这正是 GraphRAG 的优势所在。</p>
<hr />
<h3><strong>3. 但系统代价同样明确</strong></h3>
<p>从工程角度看，GraphRAG 的主要问题并不在效果，而在<strong>系统可持续性</strong>：</p>
<ul>
<li>索引阶段 Token 成本极高</li>
<li>社区结构对数据变化高度敏感</li>
<li>增量更新几乎不可行</li>
</ul>
<p>因此，GraphRAG 更适合<strong>离线分析型任务</strong>，而非高频更新的在线系统。</p>
<hr />
<h2><strong>四、LightRAG 是什么：对 GraphRAG 的工程化回应</strong></h2>
<p><strong>LightRAG</strong> 并不是对 GraphRAG 的否定，而是一种<strong>面向工程现实的重新设计</strong>。</p>
<p>它试图回答的问题是：</p>
<blockquote>
<p><strong>是否一定要构建“全局静态结构”，才能让 RAG 具备结构化推理能力？</strong></p>
</blockquote>
<p>LightRAG 的答案是：<strong>不一定。</strong></p>
<hr />
<h2><strong>五、LightRAG：把结构留到 Query 时再用</strong></h2>
<h3><strong>1. 设计立场的根本转变</strong></h3>
<p>与 GraphRAG 相比，LightRAG 的核心转变在于：</p>
<ul>
<li>不再预生成全局社区摘要</li>
<li>保留图结构，但不“冻结”结构语义</li>
<li>将结构的使用推迟到 Query 阶段</li>
</ul>
<p>可以将两者的差异理解为：</p>
<blockquote>
<p>GraphRAG：<strong>结构是提前计算好的</strong></p>
</blockquote>
<blockquote>
<p>LightRAG：<strong>结构是按需被查询和展开的</strong></p>
</blockquote>
<hr />
<h3><strong>2. 双层检索的工程意义</strong></h3>
<p>LightRAG 采用了一种<strong>图结构 + 向量检索的协同机制</strong>：</p>
<ul>
<li><strong>向量层</strong>：快速召回相关实体或节点</li>
<li><strong>图结构层</strong>：基于关系进行上下文扩展与约束</li>
</ul>
<p>这种设计带来的直接好处是：</p>
<ul>
<li>支持增量更新</li>
<li>检索路径随 Query 动态变化</li>
<li>成本与延迟显著降低</li>
</ul>
<hr />
<h2><strong>六、三种 RAG 方案的系统级对比</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>传统 RAG</strong></th>
<th><strong>GraphRAG</strong></th>
<th><strong>LightRAG</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>知识建模</td>
<td>扁平文本</td>
<td>显式全局图</td>
<td>局部图 + 向量</td>
</tr>
<tr>
<td>全局理解</td>
<td>❌</td>
<td>✅</td>
<td>部分支持</td>
</tr>
<tr>
<td>索引成本</td>
<td>低</td>
<td>极高</td>
<td>中等</td>
</tr>
<tr>
<td>增量更新</td>
<td>易</td>
<td>困难</td>
<td>易</td>
</tr>
<tr>
<td>工程可用性</td>
<td>高</td>
<td>低</td>
<td>高</td>
</tr>
</tbody></table><hr />
<h2><strong>七、结论：Graph 不是目的，结构化推理才是</strong></h2>
<p>GraphRAG 与 LightRAG 的分歧，并不在“用不用图”，而在于：</p>
<blockquote>
<p><strong>结构化能力应当放在“离线建模”还是“在线推理”阶段。</strong></p>
</blockquote>
<ul>
<li>GraphRAG 代表<strong>认知完整性优先</strong></li>
<li>LightRAG 代表<strong>工程可落地性优先</strong></li>
</ul>
<p>在可预见的未来，更可能的方向是：</p>
<blockquote>
<p><strong>图结构 × 向量检索 × 动态推理路径的融合系统</strong></p>
</blockquote>
<hr />
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/1"/><category term="RAG"/><published>2025-12-13T05:46:09+00:00</published></entry></feed>