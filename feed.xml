<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>https://github.com/lihe/MyGitBlog</id><title>RSS feed of lihe's MyGitBlog</title><updated>2025-12-16T03:28:14.019715+00:00</updated><link href="https://github.com/lihe/MyGitBlog"/><link href="https://raw.githubusercontent.com/lihe/MyGitBlog/master/feed.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><entry><id>https://github.com/lihe/MyGitBlog/issues/14</id><title>WordPiece Tokenization：从概率视角理解子词分词算法</title><updated>2025-12-16T03:28:14.465455+00:00</updated><content type="html"><![CDATA[<p>在大语言模型出现之前，<strong>Tokenizer 的设计</strong>就已经深刻影响着模型的上限。</p>
<p>WordPiece，正是子词分词算法发展史中一个<strong>非常关键、但常被简化理解的节点</strong>。</p>
<p>它不仅仅是“BERT 用的分词器”，而是一种<strong>明确以语言模型概率为目标函数的子词学习方法</strong>。</p>
<hr />
<h2><strong>一、为什么需要 WordPiece？</strong></h2>
<p>早期 NLP 分词方案存在明显矛盾：</p>
<ul>
<li>
<p><strong>按词分词（word-level）</strong></p>
<ul>
<li>词表巨大</li>
<li>OOV（未登录词）问题严重</li>
</ul>
</li>
<li>
<p><strong>按字分词（char-level）</strong></p>
<ul>
<li>序列极长</li>
<li>语义信息稀薄</li>
</ul>
</li>
</ul>
<p>于是诞生了折中方案：</p>
<blockquote>
<p><strong>子词分词（Subword Tokenization）</strong></p>
</blockquote>
<p>WordPiece、BPE、Unigram 都属于这一范式，但它们的<strong>优化目标并不相同</strong>。</p>
<hr />
<h2><strong>二、WordPiece 的核心思想</strong></h2>
<p>WordPiece 的核心不是“哪个片段最常见”，而是：</p>
<blockquote>
<p><strong>哪个子词合并，能最大化训练语料的语言模型似然。</strong></p>
</blockquote>
<p>也就是说，它从一开始就站在了<strong>概率建模</strong>而不是<strong>频率统计</strong>的角度。</p>
<p>这是它与 BPE 的根本区别。</p>
<hr />
<h2><strong>三、WordPiece 的概率视角：在优化什么？</strong></h2>
<p>WordPiece 假设一个 <strong>Unigram 语言模型</strong>：</p>
<ul>
<li>每个子词独立出现</li>
<li>语料的整体概率 = 所有子词概率的乘积</li>
</ul>
<p>对应的对数似然为：</p>
<p>$\mathcal{L} = \sum_{s \in S} f(s)\log P(s)$</p>
<p>其中：</p>
<ul>
<li>S 是当前子词集合</li>
<li>f(s) 是子词在语料中的出现次数</li>
</ul>
<p>WordPiece 的训练目标是：</p>
<blockquote>
<p><strong>通过合并子词，使整体对数似然增加得最多。</strong></p>
</blockquote>
<hr />
<h2><strong>四、WordPiece 的训练流程</strong></h2>
<h3><strong>Step 1：初始化词表</strong></h3>
<ul>
<li>从字符级开始（a, b, c, …）</li>
</ul>
<h3><strong>Step 2：统计子词频率并估计概率</strong></h3>
<p>$P(s) = \frac{f(s)}{N}$</p>
<h3><strong>Step 3：评估所有可能的合并候选</strong></h3>
<p>对任意相邻子词 a, b，计算合并收益：</p>
<p>$\Delta \mathcal{L} = f(ab)\log P(ab) - f(a)\log P(a) - f(b)\log P(b)$</p>
<h3><strong>Step 4：选择</strong></h3>
<h3><strong>ΔL 最大</strong></h3>
<h3><strong>的合并</strong></h3>
<ul>
<li>加入新子词</li>
<li>更新频率和概率</li>
</ul>
<h3><strong>Step 5：重复</strong></h3>
<p>直到达到预设词表大小（如 30k）</p>
<blockquote>
<p>这一步解释了：</p>
</blockquote>
<blockquote>
<p><strong>WordPiece 训练慢，但“合并更有语义意义”。</strong></p>
</blockquote>
<hr />
<h2><strong>五、一个典型示例（非常好理解）</strong></h2>
<p>语料包含：</p>
<ul>
<li>unaffable</li>
<li>unacceptable</li>
<li>unaccountable</li>
</ul>
<p>在训练过程中：</p>
<ul>
<li>u + n → un 会频繁提升整体似然</li>
<li>un + able 比随机字符组合更“划算”</li>
</ul>
<p>最终学到的子词往往是：</p>
<pre><code>un, able, account, ##able
</code></pre>
<p>这些片段<strong>具有稳定的语言学意义</strong>，而不仅仅是高频。</p>
<hr />
<h2><strong>六、WordPiece 的一个标志性设计：##</strong></h2>
<p>在实际分词阶段，WordPiece 使用：</p>
<ul>
<li><strong>最长匹配优先</strong></li>
<li>用 ## 标记“非词首子词”</li>
</ul>
<p>例如：</p>
<pre><code>playing → play + ##ing
</code></pre>
<p>##ing 表示：</p>
<blockquote>
<p>“这个子词不能独立作为词首出现”</p>
</blockquote>
<p>这让模型能够区分：</p>
<ul>
<li>play</li>
<li>display</li>
<li>playing</li>
</ul>
<p>在语义空间中的关系。</p>
<hr />
<h2><strong>七、WordPiece vs BPE vs Unigram</strong></h2>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>WordPiece</strong></th>
<th><strong>BPE</strong></th>
<th><strong>Unigram</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>优化目标</td>
<td>最大化似然</td>
<td>合并最高频对</td>
<td>最小化整体 loss</td>
</tr>
<tr>
<td>核心策略</td>
<td>概率增益</td>
<td>贪心频率</td>
<td>从大词表删减</td>
</tr>
<tr>
<td>是否有 ##</td>
<td>✅</td>
<td>❌</td>
<td>❌</td>
</tr>
<tr>
<td>训练复杂度</td>
<td>中</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>代表模型</td>
<td>BERT</td>
<td>GPT / RoBERTa</td>
<td>SentencePiece</td>
</tr>
</tbody></table><p>一句话区分：</p>
<ul>
<li><strong>BPE</strong>：谁最常一起出现就合谁</li>
<li><strong>WordPiece</strong>：谁让语言模型“更合理”就合谁</li>
<li><strong>Unigram</strong>：先给你所有可能，再慢慢删到最优</li>
</ul>
<hr />
<h2><strong>八、WordPiece 的优势与局限</strong></h2>
<h3><strong>优势</strong></h3>
<ul>
<li>显著减少 OOV</li>
<li>子词语义更稳定</li>
<li>英文等拼写语言表现好</li>
<li>非常适合 BERT 这类 Masked LM</li>
</ul>
<h3><strong>局限</strong></h3>
<ul>
<li>训练复杂、计算量大</li>
<li>对中文等无空格语言不友好</li>
<li>工程灵活性不如 SentencePiece</li>
<li>已不适合超大规模 LLM</li>
</ul>
<p>这也是为什么：</p>
<ul>
<li><strong>BERT 系列</strong> → WordPiece</li>
<li><strong>GPT / LLaMA / Qwen</strong> → BPE / SentencePiece</li>
</ul>
<hr />
<h2><strong>九、为什么 WordPiece 逐渐“退居二线”？</strong></h2>
<p>不是因为它不好，而是因为：</p>
<ul>
<li>SentencePiece 语言无关</li>
<li>Unigram 全局最优</li>
<li>BPE 工程实现更简单、可扩展</li>
</ul>
<p>但在理解 <strong>Tokenizer 与语言模型之间的关系</strong>时，</p>
<p>WordPiece 依然是<strong>最好的教学级算法之一</strong>。</p>
<hr />
<h2><strong>最终总结</strong></h2>
<p>WordPiece 是一种基于语言模型概率的子词分词算法，通过在训练阶段选择能最大化语料整体似然的子词合并方式，在词表规模与 OOV 问题之间取得平衡。它生成的子词通常具有更稳定的语言学意义，被广泛应用于 BERT 等模型中。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/14"/><published>2025-12-16T03:27:46+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/13</id><title>RMSNorm：为什么大语言模型开始放弃 LayerNorm？</title><updated>2025-12-16T03:28:14.625971+00:00</updated><content type="html"><![CDATA[<h2><strong>一句话总结</strong></h2>
<p><strong>RMSNorm 是一种不做均值中心化、只按均方根缩放特征向量的归一化方法，相比 LayerNorm 计算更简单、参数更少、数值更稳定，因此在深层 Transformer 和大语言模型中更合适。</strong></p>
<p>关键词：</p>
<ul>
<li>不减均值</li>
<li>只控制尺度</li>
<li>更高效、更稳定</li>
</ul>
<hr />
<h2><strong>一、为什么我们一开始需要 LayerNorm？</strong></h2>
<p>LayerNorm 的初衷很明确：</p>
<p><strong>稳定深层网络的训练。</strong></p>
<p>给定一个 token 的隐藏状态向量：</p>
<p>$x \in \mathbb{R}^d$</p>
<p>LayerNorm 做了四件事：</p>
<ol>
<li>计算均值</li>
<li>计算方差</li>
<li>做标准化（零均值、单位方差）</li>
<li>再通过可学习参数缩放和平移</li>
</ol>
<p>其形式是：</p>
<p>$\text{LN}(x) = \frac{x - \mu}{\sigma} \cdot \gamma + \beta$</p>
<p>在早期 Transformer 中，这是一个<strong>非常稳妥</strong>的选择。</p>
<p>但问题是：</p>
<blockquote>
<p><strong>在大模型中，这一步真的“必要”吗？</strong></p>
</blockquote>
<hr />
<h2><strong>二、RMSNorm 的核心思想：只做一件真正重要的事</strong></h2>
<p>RMSNorm 的设计非常克制。</p>
<p>它只关心一件事：</p>
<p><strong>控制向量的整体尺度（magnitude）</strong>。</p>
<h3><strong>数学定义</strong></h3>
<p>给定同样的输入向量：</p>
<p>$x = (x_1, x_2, \dots, x_d)$</p>
<p>先计算均方根（RMS）：</p>
<p>$\text{RMS}(x) = \sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2}$</p>
<p>然后做归一化：</p>
<p>$\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \cdot g$</p>
<p>其中：</p>
<ul>
<li>g 是可学习的缩放参数</li>
<li><strong>没有减均值</strong></li>
<li><strong>没有偏置项</strong></li>
</ul>
<hr />
<h2><strong>三、RMSNorm vs LayerNorm：真正的结构差异</strong></h2>
<img width="870" height="914" alt="Image" src="https://github.com/user-attachments/assets/d424507e-fd5e-45e2-bae0-852f99e80ce6" />
<hr />
<h2><strong>四、为什么“不减均值”在 Transformer 里没问题？</strong></h2>
<p>这是理解 RMSNorm 的关键。</p>
<p>在 Transformer 中：</p>
<ul>
<li>信息主要编码在<strong>向量方向</strong></li>
<li>Attention 和 FFN 对 <strong>scale 非常敏感</strong></li>
<li>对 <strong>绝对均值并不敏感</strong></li>
<li>Residual Connection 会不断引入偏移，均值本身就会漂移</li>
</ul>
<p>也就是说：</p>
<blockquote>
<p><strong>强行把均值拉回 0，并不是一个必要约束。</strong></p>
</blockquote>
<p>真正重要的是：</p>
<ul>
<li>不要数值爆炸</li>
<li>不要梯度失控</li>
<li>不要在深层累积不稳定</li>
</ul>
<p>RMSNorm 只控制一件事：</p>
<p><strong>“这个向量不要太大，也不要太小。”</strong></p>
<p>这恰好是 Transformer 最需要的。</p>
<hr />
<h2><strong>五、为什么 RMSNorm 在大模型中特别合适？</strong></h2>
<h3><strong>1️⃣ 计算更省（工程价值极高）</strong></h3>
<ul>
<li>少一次均值计算</li>
<li>少一次减法</li>
<li>少一个 β 参数</li>
<li>hidden size 越大，收益越明显</li>
</ul>
<p>在 4k / 8k hidden size 的 LLM 中，这不是微优化，而是<strong>实打实的吞吐差异</strong>。</p>
<hr />
<h3><strong>2️⃣ 梯度传播更稳定</strong></h3>
<p>LayerNorm 的均值和方差是动态统计量：</p>
<ul>
<li>深层模型中容易抖动</li>
<li>多层叠加后可能出现过度平滑</li>
</ul>
<p>RMSNorm 不强制中心化：</p>
<ul>
<li>梯度路径更简单</li>
<li>数值行为更可控</li>
<li>对深层网络更友好</li>
</ul>
<hr />
<h3><strong>3️⃣ 与 Pre-LN Transformer 完美契合</strong></h3>
<p>现代 LLM 基本都是：</p>
<pre><code>x → Norm → Attention → + x
x → Norm → FFN → + x
</code></pre>
<p>在这种 <strong>Pre-LN 架构</strong> 中：</p>
<ul>
<li>Norm 的作用是“数值保护”</li>
<li>而不是“特征重构”</li>
</ul>
<p>RMSNorm 刚好满足这一角色。</p>
<hr />
<h2><strong>六、哪些主流模型在用 RMSNorm？</strong></h2>
<p>这已经不是实验选择，而是<strong>事实标准</strong>：</p>
<ul>
<li>LLaMA / LLaMA 2 / LLaMA 3</li>
<li>Qwen / Qwen2</li>
<li>Mistral</li>
<li>Gemma</li>
<li>GPT-NeoX 系列</li>
</ul>
<p>你在面试中提到这些名字，<strong>是明显加分项</strong>。</p>
<hr />
<h2><strong>七、RMSNorm 的代价是什么？</strong></h2>
<ul>
<li>理论上表达能力略弱于 LayerNorm</li>
<li>在小模型或某些特殊任务中未必最优</li>
<li>对“强分布标准化”依赖的场景不合适</li>
</ul>
<p>但在 <strong>大语言模型</strong> 中：</p>
<blockquote>
<p><strong>稳定性和效率的收益，远大于这点理论损失。</strong></p>
</blockquote>
<hr />
<h2><strong>一个好记的直觉类比</strong></h2>
<ul>
<li>
<p><strong>LayerNorm</strong>：</p>
<p>把数据“搬到原点，再缩放”</p>
</li>
<li>
<p><strong>RMSNorm</strong>：</p>
<p>不管你在哪，只控制“别跑太远”</p>
</li>
</ul>
<p>在 LLM 里，我们更关心的是：</p>
<blockquote>
<p><strong>“别炸”</strong>，而不是“居中”。</p>
</blockquote>
<hr />
<h2><strong>最终总结</strong></h2>
<p>RMSNorm 是一种简化的归一化方法，只通过均方根对特征向量进行尺度归一化，不进行均值中心化。相比 LayerNorm，它计算更高效、参数更少，并在深层 Transformer 和大语言模型中表现出更好的数值稳定性，因此逐渐成为 LLM 架构中的标准选择。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/13"/><category term="Transformer"/><published>2025-12-16T02:35:23+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/12</id><title>vLLM：为大模型在线推理而生的高性能引擎</title><updated>2025-12-16T03:28:14.764963+00:00</updated><content type="html"><![CDATA[<p>在真实的线上大模型服务中，推理瓶颈往往并不来自模型算力，而是来自<strong>显存管理与并发调度</strong>。</p>
<p>vLLM 正是为解决这一问题而诞生的推理引擎。</p>
<p>它并不是对 Transformer 结构的改写，而是一次<strong>面向系统层的工程重构</strong>。</p>
<hr />
<h2><strong>一、vLLM 要解决的核心问题</strong></h2>
<p>在大规模 LLM 在线服务中，推理面临几个根本性挑战：</p>
<ul>
<li>
<p><strong>KV Cache 占用巨大</strong></p>
<p>KV Cache 随上下文长度线性增长，显存压力极高</p>
</li>
<li>
<p><strong>请求长度高度不一致</strong></p>
<p>长请求会拖垮短请求，batch 极易被打散</p>
</li>
<li>
<p><strong>显存碎片严重</strong></p>
<p>连续分配 KV Cache 容易导致 OOM</p>
</li>
<li>
<p><strong>并发吞吐与 TTFT 难以兼顾</strong></p>
<p>首 token 延迟（TTFT）与整体吞吐存在冲突</p>
</li>
</ul>
<p>传统 transformers.generate() 的设计假设是：</p>
<blockquote>
<p>单请求或静态 batch</p>
</blockquote>
<p>这在实验环境中可行，但在高并发线上服务中几乎不可用。</p>
<p>vLLM 的目标很明确：</p>
<blockquote>
<p><strong>在显存受限条件下，实现高并发、低延迟、高吞吐的大模型推理。</strong></p>
</blockquote>
<hr />
<h2><strong>二、vLLM 的核心技术：Paged KV Cache（PagedAttention）</strong></h2>
<h3><strong>1. 问题本质：KV Cache 的连续内存假设</strong></h3>
<p>在标准自回归推理中：</p>
<ul>
<li>每个请求需要一块<strong>逻辑连续</strong>的 KV Cache</li>
<li>KV Cache 随 token 生成不断增长</li>
<li>GPU 显存需要提前或动态分配大块连续空间</li>
</ul>
<p>结果是：</p>
<ul>
<li>内存碎片严重</li>
<li>长请求极易导致 OOM</li>
<li>并发请求之间难以共存</li>
</ul>
<hr />
<h3><strong>2. vLLM 的核心思想：像操作系统一样管理显存</strong></h3>
<p>vLLM 引入 <strong>PagedAttention</strong>，其设计理念直接来自操作系统的<strong>虚拟内存机制</strong>：</p>
<ul>
<li>将 KV Cache 划分为固定大小的 <strong>page（block）</strong></li>
<li>每个 page 存储固定数量的 token</li>
<li>请求的 KV Cache 在逻辑上是连续的</li>
<li>在物理显存中是<strong>非连续的</strong></li>
</ul>
<p>通过 <strong>page table</strong> 建立逻辑序列到物理 page 的映射关系。</p>
<hr />
<h3><strong>3. PagedAttention 带来的直接收益</strong></h3>
<ul>
<li>不再需要连续显存分配</li>
<li>显存碎片大幅减少</li>
<li>KV Cache 可动态分配、回收、复用</li>
<li>长短请求可以安全共存</li>
<li>并发请求规模显著提升</li>
</ul>
<blockquote>
<p><strong>vLLM 的吞吐优势，核心来自 Paged KV Cache，而不是单纯的算子加速。</strong></p>
</blockquote>
<hr />
<h2><strong>三、第二个关键技术：Continuous Batching（连续批处理）</strong></h2>
<img width="888" height="490" alt="Image" src="https://github.com/user-attachments/assets/31720f3c-cdaa-49c4-a549-f5030502ab8e" />
<h3><strong>1. 传统 Static Batching 的问题</strong></h3>
<p>传统 batching 方式要求：</p>
<ul>
<li>请求同时到达</li>
<li>序列长度对齐</li>
<li>batch 在整个生成过程中保持不变</li>
</ul>
<p>现实中这几乎不成立：</p>
<ul>
<li>有的请求很快结束</li>
<li>有的请求生成很长</li>
<li>GPU 计算资源被频繁浪费</li>
</ul>
<hr />
<h3><strong>2. vLLM 的解法：每一步动态重组 batch</strong></h3>
<p>vLLM 使用 <strong>Continuous Batching</strong>：</p>
<ul>
<li>每一个 decoding step 都重新构建 batch</li>
<li>新请求可以随时加入</li>
<li>已完成请求立即移除</li>
<li>batch 始终由“当前活跃请求”组成</li>
</ul>
<p>配合 KV Cache：</p>
<ul>
<li>Q 的计算可批量执行</li>
<li>K / V 从各自的 page cache 中读取</li>
</ul>
<hr />
<h3><strong>3. Continuous Batching 的效果</strong></h3>
<ul>
<li>GPU 利用率显著提高</li>
<li>TTFT（首 token 延迟）降低</li>
<li>并发吞吐最大化</li>
<li>不再被最慢请求拖累</li>
</ul>
<hr />
<h2><strong>四、Paged KV Cache × Continuous Batching 的协同效应</strong></h2>
<p>这两项技术是<strong>强耦合设计</strong>：</p>
<ul>
<li>
<p>Paged KV Cache：</p>
<ul>
<li>解决“显存怎么分配”</li>
</ul>
</li>
<li>
<p>Continuous Batching：</p>
<ul>
<li>解决“算力怎么用满”</li>
</ul>
</li>
</ul>
<p>只有两者结合，才能真正支撑：</p>
<blockquote>
<p><strong>高并发 + 长上下文 + 低延迟</strong></p>
</blockquote>
<p>这也是 vLLM 与传统推理框架的本质分界线。</p>
<hr />
<h2><strong>五、vLLM 的完整推理工作流程</strong></h2>
<p>一个请求在 vLLM 中的生命周期如下：</p>
<ol>
<li>
<p>请求到达 → 调度器接管</p>
</li>
<li>
<p>为该请求分配 KV Cache pages</p>
</li>
<li>
<p>请求进入活跃池</p>
</li>
<li>
<p>每一个 decoding step：</p>
<ul>
<li>从活跃请求中动态组 batch</li>
<li>从各自 page 中读取 KV Cache</li>
<li>计算下一个 token</li>
</ul>
</li>
<li>
<p>请求结束：</p>
<ul>
<li>释放其 KV Cache pages</li>
<li>立刻调度新的请求进入</li>
</ul>
</li>
</ol>
<p>整个过程没有“空 batch”，也没有“显存僵尸”。</p>
<hr />
<h2><strong>六、vLLM 的其他关键特性（工程加分点）</strong></h2>
<ul>
<li>
<p><strong>支持 MQA / GQA</strong></p>
<ul>
<li>从模型结构层减少 KV Cache 体积</li>
</ul>
</li>
<li>
<p><strong>FlashAttention 集成</strong></p>
<ul>
<li>优化 attention 计算本身</li>
</ul>
</li>
<li>
<p><strong>Tensor Parallelism</strong></p>
<ul>
<li>将模型权重切分到多 GPU</li>
</ul>
</li>
<li>
<p><strong>流式输出</strong></p>
<ul>
<li>支持 token 级别返回，降低感知延迟</li>
</ul>
</li>
<li>
<p><strong>OpenAI-compatible API</strong></p>
<ul>
<li>可直接替换现有服务</li>
</ul>
</li>
<li>
<p><strong>HuggingFace 模型兼容</strong></p>
<ul>
<li>无需重新训练或改模型结构</li>
</ul>
</li>
</ul>
<hr />
<h2><strong>七、vLLM vs HuggingFace Transformers</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>HF Transformers</strong></th>
<th><strong>vLLM</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>KV Cache 管理</td>
<td>连续分配</td>
<td>分页管理</td>
</tr>
<tr>
<td>Batching</td>
<td>静态</td>
<td>连续动态</td>
</tr>
<tr>
<td>并发能力</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>TTFT</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>显存利用率</td>
<td>差</td>
<td>优</td>
</tr>
<tr>
<td>适用场景</td>
<td>实验 / 单请求</td>
<td>线上服务</td>
</tr>
</tbody></table><p>一句话总结：</p>
<blockquote>
<p><strong>HF 适合研究，vLLM 适合生产。</strong></p>
</blockquote>
<hr />
<h2><strong>八、vLLM 的代价与边界</strong></h2>
<p>vLLM 并不是万能的：</p>
<ul>
<li>专注于推理，不支持训练</li>
<li>自定义模型 / 算子需要适配</li>
<li>极端超长上下文仍受显存限制（需 offload）</li>
</ul>
<p>但在<strong>线上 LLM 服务场景</strong>，它几乎是当前的事实标准。</p>
<hr />
<h2><strong>总结</strong></h2>
<p>vLLM 是一个为大模型在线推理而设计的高性能引擎，其核心创新在于 <strong>Paged KV Cache</strong> 和 <strong>Continuous Batching</strong>。</p>
<p>它通过操作系统级的显存管理方式和动态调度机制，解决了传统 Transformer 推理在并发、显存碎片和吞吐上的根本瓶颈，显著提升了 GPU 利用率和系统整体性能。</p>
<p><strong>vLLM 的价值不在模型，而在系统。</strong></p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/12"/><category term="Transformer"/><published>2025-12-14T15:15:29+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/11</id><title>什么是 KV Cache？</title><updated>2025-12-16T03:28:14.931020+00:00</updated><content type="html"><![CDATA[<p>在大语言模型的推理阶段，**KV Cache（Key-Value Cache）**几乎是不可或缺的基础机制。无论是 GPT、LLaMA、Qwen，还是各类推理框架（Transformers、vLLM、TensorRT-LLM），KV Cache 都直接决定了生成速度、显存占用以及系统吞吐能力。</p>
<p>它并不是一个“模型结构创新”，而是一个<strong>严格由自回归推理范式推导出来的工程必然</strong>。</p>
<hr />
<h2><strong>一、KV Cache 要解决的根本问题</strong></h2>
<p>在 Transformer Decoder 或 GPT 类模型中，文本生成采用**自回归（autoregressive）**方式：</p>
<ul>
<li>每一步生成一个新 token</li>
<li>新 token 需要对 <strong>所有历史 token</strong> 做一次注意力计算</li>
</ul>
<p>在第 t 步生成时，注意力形式为：</p>
<p>$\text{Attention}(Q_t, K_{1:t}, V_{1:t})$</p>
<p>如果不做任何缓存，那么在第 t 步：</p>
<ul>
<li>前 t-1 个 token 的 Key / Value</li>
<li>会被<strong>重复计算一次</strong></li>
</ul>
<p>随着生成长度增加，这种重复会迅速成为性能瓶颈。</p>
<hr />
<h2><strong>二、KV Cache 的核心思想</strong></h2>
<p>KV Cache 的思想非常直接：</p>
<blockquote>
<p><strong>历史 token 的表示一旦算过，就永远不再变化，不需要重复计算。</strong></p>
</blockquote>
<p>在注意力中：</p>
<ul>
<li><strong>Query（Q）</strong>：来自当前 token，每一步都会变化</li>
<li><strong>Key（K）、Value（V）</strong>：来自历史 token，一旦生成就固定</li>
</ul>
<p>因此：</p>
<ul>
<li>只缓存 <strong>K / V</strong></li>
<li>不缓存 <strong>Q</strong></li>
</ul>
<p>这就是 KV Cache 的全部动机。</p>
<hr />
<h2><strong>三、KV Cache 的具体工作机制</strong></h2>
<h3><strong>1. 首次计算（Prefill 阶段）</strong></h3>
<p>当输入 prompt 时：</p>
<ul>
<li>对 prompt 中所有 token</li>
<li>在每一层 Transformer 中</li>
<li>计算完整的 Q / K / V</li>
</ul>
<p>随后：</p>
<ul>
<li>将所有层、所有 head 的 <strong>K / V 存入缓存</strong></li>
<li>这一步计算量大，属于 <strong>compute-bound</strong></li>
</ul>
<hr />
<h3><strong>2. 自回归生成（Decode 阶段）</strong></h3>
<p>从生成第一个新 token 开始，每一步：</p>
<ul>
<li>
<p>只对 <strong>新 token</strong> 计算：</p>
<ul>
<li>$Q_{\text{new}}$</li>
<li>$K_{\text{new}}$</li>
<li>$V_{\text{new}}$</li>
</ul>
</li>
<li>
<p>从缓存中取出：</p>
<ul>
<li>$K_{\text{old}}$</li>
<li>$V_{\text{old}}$</li>
</ul>
</li>
</ul>
<p>并进行注意力计算：</p>
<p>$\text{Attention}\bigl( Q_{\text{new}}, [K_{\text{old}}, K_{\text{new}}], [V_{\text{old}}, V_{\text{new}}] \bigr)$</p>
<p>随后：</p>
<ul>
<li>将 $K_{\text{new}}, V_{\text{new}}$ 追加到 KV Cache</li>
</ul>
<p>从这一刻起，<strong>历史 token 的 K / V 永远只计算一次</strong>。</p>
<hr />
<h2><strong>四、为什么 KV Cache 能显著加速推理</strong></h2>
<p>如果不使用 KV Cache：</p>
<ul>
<li>第 t 步需要重新计算 $1 \sim t-1$ 的 K / V</li>
<li>总体计算复杂度接近 $O(t^2)$</li>
</ul>
<p>使用 KV Cache 后：</p>
<ul>
<li>历史部分不再重复计算</li>
<li>每一步只新增常数级计算</li>
</ul>
<p>整体复杂度变为：</p>
<ul>
<li><strong>Prefill：一次性高开销</strong></li>
<li><strong>Decode：近似</strong> O(t)</li>
</ul>
<p>这在长文本生成（几千 token）时，会带来<strong>数量级的速度差异</strong>。</p>
<hr />
<h2><strong>五、KV Cache 的真实代价：显存</strong></h2>
<p>KV Cache 的本质是“用空间换时间”。</p>
<p>缓存内容包括：</p>
<ul>
<li>每一层</li>
<li>每一个 attention head</li>
<li>每一个历史 token</li>
<li>的 Key 和 Value</li>
</ul>
<p>显存占用规模近似为：</p>
<p>$\text{Layers} \times \text{Heads} \times \text{SeqLen} \times \text{HeadDim} \times 2$</p>
<p>因此：</p>
<ul>
<li>
<p>上下文越长，KV Cache 越大</p>
</li>
<li>
<p>在长上下文推理中：</p>
<ul>
<li><strong>KV Cache 往往比模型权重更吃显存</strong></li>
</ul>
</li>
</ul>
<p>这也是为什么很多系统在长上下文场景下会首先“显存吃满”。</p>
<hr />
<h2><strong>六、推理阶段的两个典型特征</strong></h2>
<h3><strong>1. Prefill vs Decode</strong></h3>
<p>KV Cache 将推理过程自然分成两个阶段：</p>
<ul>
<li>
<p><strong>Prefill 阶段</strong></p>
<ul>
<li>计算密集型（GEMM 多）</li>
<li>吞吐受算力限制</li>
</ul>
</li>
<li>
<p><strong>Decode 阶段</strong></p>
<ul>
<li>内存密集型</li>
<li>性能受显存带宽和缓存调度限制</li>
</ul>
</li>
</ul>
<p>这也是很多推理优化（如 vLLM）重点针对 decode 阶段的原因。</p>
<hr />
<h3><strong>2. 为什么训练阶段不用 KV Cache</strong></h3>
<p>在训练中：</p>
<ul>
<li>整段序列并行计算</li>
<li>每个 token 都参与反向传播</li>
<li>需要完整计算图</li>
</ul>
<p>KV Cache 会破坏梯度流与并行结构，因此：</p>
<blockquote>
<p><strong>KV Cache 只用于推理，不用于训练</strong></p>
</blockquote>
<hr />
<h2><strong>七、KV Cache 的常见优化方向</strong></h2>
<h3><strong>1. Multi-Query / Grouped-Query Attention（MQA / GQA）</strong></h3>
<ul>
<li>多个 attention head <strong>共享 K / V</strong></li>
<li>显著减少 KV Cache 的体积</li>
</ul>
<p>这是许多现代模型的默认设计选择。</p>
<hr />
<h3><strong>2. Paged KV Cache（vLLM）</strong></h3>
<ul>
<li>将 KV Cache 切分成固定大小的 page</li>
<li>类似操作系统的虚拟内存管理</li>
<li>减少碎片，提升并发请求能力</li>
</ul>
<p>这是 vLLM 在工程上的关键创新之一。</p>
<hr />
<h3><strong>3. FlashAttention + KV Cache</strong></h3>
<ul>
<li>FlashAttention：优化 attention 计算本身</li>
<li>KV Cache：避免重复计算历史</li>
</ul>
<p>二者解决的是<strong>不同层面的性能问题</strong>，可以同时使用。</p>
<hr />
<h3><strong>4. KV Cache Offloading</strong></h3>
<ul>
<li>
<p>将部分 KV Cache 放到 CPU 或其他设备</p>
</li>
<li>
<p>GPU 只保留活跃窗口</p>
</li>
<li>
<p>常用于：</p>
<ul>
<li>超长上下文</li>
<li>显存受限场景</li>
</ul>
</li>
</ul>
<hr />
<h2><strong>八、一个直观类比</strong></h2>
<p>KV Cache 就像做阅读理解时的笔记：</p>
<ul>
<li>已经读过、理解过的内容记下来</li>
<li>后面答题时，不需要从头再读</li>
</ul>
<p>它并不会改变理解方式，但会极大提升效率。</p>
<hr />
<h2><strong>总结</strong></h2>
<p>KV Cache 是自回归 Transformer 推理中的核心优化机制，通过缓存历史 token 的 Key 和 Value，避免在每一步生成中重复计算注意力，使推理复杂度从近似 $O(t^2)$ 降为 $O(t)$。</p>
<p>其代价是显存占用随上下文长度线性增长，因此在实际系统中通常会结合 MQA/GQA、Paged Cache、Offloading 等技术进行优化。</p>
<p>KV Cache 并不是“可选项”，而是<strong>现代大模型推理能够落地的前提条件之一</strong>。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/11"/><category term="Transformer"/><published>2025-12-14T14:35:44+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/10</id><title>Transformer 在哪里做了权重共享？</title><updated>2025-12-16T03:28:15.089799+00:00</updated><content type="html"><![CDATA[<p>权重共享（Weight Sharing）是 Transformer 架构中一个<strong>看似基础、实则非常关键</strong>的设计选择。它直接关系到模型的参数效率、泛化能力以及对任意长度序列的支持能力。</p>
<p>不同于“层与层之间共享参数”的直觉理解，Transformer 的权重共享主要发生在<strong>位置维度和词表维度</strong>，而不是深度维度。理解这一点，是理解 Transformer 架构设计哲学的重要一步。</p>
<hr />
<h2><strong>一、什么是权重共享</strong></h2>
<p>权重共享指的是：<strong>在模型的不同计算位置或不同使用场景中复用同一组参数</strong>，而不是为每个位置或情况单独学习一套权重。</p>
<p>其核心目的包括：</p>
<ul>
<li>减少参数规模</li>
<li>强化模型的泛化能力</li>
<li>引入结构上的不变性假设</li>
</ul>
<p>在 Transformer 中，权重共享并非“随意为之”，而是与序列建模的本质高度契合。</p>
<hr />
<h2><strong>二、Transformer 中最核心的权重共享位置</strong></h2>
<h3><strong>1. 前馈网络（FFN）在所有 token 位置之间共享参数</strong></h3>
<p>在每一层 Transformer 中，前馈网络的形式为：</p>
<p>$\text{FFN}(x_i) = W_2 ,\sigma(W_1 x_i + b_1) + b_2$</p>
<p>关键点在于：</p>
<ul>
<li>对序列中<strong>每一个 token</strong>，都使用<strong>同一组</strong> $W_1, W_2$</li>
<li>参数不随 token 在序列中的位置而变化</li>
</ul>
<p>这是一种<strong>位置级共享（position-wise sharing）</strong>，其效果与 CNN 中“卷积核在空间位置上共享”高度类似。</p>
<p>这一设计保证了：</p>
<ul>
<li>同一语义变换规则可作用于任意位置</li>
<li>模型不依赖固定序列长度</li>
<li>参数规模与序列长度无关</li>
</ul>
<p>这是 Transformer 能够处理任意长度输入的基础之一。</p>
<hr />
<h3><strong>2. 自注意力中的 Q / K / V 投影矩阵在所有位置共享</strong></h3>
<p>在自注意力中，输入序列表示为 X，并通过线性变换得到：</p>
<p>$Q = XW_Q,\quad K = XW_K,\quad V = XW_V$</p>
<p>其中：</p>
<ul>
<li>$W_Q, W_K, W_V$ 在<strong>所有 token 位置之间完全共享</strong></li>
<li>不同 token 的差异，来源于输入表示 X，而不是参数本身</li>
</ul>
<p>这意味着：</p>
<ul>
<li>“如何将一个 token 映射到查询 / 键 / 值空间” 是一种<strong>位置无关的通用语义规则</strong></li>
</ul>
<p>这种共享保证了模型对序列中不同位置的 token 采取一致的语义处理方式。</p>
<hr />
<h3><strong>3. 多头注意力中：位置共享，头之间不共享</strong></h3>
<p>在多头注意力机制中：</p>
<ul>
<li>每一个 attention head 都有自己独立的 $W_Q^h, W_K^h, W_V^h$</li>
<li><strong>同一个 head 内部</strong>，这些参数在所有 token 位置上共享</li>
<li><strong>不同 head 之间</strong>，参数不共享</li>
</ul>
<p>因此可以总结为：</p>
<ul>
<li>位置维度：共享</li>
<li>head 维度：不共享</li>
</ul>
<p>这种设计使得不同 head 可以学习不同的关系子空间，而每个子空间在序列位置上保持一致的变换规则。</p>
<hr />
<h3><strong>4. 层内共享，层间不共享</strong></h3>
<p>在标准 Transformer 中：</p>
<ul>
<li>同一层内部，Attention 和 FFN 的参数对所有 token 共享</li>
<li>不同层之间，参数完全独立</li>
</ul>
<p>每一层可以被理解为学习<strong>不同层级的语义抽象</strong>，因此不进行层间共享。</p>
<p>需要注意的是，只有在特殊模型（如 ALBERT）中，才会引入层间共享作为额外的参数压缩手段。</p>
<hr />
<h3><strong>5. 输入 Embedding 与输出 Softmax 权重的共享（常见实践）</strong></h3>
<p>在许多语言模型实现中（如 GPT、BERT）：</p>
<ul>
<li>输入词嵌入矩阵</li>
<li>输出层 softmax 前的线性映射矩阵</li>
</ul>
<p>会进行权重共享（或转置共享）：</p>
<p>$W_{\text{out}} = W_{\text{embed}}^\top$</p>
<p>这种做法的优势包括：</p>
<ul>
<li>显著减少参数量</li>
<li>保证输入与输出使用同一语义空间</li>
<li>提升泛化能力和训练稳定性</li>
</ul>
<p>这是一个非常常见但并非强制的工程选择。</p>
<hr />
<h2><strong>三、为什么 Transformer 必须这样共享权重？</strong></h2>
<h3><strong>1. 序列建模中的“平移不变性”假设</strong></h3>
<p>在语言中：</p>
<blockquote>
<p>一个词出现在句首或句尾，其基本语义处理规则应当一致。</p>
</blockquote>
<p>通过在位置维度上共享参数，模型将“位置相关性”与“语义变换规则”解耦：</p>
<ul>
<li>语义规则 → 由共享参数学习</li>
<li>位置信息 → 由位置编码提供</li>
</ul>
<hr />
<h3><strong>2. 参数效率与泛化能力</strong></h3>
<p>如果不做共享：</p>
<ul>
<li>参数量会随序列长度线性或指数增长</li>
<li>模型更容易过拟合位置模式</li>
<li>泛化到更长序列将变得不可行</li>
</ul>
<p>共享参数使模型学到的是<strong>可复用的语言结构规律</strong>，而不是对某些固定位置的记忆。</p>
<hr />
<h3><strong>3. 支持任意长度输入</strong></h3>
<p>Transformer 能够在推理时处理比训练阶段更长的序列，其前提是：</p>
<ul>
<li>参数不依赖于具体的绝对位置索引</li>
</ul>
<p>位置级权重共享正是这一能力的结构保障。</p>
<hr />
<h2><strong>四、哪些地方没有做权重共享</strong></h2>
<p>为了避免误解，有必要明确：</p>
<ul>
<li>Transformer <strong>不在不同层之间共享参数</strong></li>
<li>不同 attention head 之间也不共享参数</li>
<li>模型深度维度上的多样性是有意保留的</li>
</ul>
<p>这种“共享与不共享的边界”，正是 Transformer 表达能力与稳定性之间的重要平衡。</p>
<hr />
<h2><strong>五、权重共享位置一览</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>是否共享</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>不同 token 位置</td>
<td>✅ 共享</td>
</tr>
<tr>
<td>同一层内部</td>
<td>✅ 共享</td>
</tr>
<tr>
<td>不同层之间</td>
<td>❌ 不共享</td>
</tr>
<tr>
<td>不同 attention head</td>
<td>❌ 不共享</td>
</tr>
<tr>
<td>输入 / 输出 embedding</td>
<td>✅（常见）</td>
</tr>
</tbody></table><hr />
<h2><strong>总结</strong></h2>
<p>Transformer 的权重共享并不 hookup 在“层”这个维度上，而是集中体现在<strong>位置维度和词表维度</strong>。</p>
<p>通过在所有 token 位置上共享 Attention 和 FFN 的参数，Transformer 实现了类似卷积的平移不变性，并得以支持任意长度序列建模；而通过共享输入与输出 embedding，则进一步提升了参数效率与语义一致性。</p>
<p>这种共享策略并非工程折中，而是由序列建模与注意力机制的本质共同决定的结构选择。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/10"/><category term="Transformer"/><published>2025-12-14T13:58:51+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/9</id><title>混合精度训练时，哪些参数应该使用高精度，哪些该使用低精度？</title><updated>2025-12-16T03:28:15.231001+00:00</updated><content type="html"><![CDATA[<h2><strong>一句话总原则</strong></h2>
<blockquote>
<p><strong>参与梯度累积、数值范围敏感、长期状态的量 → 高精度</strong></p>
</blockquote>
<blockquote>
<p><strong>只参与前向计算、短期使用、可容忍近似误差的量 → 低精度</strong></p>
</blockquote>
<hr />
<h2><strong>一、为什么混合精度不能“全 FP16 / BF16”？</strong></h2>
<p>低精度的问题不是“不准”，而是：</p>
<ul>
<li><strong>动态范围小</strong></li>
<li><strong>舍入误差大</strong></li>
<li><strong>累积误差不可逆</strong></li>
</ul>
<p>所以凡是<strong>会反复累积、或作为基准的量</strong>，一旦精度不够，训练就会：</p>
<ul>
<li>loss 抖动</li>
<li>梯度爆炸 / 消失</li>
<li>或者悄悄退化（最危险）</li>
</ul>
<hr />
<h2><strong>二、哪些参数 / 张量必须用高精度（FP32 或 BF16）</strong></h2>
<h3>1️⃣ Optimizer 状态</h3>
<p><strong>必须 FP32</strong></p>
<p>包括：</p>
<ul>
<li>
<p>Adam / AdamW 的</p>
<ul>
<li>一阶动量 m</li>
<li>二阶动量 v</li>
</ul>
</li>
<li>
<p>RMSProp 的 moving average</p>
</li>
<li>
<p>SGD + momentum 的 momentum buffer</p>
</li>
</ul>
<p><strong>原因</strong>：</p>
<ul>
<li>这是<strong>长期累积状态</strong></li>
<li>数值会跨越大量 step</li>
<li>FP16 累积误差极大</li>
</ul>
<blockquote>
<p>实际上：</p>
</blockquote>
<blockquote>
<p><strong>99% 的训练不稳定，根因都在 optimizer state 精度不够</strong></p>
</blockquote>
<hr />
<h3><strong>2️⃣ Master Weights（权重的 FP32 副本）</strong></h3>
<p><strong>推荐 FP32</strong></p>
<p>常见做法：</p>
<ul>
<li>显存中用 FP16 / BF16 权重参与前向</li>
<li>同时维护一份 <strong>FP32 master copy</strong></li>
<li>反向更新 master copy</li>
<li>再 cast 回低精度</li>
</ul>
<p><strong>原因</strong>：</p>
<ul>
<li>梯度更新是“微小增量”</li>
<li>FP16 很容易被直接 round 掉</li>
</ul>
<hr />
<h3><strong>3️⃣ Loss 计算与归一化相关量</strong></h3>
<p><strong>必须高精度</strong></p>
<p>包括：</p>
<ul>
<li>loss 本身</li>
<li>loss scaling（动态 loss scale）</li>
<li>BatchNorm 的 running mean / var</li>
<li>LayerNorm 的统计计算（常内部 FP32）</li>
</ul>
<p><strong>原因</strong>：</p>
<ul>
<li>loss 是所有梯度的源头</li>
<li>数值不稳会放大到整个网络</li>
</ul>
<hr />
<h3><strong>4️⃣ 梯度累积相关变量</strong></h3>
<p><strong>推荐 FP32</strong></p>
<p>包括：</p>
<ul>
<li>梯度累积 buffer</li>
<li>多 step 累加的 grad</li>
</ul>
<p><strong>原因</strong>：</p>
<ul>
<li>梯度本身已经是高噪声信号</li>
<li>再低精度累加极易失真</li>
</ul>
<hr />
<h2><strong>三、哪些可以安全使用低精度（FP16 / BF16）</strong></h2>
<h3><strong>1️⃣ 模型权重（参与前向）</strong></h3>
<p><strong>可以低精度</strong></p>
<ul>
<li>Linear / Conv / Attention 权重</li>
<li>FFN 权重</li>
<li>Embedding 权重</li>
</ul>
<p>前提：</p>
<ul>
<li>有 FP32 master copy 或 BF16 宽动态范围</li>
</ul>
<hr />
<h3><strong>2️⃣ 前向激活（Activations）</strong></h3>
<p><strong>可以低精度（最主要节省显存来源）</strong></p>
<ul>
<li>Q / K / V</li>
<li>attention output</li>
<li>FFN 中间层激活</li>
</ul>
<p>原因：</p>
<ul>
<li>激活是<strong>短生命周期</strong></li>
<li>用完即丢</li>
<li>误差不会长期积累</li>
</ul>
<hr />
<h3><strong>3️⃣ Attention / FFN 中的矩阵乘法</strong></h3>
<p><strong>低精度是主流</strong></p>
<ul>
<li>Tensor Core 对 FP16 / BF16 高度优化</li>
<li>性能提升巨大</li>
</ul>
<hr />
<h3><strong>4️⃣ LoRA / Adapter 参数（通常 BF16 / FP16）</strong></h3>
<ul>
<li>参数量小</li>
<li>梯度相对稳定</li>
<li>实践中 BF16 非常稳</li>
</ul>
<hr />
<h2><strong>四、FP16 vs BF16 的关键区别</strong></h2>
<table>
<thead>
<tr>
<th><strong>项</strong></th>
<th><strong>FP16</strong></th>
<th><strong>BF16</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Mantissa（精度）</td>
<td>高</td>
<td>低</td>
</tr>
<tr>
<td>Exponent（范围）</td>
<td>小</td>
<td>大</td>
</tr>
<tr>
<td>易溢出</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>是否需要 loss scaling</td>
<td>必须</td>
<td>通常不需要</td>
</tr>
<tr>
<td>LLM 训练主流</td>
<td>旧</td>
<td><strong>新主流</strong></td>
</tr>
</tbody></table><p><strong>结论</strong>：</p>
<ul>
<li><strong>能用 BF16 就别用 FP16</strong></li>
<li>BF16 天生更适合大模型训练</li>
</ul>
<hr />
<h2><strong>五、典型框架里的默认做法</strong></h2>
<h3><strong>PyTorch AMP（autocast）</strong></h3>
<ul>
<li>
<p>前向：</p>
<ul>
<li>Linear / MatMul → FP16 / BF16</li>
</ul>
</li>
<li>
<p>归一化 / softmax：</p>
<ul>
<li>自动回退 FP32</li>
</ul>
</li>
<li>
<p>Optimizer：</p>
<ul>
<li>state 永远 FP32</li>
</ul>
</li>
</ul>
<hr />
<h3><strong>DeepSpeed / FSDP</strong></h3>
<ul>
<li>参数分片：低精度</li>
<li>Optimizer state：FP32</li>
<li>梯度 reduce：FP32 或混合</li>
</ul>
<hr />
<h3><strong>QLoRA 特例</strong></h3>
<ul>
<li>Base 权重：<strong>NF4（存储）</strong></li>
<li>计算时：反量化为 BF16</li>
<li>LoRA 参数：BF16</li>
<li>Optimizer state：FP32</li>
</ul>
<hr />
<h2><strong>六、一个清晰的分类表</strong></h2>
<table>
<thead>
<tr>
<th><strong>类型</strong></th>
<th><strong>精度</strong></th>
<th><strong>原因</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Optimizer state</td>
<td>FP32</td>
<td>长期累积</td>
</tr>
<tr>
<td>Master weights</td>
<td>FP32</td>
<td>微小更新</td>
</tr>
<tr>
<td>Loss / norm 统计</td>
<td>FP32</td>
<td>稳定性</td>
</tr>
<tr>
<td>梯度累积</td>
<td>FP32</td>
<td>防止失真</td>
</tr>
<tr>
<td>前向权重</td>
<td>FP16 / BF16</td>
<td>吞吐</td>
</tr>
<tr>
<td>Activations</td>
<td>FP16 / BF16</td>
<td>短生命周期</td>
</tr>
<tr>
<td>Attention / FFN 计算</td>
<td>FP16 / BF16</td>
<td>Tensor Core</td>
</tr>
</tbody></table><hr />
<h2><strong>最终一句话总结</strong></h2>
<blockquote>
<p>混合精度训练的核心不是“能不能用低精度”，而是<strong>绝不让任何会被长期累积、或作为数值基准的变量使用低精度</strong>；前向计算尽量低精度，反向更新与状态一律高精度。</p>
</blockquote>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/9"/><category term="微调"/><published>2025-12-14T13:21:44+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/8</id><title>为什么 Transformer 使用 LayerNorm 而不是 BatchNorm？</title><updated>2025-12-16T03:28:15.387875+00:00</updated><content type="html"><![CDATA[<p>在 Transformer 的设计中，<strong>归一化方式并不是一个可随意替换的工程细节，而是由模型结构与训练范式共同决定的核心选择</strong>。理解这一点，有助于从根本上把握 Transformer 与传统卷积网络在建模假设上的差异。</p>
<h2><strong>一句话结论</strong></h2>
<p>Transformer 使用 <strong>LayerNorm（层归一化）而不是 BatchNorm（批归一化）</strong>，是因为 Transformer 的计算是逐 token、支持变长序列，并高度依赖自注意力与自回归生成；而 BatchNorm 对 batch 维度和样本分布有强假设，在这些场景下不成立，甚至会破坏模型行为。</p>
<hr />
<img width="368" height="401" alt="Image" src="https://github.com/user-attachments/assets/65e23d8d-3ce4-4d23-8f5b-7791573a1b91" />
<h2><strong>一、BatchNorm 的设计前提</strong></h2>
<p>BatchNorm 在提出时，隐含了几个关键假设：</p>
<p><strong>1. 样本是同构的</strong></p>
<p>同一层中，不同样本在同一维度上具有一致语义含义；第 <em>i</em> 个通道在 batch 内代表“同一种特征”。</p>
<p><strong>2. batch size 足够大</strong></p>
<p>BatchNorm 的均值和方差依赖 batch 内统计，小 batch 会导致估计噪声显著。</p>
<p><strong>3. 样本顺序不重要</strong></p>
<p>BatchNorm 在 batch 维度上聚合统计信息，不区分样本先后或结构角色。</p>
<p>这些假设在 CNN 中通常成立，但在 Transformer 中几乎全部失效。</p>
<hr />
<h2><strong>二、Transformer 的计算范式与 BatchNorm 的冲突</strong></h2>
<h3><strong>1. Transformer 是逐 token（token-wise）建模</strong></h3>
<p>在 Transformer 中：</p>
<ul>
<li>每个 token 是一个独立的语义单元</li>
<li>不同位置的 token 具有不同语义角色与分布特性</li>
</ul>
<p>如果对 batch 内所有 token 使用 BatchNorm，相当于将：</p>
<blockquote>
<p>句首词、句中词、句尾词</p>
</blockquote>
<blockquote>
<p>当作同一分布进行归一化</p>
</blockquote>
<p>这在语言建模的语义层面并不成立。</p>
<hr />
<h3><strong>2. 序列是变长的</strong></h3>
<p>Transformer 天然支持变长序列，并通过 padding + mask 处理不等长输入。</p>
<p>但 BatchNorm 并不知道哪些位置是 padding：</p>
<ul>
<li>padding 会参与 batch 统计</li>
<li>均值与方差会随 padding 比例变化</li>
</ul>
<p>结果是：<strong>同一句话在不同 batch 中的归一化结果可能不同</strong>，破坏模型一致性。</p>
<hr />
<h3><strong>3. 自回归生成场景下 BatchNorm 几乎不可用</strong></h3>
<p>在 GPT、LLaMA 等自回归模型中：</p>
<ul>
<li>推理时 batch size 通常为 1</li>
<li>token 逐步生成</li>
</ul>
<p>此时 BatchNorm 的 batch 统计毫无意义，且训练与推理行为不一致，这对语言模型是致命问题。</p>
<hr />
<h2><strong>三、LayerNorm 与 Transformer 的天然匹配</strong></h2>
<p>LayerNorm 的归一化方式是：</p>
<p>$\text{LN}(x) = \frac{x - \mu_{\text{feature}}}{\sigma_{\text{feature}}}$</p>
<p>其统计方式具有以下特性：</p>
<ul>
<li>在<strong>单个样本内部</strong>计算</li>
<li>沿<strong>特征维度</strong>归一化</li>
<li>不依赖 batch</li>
</ul>
<h3><strong>LayerNorm 的关键优势</strong></h3>
<p><strong>1. 与 batch size 无关</strong></p>
<p>batch size = 1 时仍然稳定，训练与推理行为一致。</p>
<p><strong>2. 逐 token 独立</strong></p>
<p>每个 token 仅基于自身特征归一化，不受其他 token 或样本干扰。</p>
<p><strong>3. 天然支持变长序列</strong></p>
<p>mask 与 padding 不会污染统计结果。</p>
<p>这正是 Transformer 所需要的归一化方式。</p>
<hr />
<h2><strong>四、注意力机制决定了不能使用 BatchNorm</strong></h2>
<p>自注意力的核心计算为：</p>
<p>$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V$</p>
<p>注意力机制对<strong>数值尺度极其敏感</strong>，而 softmax 会进一步放大分布变化。</p>
<p>如果使用 BatchNorm：</p>
<ul>
<li>不同 batch 的统计不同</li>
<li>同一 token 在不同 batch 中的 attention 权重可能不同</li>
</ul>
<p>这会导致模型行为不可控、训练不稳定。</p>
<p>LayerNorm 则保证：</p>
<p><strong>一个 token 的注意力行为仅由其自身和上下文决定，而不受 batch 中其他样本影响。</strong></p>
<hr />
<h2><strong>五、工程视角：BatchNorm 会破坏可复现性</strong></h2>
<p>BatchNorm 引入了：</p>
<ul>
<li>batch 依赖</li>
<li>运行时统计漂移</li>
<li>train / eval 模式切换</li>
</ul>
<p>而 Transformer 的设计目标是：</p>
<ul>
<li>稳定</li>
<li>可扩展</li>
<li>易并行</li>
<li>高可复现性</li>
</ul>
<p>LayerNorm 不依赖 running statistics，在分布式训练和超大模型场景下更可靠。这也是为什么几乎所有成功的 Transformer 变体都采用 LayerNorm 或其改进形式。</p>
<hr />
<h2><strong>六、LayerNorm 在大模型中的进一步演化</strong></h2>
<p>在大语言模型中，LayerNorm 甚至被进一步简化或重构：</p>
<ul>
<li>
<p><strong>RMSNorm（LLaMA、GPT-NeoX）</strong></p>
<ul>
<li>不减均值，仅做方差归一</li>
<li>更快、更稳定</li>
</ul>
</li>
<li>
<p><strong>Pre-LN Transformer</strong></p>
<ul>
<li>将 LayerNorm 放在子层之前</li>
<li>显著缓解深层网络的训练不稳定问题</li>
</ul>
</li>
</ul>
<p>这表明：<strong>归一化方式在 Transformer 中是结构核心，而非附属组件。</strong></p>
<hr />
<h2><strong>七、一个直观的类比</strong></h2>
<ul>
<li>
<p>BatchNorm：</p>
<p>“你这道题做得好不好，要参考全班平均水平”</p>
</li>
<li>
<p>LayerNorm：</p>
<p>“你这道题做得好不好，只取决于你自己的解题结构”</p>
</li>
</ul>
<p>语言建模显然需要后者。</p>
<hr />
<h2><strong>总结</strong></h2>
<p>Transformer 使用 LayerNorm 而不是 BatchNorm，并非经验选择，而是由其逐 token、自回归、变长序列和注意力驱动的计算范式所决定。LayerNorm 在特征维度上对单个样本独立归一化，保证了训练与推理的一致性、数值稳定性和模型行为的可控性，因此成为 Transformer 架构中的标准归一化方案。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/8"/><category term="Transformer"/><published>2025-12-14T10:30:34+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/7</id><title>Prefix-Tuning：在注意力机制中引入可学习前缀的参数高效微调方法</title><updated>2025-12-16T03:28:15.545144+00:00</updated><content type="html"><![CDATA[<p>随着预训练语言模型规模不断扩大，全参数微调在计算成本、存储开销和稳定性方面逐渐暴露出局限性。在此背景下，一类以<strong>冻结模型参数、仅引入少量可训练模块</strong>为特征的参数高效微调方法（PEFT）逐渐成为主流方向。Prefix-Tuning 正是其中具有代表性的一种方法。</p>
<p>Prefix-Tuning 最早由 Li 和 Liang 在 2021 年提出，其核心思想是在不修改模型权重的前提下，通过在 Transformer 的注意力机制中引入可学习的前缀向量，引导模型在生成阶段表现出符合下游任务需求的行为。</p>
<hr />
<h2><strong>方法动机与提出背景</strong></h2>
<p>传统微调方法主要面临两类问题：</p>
<p>一方面，全参数微调需要更新大量模型权重，在大模型场景下带来显著的显存与算力压力，同时也容易引发过拟合或灾难性遗忘。</p>
<p>另一方面，基于自然语言提示或仅作用于输入层的 Prompt / P-Tuning 方法，对模型深层计算路径的影响有限，在复杂生成任务中控制能力不足。</p>
<p>Prefix-Tuning 的设计目标是在保持模型参数完全冻结的情况下，对模型的<strong>内部推理过程</strong>施加更直接、可控的影响。</p>
<hr />
<h2><strong>核心思想与基本原理</strong></h2>
<img width="415" height="342" alt="Image" src="https://github.com/user-attachments/assets/5ce41760-fc0f-4424-8c1e-944131851c6c" />
<h3><strong>注意力机制回顾</strong></h3>
<p>在 Transformer 中，自注意力机制的核心计算形式为：</p>
<p>$\text{Attention}(Q, K, V)$</p>
<p>其中，查询（Q）来自当前 token 表示，键（K）和值（V）来自上下文 token 表示。注意力输出本质上是对值向量的加权组合，权重由查询与键的相似度决定。</p>
<h3><strong>Prefix-Tuning 的关键改动</strong></h3>
<p>Prefix-Tuning 在 Transformer 的<strong>每一层注意力模块中</strong>引入一组可训练的前缀向量。这些前缀并非真实文本 token，而是连续的高维向量，用于扩展注意力中的键和值：</p>
<p>$K = [K_{\text{prefix}} ; K_{\text{input}}], \quad V = [V_{\text{prefix}} ; V_{\text{input}}]$</p>
<p>其中：</p>
<ul>
<li>查询 Q 保持不变；</li>
<li>前缀向量仅参与注意力计算，不进入前馈网络；</li>
<li>前缀在同一任务中对所有输入共享。</li>
</ul>
<p>这种设计使模型在每一层计算注意力时，都会将前缀作为潜在的重要上下文进行考虑。</p>
<hr />
<h2><strong>Prefix-Tuning 的作用机制</strong></h2>
<p>Prefix-Tuning 的有效性主要来源于以下几个方面：</p>
<p><strong>作用层级更深</strong></p>
<p>不同于仅在输入层注入信息的提示方法，Prefix-Tuning 在每一层注意力计算中引入前缀，对模型的中间表示与决策路径产生持续影响。</p>
<p><strong>摆脱自然语言约束</strong></p>
<p>前缀由连续向量构成，不受自然语言形式限制，能够在更大的表示空间中学习任务相关信息。</p>
<p><strong>直接干预注意力分布</strong></p>
<p>注意力机制决定了模型在每一层“关注什么”。通过在键和值中引入前缀，Prefix-Tuning 实际上在每一层对模型的关注重点进行调节。</p>
<p>从功能上看，Prefix-Tuning 相当于在模型内部构建了一段隐式的、可学习的上下文。</p>
<hr />
<h2><strong>训练与参数规模</strong></h2>
<p>Prefix-Tuning 的训练流程具有较强的工程友好性：</p>
<ol>
<li>为每一层初始化一组前缀向量（通常对应 K/V）；</li>
<li>冻结原始 Transformer 的全部参数；</li>
<li>前向传播时将前缀拼接到注意力的键和值中；</li>
<li>使用下游任务损失进行反向传播；</li>
<li>仅更新前缀相关参数。</li>
</ol>
<p>在实际应用中，Prefix-Tuning 的参数规模通常为万级到十万级，远小于全参数微调，也通常小于基于低秩权重更新的方法。</p>
<hr />
<h2><strong>与相关方法的关系</strong></h2>
<p>从结构干预深度来看，Prefix-Tuning 位于输入级提示方法与权重级方法之间：</p>
<ul>
<li>Prompt Engineering 通过自然语言文本影响模型行为；</li>
<li>P-Tuning 在输入 embedding 层引入可学习提示；</li>
<li>Prefix-Tuning 在每一层注意力的键和值中引入前缀；</li>
<li>LoRA 等方法直接修改模型权重矩阵的有效表示。</li>
</ul>
<p>这种位置上的差异决定了 Prefix-Tuning 在控制能力与参数效率之间取得了较为平衡的效果。</p>
<hr />
<h2><strong>优势与局限</strong></h2>
<p>Prefix-Tuning 的主要优势包括：</p>
<ul>
<li>参数效率高，仅需训练少量前缀向量；</li>
<li>不修改模型权重，训练与部署过程相对安全稳定；</li>
<li>对生成类任务（如摘要、翻译、对话）具有良好适应性；</li>
<li>支持多任务场景下加载不同前缀进行快速切换。</li>
</ul>
<p>其局限性也同样明确：</p>
<ul>
<li>推理阶段需额外加载前缀，带来一定计算与存储开销；</li>
<li>表达能力通常不及直接修改权重的方法；</li>
<li>实现复杂度高于仅作用于输入层的提示方法。</li>
</ul>
<hr />
<h2><strong>应用场景与实践经验</strong></h2>
<p>Prefix-Tuning 更适合以下场景：</p>
<ul>
<li>中等复杂度的文本生成任务；</li>
<li>下游数据规模有限；</li>
<li>对模型参数安全性要求较高；</li>
<li>多任务或多领域快速切换需求明显。</li>
</ul>
<p>在需要显著改变模型能力或进行复杂推理、代码生成等任务时，通常需要结合或转向更强的权重级参数高效方法。</p>
<hr />
<h2><strong>总结</strong></h2>
<p>Prefix-Tuning 通过在 Transformer 每一层注意力机制中引入可学习的前缀向量，在冻结模型参数的前提下，实现了对模型生成行为的有效引导。该方法在参数效率、稳定性与控制能力之间提供了一种折中方案，是参数高效微调技术体系中的重要组成部分。</p>
<p>在现代大模型应用中，Prefix-Tuning 常与其他 PEFT 方法共同构成一条从“提示级控制”到“权重级适配”的连续技术路径。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/7"/><category term="微调"/><published>2025-12-14T04:18:45+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/6</id><title>Transformer 中 FFN 是做什么的？——为什么注意力之外还需要前馈网络</title><updated>2025-12-16T03:28:15.690408+00:00</updated><content type="html"><![CDATA[<p>在 Transformer 架构中，自注意力机制（Self-Attention）往往更容易被关注，但真正决定模型表达能力上限的核心模块之一，是每一层中与注意力并列存在的前馈神经网络（Feed-Forward Network, FFN）。FFN 并非简单的附属组件，而是 Transformer 能够建模复杂语义与非线性关系的关键所在。</p>
<img width="1225" height="384" alt="Image" src="https://github.com/user-attachments/assets/ebfb108f-344b-4773-977e-d0045407e3be" />
<hr />
<h2><strong>注意力与 FFN 的职责分工</strong></h2>
<p>从信息流的角度来看，Transformer 的每一层可以拆分为两类互补的计算过程：</p>
<ul>
<li>
<p><strong>自注意力机制</strong></p>
<p>负责在不同 token 之间建立依赖关系，对上下文信息进行加权聚合，解决“从哪里获取信息”的问题。</p>
</li>
<li>
<p><strong>前馈网络（FFN）</strong></p>
<p>负责在单个 token 内部对已聚合的信息进行非线性变换和特征重组，解决“如何理解和加工信息”的问题。</p>
</li>
</ul>
<p>两者形成了清晰的分工：注意力负责信息路由，FFN 负责信息加工。</p>
<hr />
<h2><strong>FFN 的标准结构</strong></h2>
<p>在经典 Transformer 中，FFN 是一个逐位置（position-wise）的两层全连接网络，其基本形式为：</p>
<p>$\text{FFN}(x) = W_2 , \sigma(W_1 x + b_1) + b_2$</p>
<p>其中：</p>
<ul>
<li>$x \in \mathbb{R}^{d_{\text{model}}}$ 是单个 token 的表示；</li>
<li>$W_1$ 将特征维度从 $d_{\text{model}} $升维到 $d_{\text{ff}}$；</li>
<li>$\sigma$ 是非线性激活函数（ReLU、GELU、SwiGLU 等）；</li>
<li>$W_2$ 再将特征映射回模型维度。</li>
</ul>
<p>这一网络对序列中每个位置独立作用，参数在所有位置共享，不引入 token 间的交互。</p>
<hr />
<h2><strong>升维—非线性—降维的设计动机</strong></h2>
<p>FFN 中“先升维、再降维”的结构并非偶然，而是 Transformer 表达能力的核心来源。</p>
<ol>
<li>
<p><strong>扩展表示空间</strong></p>
<p>中间层维度 $d_{\text{ff}}$ 通常是 $d_{\text{model}} $的 4 倍左右。高维空间提供了更强的函数拟合能力，使模型能够表示复杂的特征组合。</p>
</li>
<li>
<p><strong>引入非线性建模能力</strong></p>
<p>自注意力本质上是线性加权求和，只能重组已有特征，无法产生新的语义结构。FFN 通过非线性激活，使模型能够学习条件关系和高阶特征。</p>
</li>
<li>
<p><strong>特征解耦与重组</strong></p>
<p>升维后的空间允许不同语义子特征被分离、激活和重新组合，随后再压缩为更抽象、更有判别力的表示。</p>
</li>
</ol>
<p>从这一角度看，FFN 更接近 Transformer 中真正的“计算单元”，而注意力更多承担结构性信息组织的角色。</p>
<hr />
<h2><strong>FFN 对语义理解的作用</strong></h2>
<p>在经过注意力计算后，每个 token 的表示已经融合了上下文信息，但这些信息仍然是线性叠加的结果。FFN 在此基础上完成语义层面的再解释。</p>
<p>例如，在上下文信息已被聚合后，FFN 能够根据特征组合激活特定语义模式，将模糊的上下文混合表示映射为更明确的高层语义状态。这种能力并非来自 token 间交互，而是来自对单个 token 表示的深度非线性变换。</p>
<hr />
<h2><strong>为什么 FFN 采用逐位置设计</strong></h2>
<p>FFN 以逐位置方式作用，原因在于 Transformer 的结构分层原则：</p>
<ul>
<li>token 之间的依赖关系已经由自注意力显式建模；</li>
<li>FFN 专注于 token 内部的语义加工；</li>
<li>职责分离使模型结构更清晰、训练更稳定。</li>
</ul>
<p>这种设计也使 FFN 在计算上高度并行，适合大规模模型训练。</p>
<hr />
<h2><strong>缺失 FFN 的影响</strong></h2>
<p>如果 Transformer 中仅保留注意力而移除 FFN，那么整个网络将退化为线性映射的堆叠。即便层数增加，模型仍无法表示复杂的非线性函数，表达能力会受到根本性限制。</p>
<p>在实际的大语言模型中，FFN 不仅承担主要的非线性建模任务，其参数量和计算量也通常占据模型总量的绝大部分。这一事实从工程层面进一步印证了 FFN 在 Transformer 中的核心地位。</p>
<hr />
<h2><strong>FFN 的结构演进</strong></h2>
<p>随着模型规模和任务复杂度的提升，FFN 的形式也在不断演化：</p>
<ul>
<li><strong>GELU FFN</strong>：常见于 BERT、早期 GPT 系列</li>
<li><strong>SwiGLU / GEGLU</strong>：引入门控机制，提高表达效率（LLaMA、PaLM、T5）</li>
<li><strong>MoE-FFN</strong>：通过专家路由进一步提升模型容量（Mixtral、DeepSeek-MoE）</li>
</ul>
<p>尽管实现形式不同，这些变体仍遵循同一基本思想：在高维空间中进行非线性特征加工，再映射回模型表示空间。</p>
<hr />
<h2><strong>总结</strong></h2>
<p>在 Transformer 架构中，前馈神经网络承担着对每个 token 表示进行深度语义加工的职责。自注意力机制负责跨 token 的信息聚合，而 FFN 通过升维、非线性激活与降维，使模型具备建模复杂语义关系的能力。</p>
<p>正是这种“注意力负责结构，FFN 负责计算”的分工，使 Transformer 成为一种既具全局建模能力、又具强表达能力的通用序列建模架构。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/6"/><category term="Transformer"/><published>2025-12-13T16:20:47+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/5</id><title>P-Tuning 与 P-Tuning v2：参数高效 Prompt 微调方法解析</title><updated>2025-12-16T03:28:15.833072+00:00</updated><content type="html"><![CDATA[<h2><strong>一、方法提出背景</strong></h2>
<p>在大语言模型的下游任务适配中，Prompt 的构造方式对模型性能具有决定性影响。以 GPT-3 为代表的 In-Context Learning 依赖人工设计的离散模板，但这种方式存在明显局限：</p>
<ul>
<li>模板对措辞、顺序和示例极其敏感</li>
<li>轻微修改即可导致性能大幅波动</li>
<li>离散 token 的自动化搜索成本高，且难以保证稳定最优</li>
</ul>
<p>同时，全参数微调虽然效果稳定，但在大模型场景下带来了显著的显存、算力与维护成本，并伴随潜在的灾难性遗忘风险。</p>
<p>在这一背景下，P-Tuning 被提出，其核心目标是在<strong>冻结模型主体参数的前提下，通过可学习 Prompt 实现稳定、高效的任务适配</strong>。</p>
<hr />
<h2><strong>二、P-Tuning 的核心思想</strong></h2>
<p>P-Tuning 将传统 Prompt 中的离散 token 替换为一组<strong>连续可微的虚拟 token（Virtual Tokens）</strong>，并直接在 embedding 空间中进行优化。</p>
<p>核心特征包括：</p>
<ul>
<li>Prompt 不再是自然语言文本，而是可训练向量</li>
<li>Prompt 作为模型输入的一部分参与前向传播</li>
<li>模型主体参数保持冻结，仅优化 Prompt 相关参数</li>
</ul>
<p>这一设计使 Prompt 构造从“人工经验问题”转化为“连续优化问题”。</p>
<hr />
<h2><strong>三、P-Tuning 的技术原理</strong></h2>
<h3><strong>1. 连续 Prompt 表示</strong></h3>
<p>P-Tuning 将人工模板中的真实 token 替换为一组虚拟 token，这些 token 以 embedding 的形式插入输入序列中。</p>
<p>虚拟 token 的插入位置并不固定，可以是前缀，也可以位于输入序列的其他位置。</p>
<h3><strong>2. Prompt Encoder 设计</strong></h3>
<p>由于预训练语言模型的 embedding 空间高度离散，若直接随机初始化虚拟 token，优化过程容易陷入局部最优。</p>
<p>为此，P-Tuning 引入 Prompt Encoder，对虚拟 token 进行结构化建模：</p>
<ul>
<li>使用 LSTM + MLP 对 Prompt embedding 进行编码</li>
<li>显式建模 Prompt token 之间的相关性</li>
<li>提升收敛速度与优化稳定性</li>
</ul>
<p>实验结果表明，引入 Prompt Encoder 后，P-Tuning 在多个任务上可以达到甚至超过全参数微调的效果。</p>
<hr />
<h2><strong>四、P-Tuning 的实验现象与结论</strong></h2>
<p>在对比实验中，P-Tuning 展现出若干值得注意的现象：</p>
<ul>
<li>在相同参数规模下，全参数微调时 BERT 在 NLU 任务中通常优于 GPT</li>
<li>在 P-Tuning 设定下，GPT 在多个任务上可反超 BERT</li>
<li>P-Tuning 在部分任务中可达到与全参数微调一致的性能</li>
</ul>
<p>这些结果表明，<strong>模型能力并不完全由参数更新方式决定，输入空间的可学习结构同样具有重要影响</strong>。</p>
<hr />
<h2><strong>五、P-Tuning v2 的提出动机</strong></h2>
<p>尽管 P-Tuning 在多个任务中取得了良好效果，但其仍存在明显局限：</p>
<h3><strong>1. 规模泛化能力不足</strong></h3>
<ul>
<li>在大模型（&gt;10B）上效果接近全参微调</li>
<li>在中小模型（100M–1B）上性能明显下降</li>
</ul>
<h3><strong>2. 任务泛化能力不足</strong></h3>
<ul>
<li>在部分 NLU 分类任务中表现良好</li>
<li>在序列标注等结构化任务上效果不稳定</li>
</ul>
<h3><strong>3. 提示深度受限</strong></h3>
<ul>
<li>Prompt 仅作用于输入 embedding 层</li>
<li>对深层 Transformer 表征的影响较为间接</li>
</ul>
<p>为解决上述问题，P-Tuning v2 被提出。</p>
<hr />
<h2><strong>六、P-Tuning v2 的核心改进</strong></h2>
<p>P-Tuning v2（论文 <em>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</em>）引入了<strong>深度 Prompt 优化机制</strong>。</p>
<h3><strong>1. 多层 Prompt 注入</strong></h3>
<ul>
<li>在 Transformer 的每一层引入 Prompt token</li>
<li>Prompt 直接参与中间层表示计算</li>
<li>对模型预测产生更直接的影响</li>
</ul>
<h3><strong>2. 参数规模提升但仍高效</strong></h3>
<ul>
<li>可学习参数比例从约 0.01% 提升至 0.1%–3%</li>
<li>相较全参数微调仍具显著参数效率优势</li>
</ul>
<h3><strong>3. 结构设计上的简化</strong></h3>
<ul>
<li>移除 P-Tuning v1 中的重参数化 Prompt Encoder</li>
<li>直接将 Prompt 作为可学习参数</li>
<li>在小模型场景下表现更稳定</li>
</ul>
<hr />
<h2><strong>七、任务适配与训练策略</strong></h2>
<p>P-Tuning v2 在工程实现中引入了多项关键策略：</p>
<ul>
<li>
<p>针对不同任务设置不同 Prompt Length</p>
<ul>
<li>简单分类任务：较短 Prompt 即可</li>
<li>阅读理解、序列标注：需要更长 Prompt</li>
</ul>
</li>
<li>
<p>引入多任务 Prompt 预训练</p>
<ul>
<li>缓解 Prompt 随机初始化带来的优化困难</li>
<li>提升在复杂任务上的泛化能力</li>
</ul>
</li>
<li>
<p>回归传统分类头设计</p>
<ul>
<li>不再依赖 Label Word Verbalizer</li>
<li>使用标准 Classification Head</li>
<li>显著提升在序列标注任务中的适用性</li>
</ul>
</li>
</ul>
<hr />
<h2><strong>八、实验结果总结</strong></h2>
<p>实验结果显示：</p>
<ul>
<li>在 330M–10B 不同规模模型上，P-Tuning v2 均可达到与全参数微调相当的性能</li>
<li>在 RTE、NER、QA、SRL 等复杂序列任务上，P-Tuning v2 明显优于 Prompt Tuning 与 P-Tuning v1</li>
<li>Prompt Length 对性能影响显著，且随任务复杂度提升而增加</li>
</ul>
<p>这些结果表明，P-Tuning v2 在规模与任务维度上均具备良好的通用性。</p>
<hr />
<h2><strong>九、方法定位总结</strong></h2>
<p>从方法演进角度看：</p>
<ul>
<li><strong>P-Tuning</strong>：对 Prompt Tuning 的连续化与可微改进</li>
<li><strong>P-Tuning v2</strong>：将 Prefix Tuning 的深度思想引入 NLU 场景，并进行工程化优化</li>
</ul>
<p>P-Tuning v2 在多个模型规模与任务类型中展现出与全参数微调相媲美的性能，可作为参数高效微调方法的重要基线之一。</p>
<hr />
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/5"/><category term="微调"/><published>2025-12-13T11:33:50+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/4</id><title>GRPO 详解：DeepSeek-R1/DeepSeekMath 为什么用它替代 PPO？</title><updated>2025-12-16T03:28:16.157653+00:00</updated><content type="html"><![CDATA[<p>过去一年，“推理模型（Reasoning LLM）”的对齐路线几乎被反复验证了一件事：</p>
<p><strong>SFT 只是起点，真正把推理能力拉起来的，往往是强化学习阶段。</strong></p>
<p>而在 DeepSeek-R1 相关讨论里，GRPO（Group Relative Policy Optimization）频繁出现，原因并不玄学——它来自 DeepSeek 之前的工作《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》，并在推理/可验证任务上表现出很强的工程合理性。</p>
<p>这篇文章总结一条完整逻辑链：</p>
<ul>
<li>为什么从 PPO 走向 GRPO</li>
<li>GRPO 的目标函数到底在干嘛</li>
<li>结果监督 vs 过程监督的 GRPO</li>
<li>迭代 RL（reward model 跟不上怎么办）</li>
<li>DeepSeekMath 的一个很“硬核”的发现：<strong>RL 为什么有效</strong></li>
</ul>
<hr />
<h2><strong>1. 背景：LLM 对齐训练的常见路线</strong></h2>
<p>在大模型微调里，一个典型流程是：</p>
<p><strong>预训练 → SFT（监督微调）→ RL（强化学习对齐）</strong></p>
<p>SFT 让模型“会答题”，但 RL 往往能让模型“更稳定地答对题”。尤其在数学/代码/逻辑推理这种任务中，RL 的提升往往更明显。</p>
<p>问题是：传统 RL 对齐里最常用的 PPO，在 LLM 语境下有几处天然痛点。</p>
<hr />
<h2><strong>2. 从 PPO 到 GRPO：PPO 在大模型上的痛点是什么？</strong></h2>
<p>PPO（Proximal Policy Optimization）被广泛用在 RLHF/RLAIF 里。它的核心是最大化一个“带截断的替代目标”，并借助优势函数 A 做策略更新。</p>
<p>在 LLM 场景，PPO 常见训练形式还会加入 <strong>KL 约束</strong>，防止策略偏离参考模型（通常是 SFT 模型）太多。</p>
<h3><strong>PPO 的关键问题</strong></h3>
<p><strong>(1) 必须训练一个 Value/Critic（值函数）</strong></p>
<p>值函数通常规模接近策略模型，带来显著的内存与算力开销。更关键的是：critic 训练不稳定、和 policy 不匹配时，训练容易“崩”。</p>
<p><strong>(2) 工程复杂度高</strong></p>
<p>Actor + Critic + GAE + KL 控制 + clip，各组件耦合，调参成本高。</p>
<p><strong>(3) LLM 的奖励结构让 critic 更难学</strong></p>
<p>很多对齐设置里，奖励模型只在<strong>最后一个 token</strong>（答案结束）给一个整体分数。</p>
<p>这使得“token 级价值估计”在训练上更复杂，critic 的学习信号更弱、更不稳定。</p>
<p>于是一个很现实的想法出现了：</p>
<blockquote>
<p>很多时候我们并不需要“绝对奖励值”，只需要在同一个 prompt 下<strong>哪条回答更好</strong>。</p>
</blockquote>
<p>这就是 GRPO 的出发点。</p>
<hr />
<h2><strong>3. GRPO 的核心思想：用“组内相对优势”替代 Critic</strong></h2>
<h3><strong>3.1 一句话定义</strong></h3>
<p><strong>GRPO 是一种不需要 Critic 的强化学习对齐方法</strong>：</p>
<p>对同一 prompt 采样一组回答，用奖励模型打分，然后用<strong>组内相对差异</strong>构造优势，直接更新策略。</p>
<p>关键词：<strong>group-relative / no critic / 替代 PPO / 更稳定更省</strong></p>
<h3><strong>3.2 GRPO 的训练流程</strong></h3>
<p>给定一个问题（prompt）q，从旧策略采样一组输出（group）：</p>
<p>${o_1, o_2, \dots, o_G} \sim \pi_{\theta_{\text{old}}}(\cdot|q)$</p>
<p>用奖励模型（或规则验证器）对每个输出打分：</p>
<p>$r_i = R(q, o_i)$</p>
<p>接下来是 GRPO 的灵魂：构造<strong>组内相对优势</strong>（baseline 不再来自 value model，而来自组平均）：</p>
<p>一种常见形式是：</p>
<p>$A_i = r_i - \frac{1}{G}\sum_{j=1}^{G} r_j$</p>
<p>很多实现里还会做标准化（你材料里也提到）：</p>
<p>$\hat r_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)}$</p>
<p>然后把优势信号用于策略更新，形式上类似 PPO 的“比率 + clip”，但<strong>不需要 Critic/GAE</strong>。</p>
<hr />
<h2><strong>4. GRPO 的目标函数：它到底优化了什么？</strong></h2>
<p>GRPO 用如下目标对策略进行优化（直觉上是 PPO 风格的 surrogate objective + KL 正则）：</p>
<ul>
<li>PPO 里常见做法是把 KL 惩罚放进 reward 里（token-level reward shaping）</li>
<li>GRPO 的关键区别是：<strong>把 KL 作为损失函数里的正则项</strong>，而不是塞进 reward</li>
</ul>
<p>直觉很重要：</p>
<blockquote>
<p>PPO：奖励里加 KL 惩罚（reward shaping）</p>
</blockquote>
<blockquote>
<p>GRPO：损失里加 KL 正则（regularization）</p>
</blockquote>
<p>这让训练更干净，工程上更容易控制。</p>
<p>此外，GRPO 还使用一个无偏估计来计算 KL 散度，并强调该值为正，用于稳定训练。</p>
<hr />
<h2><strong>5. 结果监督 GRPO：只有“最终答案分”也能训起来</strong></h2>
<p>把 RL 分成了“结果监督”和“过程监督”，这是推理模型训练里非常关键的区分。</p>
<h3><strong>5.1 结果监督（Outcome Supervision）</strong></h3>
<p>对于同一个问题 q，采样 G 个输出 $o_i$。</p>
<p>奖励模型给每个完整输出一个分数 $r_i$，并进行组内标准化：</p>
<p>$\hat r_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)}$</p>
<p>然后把这个标准化奖励作为该输出<strong>所有 token 的优势</strong>：</p>
<p>$\hat A_{i,t} = \hat r_i$</p>
<p>也就是说：</p>
<p><strong>整段输出的每个 token，都共享同一个优势信号。</strong></p>
<p>这在工程上很“粗”，但非常省事，且在很多任务上确实有效。</p>
<hr />
<h2><strong>6. 过程监督 GRPO：推理任务为什么需要 step-level reward？</strong></h2>
<p>结果监督的问题也很明显：</p>
<p>对于复杂数学推理，仅在最终答案给奖励，信号太稀疏。</p>
<h3><strong>6.1 过程监督（Process Supervision）</strong></h3>
<p>过程奖励模型会在推理步骤的结束 token（例如每个 step 的结束标记）给分：</p>
<p>对第 i 个输出，共有 $K_i$ 个步骤，步骤结束位置索引为 $\text{index}(j)$，得到奖励序列：</p>
<p>${r^i_{\text{index}(1)}, \dots, r^i_{\text{index}(K_i)}}$</p>
<p>同样做 mean/std 标准化得到 $\tilde r$。</p>
<p>然后每个 token 的优势定义为<strong>从当前 token 往后的 step reward 之和</strong>：</p>
<p>$\hat A_{i,t} = \sum_{\text{index}(j)\ge t} \tilde r^i_{\text{index}(j)}$</p>
<p>直觉上：</p>
<p>token 越早，能“背负”的后续推理步骤越多；</p>
<p>这使得过程监督能更有效地塑形推理轨迹，而不只是塑形最终答案。</p>
<hr />
<h2><strong>7. 迭代强化学习：reward model 跟不上 policy 怎么办？</strong></h2>
<p>一个非常现实的问题：</p>
<blockquote>
<p>随着策略模型不断变强，旧的奖励模型可能无法有效监督新的策略分布。</p>
</blockquote>
<p>于是 DeepSeekMath 引入了<strong>迭代 RL + GRPO</strong>：</p>
<ul>
<li>每次迭代，用当前策略生成数据</li>
<li>构建新的奖励模型训练集</li>
<li>用 replay 机制持续训练奖励模型（历史数据占比约 10%）</li>
</ul>
<p>这本质是在做一件事：</p>
<blockquote>
<p>让 RM 跟上 policy 的分布漂移，否则 RL 只会“优化一个过时的打分器”。</p>
</blockquote>
<p>这也是很多 RLHF 工程里不可避免的循环：<strong>policy 变强 → 分布变了 → RM 失效 → 必须续训 RM</strong>。</p>
<hr />
<h2><strong>8. DeepSeekMath 的一个关键发现：RL 为什么有效？</strong></h2>
<p>DeepSeekMath 对比了 SFT 与 RL 后模型的 <strong>Pass@K</strong> 与 <strong>Maj@K</strong>：</p>
<ul>
<li>RL <strong>显著提升 Maj@K</strong></li>
<li>RL <strong>没有提升 Pass@K</strong></li>
</ul>
<p>这说明：</p>
<blockquote>
<p>RL 的主要作用不是把“TopK 里的上限”推高（能力上限不变），</p>
</blockquote>
<blockquote>
<p>而是让输出分布更稳健，让正确答案更容易成为“多数/常见输出”。</p>
</blockquote>
<p>一句话翻译成大白话：</p>
<p><strong>RL 更像是在“调分布”，让你更常答对，而不是让你会更多。</strong></p>
<p>这也解释了为什么推理模型的 RL 往往让表现更“稳”、更“像会推理”。</p>
<hr />
<h2><strong>9. 统一范式：SFT / DPO / PPO / GRPO 本质上在优化什么？</strong></h2>
<p>所有方法的梯度可以写成一个统一形式，包含三个关键组成部分：</p>
<ol>
<li>数据源 D（训练数据从哪来）</li>
<li>奖励函数（训练信号来源，如 RM/verifier）</li>
<li>算法 A（如何把数据与奖励变成梯度系数）</li>
</ol>
<p>在这个范式下：</p>
<ul>
<li><strong>SFT</strong>：用人类挑选的数据监督学习</li>
<li><strong>RFT</strong>：对模型采样结果做过滤再监督</li>
<li><strong>DPO</strong>：对成对偏好做离线优化（偏监督）</li>
<li><strong>Online RFT</strong>：用实时 policy 采样 + 过滤</li>
<li><strong>PPO/GRPO</strong>：用实时 policy 采样 + 强化学习更新</li>
</ul>
<blockquote>
<p>方法很多，但核心变量其实就三个：数据从哪来、奖励怎么给、算法怎么用奖励更新模型。</p>
</blockquote>
<hr />
<h2><strong>10. GRPO vs PPO vs DPO：面试最容易问的三角对比</strong></h2>
<h3><strong>10.1 GRPO vs PPO</strong></h3>
<ul>
<li>PPO 的优势依赖 Critic / GAE</li>
<li>GRPO 用组内均值当 baseline，直接构造相对优势</li>
<li>GRPO 工程更简单、成本更低、稳定性更依赖 reward 排序质量</li>
</ul>
<p>一句话：</p>
<p><strong>PPO 学“我比期望好多少”，GRPO 学“我比同组其他答案好多少”。</strong></p>
<h3><strong>10.2 GRPO vs DPO</strong></h3>
<ul>
<li>DPO：离线偏好数据（chosen/rejected），偏监督学习</li>
<li>GRPO：在线采样 + reward 打分，是真 RL（但简化了 critic）</li>
</ul>
<p>一句话：</p>
<p><strong>DPO 用人类给的对比对；GRPO 用模型自己采样的一组答案做对比。</strong></p>
<hr />
<h2><strong>11. GRPO 特别适合哪些任务？</strong></h2>
<p>推理模型偏爱 GRPO 的原因，是它天然适合“可自动打分”的任务：</p>
<ul>
<li>数学（verifier 判对错）</li>
<li>代码（单测/编译/静态分析）</li>
<li>逻辑推理（规则检查）</li>
<li>任何能构建稳定 reward 的任务</li>
</ul>
<p>当 reward 足够可靠时，组内排序会非常稳定，GRPO 的优势就会被放大。</p>
<hr />
<h2><strong>12. 局限与工程注意点</strong></h2>
<p>GRPO 也不是银弹，也有风险：</p>
<ul>
<li>需要同 prompt 多采样（group size G）→ 成本来自采样</li>
<li>group 太小 → 方差大，排序不稳定</li>
<li>reward 噪声大 → 相对优势抖动，训练不稳</li>
</ul>
<p>工程上通常会做：</p>
<ul>
<li>合理的 G（group size）选择</li>
<li>KL 正则强度控制</li>
<li>clip 稳定训练</li>
<li>更可靠的 verifier / PRM 构建与迭代</li>
</ul>
<hr />
<h1><strong>总结：为什么 DeepSeek-R1/推理模型会偏爱 GRPO？</strong></h1>
<blockquote>
<p><strong>GRPO 通过“同 prompt 组内比较”构造相对优势，去掉 critic，极大降低 RL 对齐的工程复杂度与训练成本，并在可验证推理任务上更稳定、更有效。</strong></p>
</blockquote>
<blockquote>
<p>DeepSeekMath 的证据表明，RL 的提升往往来自“让正确答案更常出现”（Maj@K ↑），而不是把能力上限推高（Pass@K 不变）。</p>
</blockquote>
<blockquote>
<p>这也解释了为什么 GRPO 这类稳定分布的 RL 方法，在推理模型时代特别吃香。</p>
</blockquote>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/4"/><category term="PPO"/><published>2025-12-13T07:30:41+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/3</id><title>QLoRA 深度解析：用 4bit 量化，把 70B 大模型拉进单卡时代</title><updated>2025-12-16T03:28:16.337961+00:00</updated><content type="html"><![CDATA[<h2><strong>前言：QLoRA 为什么值得被认真理解？</strong></h2>
<p>QLoRA 的作者 <strong>Tim Dettmers</strong>，是模型量化领域的核心人物之一，同时也深度参与了 <strong>BLOOM</strong> 等大模型的工程化落地。</p>
<p>模型量化（Quantization）和参数高效微调（PEFT）看似属于两个方向，但它们背后有一个共同目标：</p>
<blockquote>
<p><strong>让大模型的训练与推理更快、更省、更可落地。</strong></p>
</blockquote>
<p>QLoRA 正是将“<strong>量化</strong>”与“<strong>LoRA 微调</strong>”这两条路线真正融合到了一起，首次实现了：</p>
<blockquote>
<p><strong>在 4bit 量化模型上稳定训练 LoRA，且效果几乎不损失。</strong></p>
</blockquote>
<p>这让 <strong>70B 级别模型的单卡微调</strong> 成为现实。</p>
<hr />
<h2><strong>一、为什么需要 QLoRA？</strong></h2>
<h3><strong>1. 传统微调的显存瓶颈</strong></h3>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>显存需求</strong></th>
<th><strong>问题</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>全参数微调</td>
<td>极高（65B &gt;300GB）</td>
<td>完全不可落地</td>
</tr>
<tr>
<td>LoRA（FP16）</td>
<td>较高（65B &gt;120GB）</td>
<td>模型仍需 FP16 加载</td>
</tr>
<tr>
<td>GPTQ / AWQ</td>
<td>低</td>
<td>只能推理，难以训练</td>
</tr>
</tbody></table><p>结论非常明确：</p>
<blockquote>
<p><strong>我们需要一种：既能量化、又能训练的方案。</strong></p>
</blockquote>
<p>QLoRA 就是这个答案。</p>
<hr />
<h2><strong>二、QLoRA 的一句话定义</strong></h2>
<blockquote>
<p><strong>QLoRA（Quantized LoRA）</strong></p>
</blockquote>
<blockquote>
<p>是一种在 <strong>4bit 量化模型上冻结主权重，仅训练 LoRA 低秩参数</strong> 的高效微调方法，在显存降低约 75% 的同时，训练效果接近甚至优于全参微调。</p>
</blockquote>
<hr />
<h2><strong>三、QLoRA 的整体技术框架</strong></h2>
<p>QLoRA 并不是单一技术，而是<strong>四项关键工程与算法设计的组合</strong>：</p>
<ol>
<li><strong>NF4（4bit Normal Float）量化</strong></li>
<li><strong>分块 + 分位数量化</strong></li>
<li><strong>双重量化（Double Quantization）</strong></li>
<li><strong>分页优化（Paged Optimizer / Paged Attention）</strong></li>
<li><strong>LoRA Adapter 训练</strong></li>
</ol>
<p>下面逐一拆解。</p>
<hr />
<h2><strong>四、模型量化基础（理解 QLoRA 的前提）</strong></h2>
<h3><strong>4.1 什么是模型量化？</strong></h3>
<p>模型量化的目标是：</p>
<blockquote>
<p><strong>用更低精度表示权重，在可接受的误差范围内减少模型体积和显存占用。</strong></p>
</blockquote>
<p>本质上，这是一个<strong>有损压缩问题</strong>。</p>
<p>常见量化方式包括：</p>
<ul>
<li>线性量化（absmax）</li>
<li>零点量化（zero-point）</li>
<li>非线性量化（分位数量化）</li>
</ul>
<hr />
<h3><strong>4.2 分位数量化（Quantile Quantization）</strong></h3>
<p>QLoRA 的关键观察是：</p>
<blockquote>
<p><strong>大模型权重通常近似服从正态分布。</strong></p>
</blockquote>
<p>因此，与其用线性区间均匀切分，不如：</p>
<ul>
<li>按 <strong>CDF 等概率切分</strong></li>
<li>让每个量化 bin 出现频率尽可能一致</li>
</ul>
<p>这就是 <strong>分位数量化</strong> 的核心思想。</p>
<hr />
<h2><strong>五、NF4：QLoRA 能在 4bit 下不掉点的根本原因</strong></h2>
<h3><strong>5.1 一句话理解 NF4</strong></h3>
<blockquote>
<p><strong>NF4 是一种专为大模型权重分布设计的 4bit 非均匀量化格式，通过将有限的 4bit 表示能力集中在高概率区域，从而在极低精度下最大化表达能力。</strong></p>
</blockquote>
<p>它不是“更激进的 INT4”，而是<strong>完全不同的量化思路</strong>。</p>
<hr />
<h3><strong>5.2 为什么传统 INT4 不适合大模型？</strong></h3>
<p>INT4 采用的是<strong>均匀量化</strong>：</p>
<ul>
<li>4bit → 16 个等距离散值</li>
<li>默认假设数值分布是均匀的</li>
</ul>
<p>但大模型（LLM）的权重分布具有非常明确的统计特征：</p>
<ul>
<li>近似 <strong>零均值</strong></li>
<li>近似 <strong>正态分布</strong></li>
<li><strong>大量权重集中在 0 附近</strong></li>
<li>极端大值数量极少，但幅度大</li>
</ul>
<p>这会直接导致 INT4 的问题：</p>
<ul>
<li>中心区域（最重要的权重区间）精度不足</li>
<li>量化桶大量浪费在几乎不会出现的区间</li>
<li>少量误差被放大为性能下降</li>
</ul>
<p><strong>换句话说：INT4 在“统计意义上”并不匹配 LLM。</strong></p>
<hr />
<h3><strong>5.3 NF4 的核心思想：非均匀量化</strong></h3>
<p>NF4 的设计目标非常明确：</p>
<blockquote>
<p><strong>把有限的 16 个量化值，用在最“值得用”的地方。</strong></p>
</blockquote>
<p>具体来说：</p>
<ul>
<li>在 <strong>0 附近分布更密</strong></li>
<li>在 <strong>两端极值区域分布更稀</strong></li>
</ul>
<p>这使得：</p>
<ul>
<li>常见的小权重 → 高精度表示</li>
<li>罕见的大权重 → 容忍更大误差</li>
</ul>
<p>这种设计与 LLM 权重的真实分布高度匹配。</p>
<hr />
<h3><strong>5.4 NF4 是怎么实现的？</strong></h3>
<h4><strong>1️⃣ 预定义 16 个“浮点量化值”</strong></h4>
<p>NF4 并不是简单的整数映射，而是<strong>预先定义好的 16 个浮点值</strong>，这些值近似来自：</p>
<ul>
<li>标准正态分布的分位点（quantiles）</li>
<li>经过归一化与截断处理</li>
</ul>
<p>你可以把它理解为：</p>
<pre><code>[-a, ..., -small, 0, +small, ..., +a]
</code></pre>
<p>但它们<strong>不是等距的</strong>，而是统计意义上更合理的分布。</p>
<hr />
<h4><strong>2️⃣ 权重映射到最近的 NF4 值</strong></h4>
<p>量化过程本质上是一个最近邻映射：</p>
<p>$q = \arg\min_{v \in \text{NF4 values}} |w - v|$</p>
<p>反量化时：</p>
<p>$\hat{w} = s \cdot q$</p>
<p>其中：</p>
<ul>
<li>q：4bit index</li>
<li>s：FP16（或经双重量化后的）scale</li>
</ul>
<p><strong>scale 的精度非常关键</strong>，它决定了整个 block 的动态范围。</p>
<hr />
<h3><strong>5.5 为什么 NF4 比 INT4 精度高？</strong></h3>
<h4><strong>（1）分布匹配</strong></h4>
<ul>
<li>LLM 权重 ≈ 正态分布</li>
<li>NF4 量化点 ≈ 正态分布分位点</li>
</ul>
<p>→ 在统计意义上是近似最优的 4bit 表示。</p>
<hr />
<h4><strong>（2）误差集中在“不重要区域”</strong></h4>
<ul>
<li>小权重：数量多、贡献大 → 高精度</li>
<li>大权重：数量少 → 容忍误差</li>
</ul>
<p>整体误差被有效控制。</p>
<hr />
<h4><strong>（3）论文实验结论</strong></h4>
<p>QLoRA 原论文明确指出：</p>
<blockquote>
<p><strong>NF4 在 4bit 条件下的量化误差，接近 FP16，显著优于 INT4。</strong></p>
</blockquote>
<p>这正是 QLoRA 能在 4bit 下“几乎不掉点”的关键原因。</p>
<hr />
<h3><strong>5.6 NF4 在 QLoRA 中的角色定位</strong></h3>
<p>在 QLoRA 框架中，各部分分工非常清晰：</p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>精度</strong></th>
<th><strong>是否训练</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Base 权重</td>
<td>NF4</td>
<td>❌ 冻结</td>
<td>压缩模型、降低显存</td>
</tr>
<tr>
<td>Scale</td>
<td>FP16 / 双量化</td>
<td>❌</td>
<td>恢复动态范围</td>
</tr>
<tr>
<td>LoRA A/B</td>
<td>FP16 / BF16</td>
<td>✅</td>
<td>学习新任务、补偿误差</td>
</tr>
</tbody></table><p>可以这样理解：</p>
<blockquote>
<p><strong>NF4 负责“把模型装进显存”，</strong></p>
</blockquote>
<blockquote>
<p><strong>LoRA 负责“把能力补回来”。</strong></p>
</blockquote>
<hr />
<h3>5.7 NF4 vs INT4</h3>
<table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>INT4</strong></th>
<th><strong>NF4</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>量化方式</td>
<td>均匀</td>
<td>非均匀</td>
</tr>
<tr>
<td>感知权重分布</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>中心区域精度</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>是否适合 LLM</td>
<td>一般</td>
<td>非常适合</td>
</tr>
<tr>
<td>QLoRA 默认</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody></table><p>一句话总结：</p>
<blockquote>
<p><strong>INT4 是通用压缩方案，NF4 是为大模型定制的 4bit 浮点格式。</strong></p>
</blockquote>
<hr />
<h3><strong>5.8 一个常被追问的点</strong></h3>
<p><strong>NF4 是浮点还是整数？</strong></p>
<ul>
<li><strong>逻辑上</strong>：4bit 浮点格式</li>
<li><strong>实现上</strong>：4bit index + FP16 scale</li>
</ul>
<p>它不是传统 IEEE 浮点，而是<strong>为神经网络权重定制的表示形式</strong>。</p>
<hr />
<h2><strong>小结</strong></h2>
<p>如果没有 NF4：</p>
<ul>
<li>4bit 量化 ≈ 性能大幅下降</li>
<li>LoRA 无法完全补偿量化误差</li>
</ul>
<p>而正是 NF4 的存在，才让下面这件事成立：</p>
<blockquote>
<p><strong>在 4bit 条件下冻结主权重，只训练少量 LoRA 参数，依然可以达到接近全参微调的效果。</strong></p>
</blockquote>
<p>这也是为什么——</p>
<p><strong>几乎所有严肃的 QLoRA 实现，都会默认使用 NF4。</strong></p>
<hr />
<h2><strong>六、双重量化：进一步压缩显存</strong></h2>
<p>在 4bit 量化中：</p>
<ul>
<li>权重 → 4bit</li>
<li>量化常数 c → FP32（显存杀手）</li>
</ul>
<p>QLoRA 的解决方案是：</p>
<blockquote>
<p><strong>对量化常数本身再做一次 8bit 量化。</strong></p>
</blockquote>
<p>即：</p>
<ul>
<li>第一层：权重量化</li>
<li>第二层：量化常数量化</li>
</ul>
<p>结果：</p>
<ul>
<li>量化常数显存占用从 <strong><del>1.6% → </del>0.37%</strong></li>
<li>几乎没有额外精度损失</li>
</ul>
<hr />
<h2><strong>七、Paged Optimizer：避免训练 OOM 的关键工程技巧</strong></h2>
<p>即使权重量化，训练中仍存在一个风险：</p>
<blockquote>
<p><strong>显存峰值（activation + gradient checkpoint）导致 OOM。</strong></p>
</blockquote>
<p>QLoRA 引入 <strong>分页优化（Paged Optimizer / Paged Attention）</strong>：</p>
<ul>
<li>将部分 KV cache / 激活页换出到 CPU 内存</li>
<li>类似操作系统的虚拟内存分页机制</li>
<li>在显存紧张时动态调度</li>
</ul>
<p>这使得：</p>
<blockquote>
<p><strong>13B–70B 模型在单卡训练成为可能。</strong></p>
</blockquote>
<hr />
<h2><strong>八、QLoRA 如何与 LoRA 结合？</strong></h2>
<p>核心公式仍然是 LoRA 的形式：</p>
<p>$W' = W + \Delta W,\quad \Delta W = BA$</p>
<p>区别在于：</p>
<ul>
<li>W：4bit NF4 量化权重（冻结）</li>
<li>A, B：FP16 / BF16 LoRA 参数（可训练）</li>
</ul>
<p>训练时：</p>
<ul>
<li>主权重仅反量化参与前向</li>
<li><strong>梯度只回传到 LoRA</strong></li>
</ul>
<p>这也是 QLoRA 能“量化 + 训练”共存的根本原因。</p>
<hr />
<h2><strong>九、QLoRA 的能力边界</strong></h2>
<h3><strong>支持的任务</strong></h3>
<table>
<thead>
<tr>
<th><strong>能力</strong></th>
<th><strong>是否支持</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>SFT</td>
<td>✅</td>
</tr>
<tr>
<td>DPO / ORPO</td>
<td>✅</td>
</tr>
<tr>
<td>多轮对话</td>
<td>✅</td>
</tr>
<tr>
<td>单卡 70B</td>
<td>✅</td>
</tr>
</tbody></table><hr />
<h3><strong>不适合的场景</strong></h3>
<ul>
<li>模型结构级改动（attention 重构）</li>
<li>LoRA rank 太小（易欠拟合）</li>
<li>极端高精度任务（数学 / 代码）</li>
</ul>
<hr />
<h2><strong>十、总结：QLoRA 的真正价值</strong></h2>
<p>QLoRA 的本质不是“LoRA 的改进”，而是：</p>
<blockquote>
<p><strong>把大模型训练，从“算力竞赛”拉回到“工程理性”。</strong></p>
</blockquote>
<p>它证明了一件事：</p>
<ul>
<li>量化 ≠ 只能推理</li>
<li>低精度 ≠ 低性能</li>
</ul>
<p>在今天，QLoRA 已经成为：</p>
<blockquote>
<p><strong>企业 SFT / DPO / 垂域微调的事实标准方案。</strong></p>
</blockquote>
<hr />
<h2><strong>写在最后</strong></h2>
<p>这篇文章既是对 QLoRA 的系统性梳理，也是一种个人补课记录。</p>
<p>如果其中存在理解偏差，欢迎讨论与指正。</p>
<hr />
<h3><strong>参考</strong></h3>
<ul>
<li>[1] QLoRA: Efficient Finetuning of Quantized LLMs</li>
<li>[2] LoRA: Low-Rank Adaptation of Large Language Models</li>
</ul>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/3"/><category term="LoRA"/><published>2025-12-13T06:57:38+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/2</id><title>从 Function Calling 到 MCP：大模型如何真正接入真实世界？</title><updated>2025-12-16T03:28:16.537517+00:00</updated><content type="html"><![CDATA[<blockquote>
<p>当我们谈论 AI Agent、Copilot、企业级智能系统时，真正的分水岭早已不在模型参数量，而在一个更底层的问题上：</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>大模型，究竟是“会说话的百科全书”，还是“能动手做事的系统”？</strong></p>
</blockquote>
<p>Function Calling 和 MCP（Model Context Protocol），正是围绕这个问题展开的两代关键答案。</p>
<hr />
<h2><strong>一、没有工具的大模型，本质上是“被困在房间里的人”</strong></h2>
<p>早期的大语言模型，即便知识极其丰富，本质上仍然是一个<strong>封闭系统</strong>：</p>
<ul>
<li>无法访问实时数据</li>
<li>不能查询数据库</li>
<li>不能操作文件系统</li>
<li>不能调用外部 API</li>
</ul>
<p>它只能基于<strong>训练时学到的静态知识</strong>进行回答。</p>
<p>这在“解释概念”“生成文本”时问题不大，但一旦进入真实应用场景，就会立刻失效：</p>
<ul>
<li>查最新数据</li>
<li>进行精确计算</li>
<li>与业务系统交互</li>
<li>执行确定性任务</li>
</ul>
<p><strong>模型需要“手”和“工具”。</strong></p>
<hr />
<h2><strong>二、Function Calling：第一次让模型“会用工具”</strong></h2>
<h3><strong>1. Function Calling 是什么？</strong></h3>
<p>Function Calling 是 OpenAI 在 2023 年提出的一种能力扩展机制，其核心目标只有一个：</p>
<blockquote>
<p><strong>让模型在推理过程中，决定是否调用外部函数，并生成结构化参数。</strong></p>
</blockquote>
<p>需要特别强调的是：</p>
<ul>
<li>模型 <strong>不执行函数</strong></li>
<li>模型 <strong>只输出调用意图与参数</strong></li>
<li>真正的执行发生在宿主程序（Backend / Server）中</li>
</ul>
<p>换句话说，模型负责“想”，系统负责“做”。</p>
<hr />
<h3><strong>2. 典型工作流程</strong></h3>
<pre><code>用户输入
↓
LLM 推理
↓
生成 function_name + arguments（JSON）
↓
宿主系统执行函数
↓
结果返回给 LLM
↓
最终自然语言回答
</code></pre>
<p>示例（简化）：</p>
<pre><code>{
  &quot;name&quot;: &quot;get_weather&quot;,
  &quot;arguments&quot;: {
    &quot;city&quot;: &quot;北京&quot;,
    &quot;date&quot;: &quot;今天&quot;
  }
}
</code></pre>
<hr />
<h3><strong>3. 为什么说 Function Calling 是一次重要跃迁？</strong></h3>
<p>因为它第一次让大模型具备了：</p>
<ul>
<li>与真实系统交互的能力</li>
<li>执行确定性任务的可能性</li>
<li>构建 Agent 的基础动作接口</li>
</ul>
<p>我们今天在 Coze、Dify 等低代码 Agent 平台中看到的各种“插件”，本质上都是 <strong>Function Calling 的工程封装</strong>。</p>
<hr />
<h2><strong>三、Function Calling 的工程代价与瓶颈</strong></h2>
<p>Function Calling 虽然重要，但它从一开始就<strong>不是为大规模系统设计的标准</strong>。</p>
<h3><strong>1. 模型强依赖</strong></h3>
<ul>
<li>模型必须 <strong>原生支持 Function Calling</strong></li>
<li>甚至需要 <strong>专门的 Function Call 微调</strong></li>
</ul>
<p>例如在 ShareGPT 风格的数据集中，会出现专门的结构字段：</p>
<pre><code>{
  &quot;from&quot;: &quot;function_call&quot;,
  &quot;value&quot;: &quot;工具参数&quot;
}
</code></pre>
<p>这意味着：</p>
<blockquote>
<p><strong>不是所有模型天然就“会用工具”。</strong></p>
</blockquote>
<hr />
<h3><strong>2. 协议碎片化问题</strong></h3>
<p>不同模型、不同厂商的 Function Calling：</p>
<ul>
<li>Schema 不统一</li>
<li>调用格式不同</li>
<li>错误处理方式各异</li>
</ul>
<p>工程上，几乎不可避免地需要为每个模型维护一套适配层。</p>
<hr />
<h3><strong>3. 能力边界清晰但狭窄</strong></h3>
<p>Function Calling 更像是：</p>
<ul>
<li>单次</li>
<li>无状态</li>
<li>函数级别</li>
</ul>
<p>它非常适合：</p>
<ul>
<li>查天气</li>
<li>查订单</li>
<li>简单计算</li>
</ul>
<p>但一旦进入：</p>
<ul>
<li>多工具协作</li>
<li>状态管理</li>
<li>复杂资源访问</li>
</ul>
<p>就会迅速变得笨重。</p>
<hr />
<h2><strong>四、MCP 出现的背景：Function Calling 不够用了</strong></h2>
<h3><strong>1. MCP 是什么？</strong></h3>
<p><strong>MCP（Model Context Protocol）</strong> 是 Anthropic 提出的一种开放标准协议，其目标不是“再发明一种函数调用”，而是：</p>
<blockquote>
<p><strong>为大模型与外部世界之间，建立一个标准化、可控、可扩展的通信层。</strong></p>
</blockquote>
<p>一句话定义：</p>
<blockquote>
<p>MCP 是模型接入工具、数据和能力的“通用协议层”。</p>
</blockquote>
<hr />
<h3><strong>2. MCP 要解决的核心问题</strong></h3>
<p>Function Calling 无法优雅解决的问题包括：</p>
<ul>
<li>工具数量爆炸</li>
<li>数据源类型复杂（DB / 文件 / API / Git）</li>
<li>不同语言、不同部署环境</li>
<li>权限与安全边界不清晰</li>
<li>状态与上下文难以管理</li>
</ul>
<p>MCP 的目标非常明确：</p>
<blockquote>
<p><strong>把“外部能力”系统性地包装成模型可理解、可控制的上下文接口。</strong></p>
</blockquote>
<hr />
<h2><strong>五、MCP 的标准架构（关键）</strong></h2>
<p>MCP 采用典型的 <strong>Client–Server 架构</strong>：</p>
<pre><code>LLM（Client）
   ↕ MCP Protocol
MCP Server
   ├─ Tools
   ├─ Resources
   └─ Prompts
</code></pre>
<h3><strong>MCP Server 提供三类能力</strong></h3>
<h4><strong>1️⃣ Tools（可执行能力）</strong></h4>
<ul>
<li>标准化工具接口</li>
<li>明确输入 / 输出 schema</li>
<li>受控权限</li>
</ul>
<h4><strong>2️⃣ Resources（数据资源）</strong></h4>
<ul>
<li>文件系统</li>
<li>数据库</li>
<li>API 结果</li>
<li>Git 仓库</li>
<li>本地或远程状态</li>
</ul>
<p>模型可以“读资源”，而不是每次都靠 prompt 注入。</p>
<h4><strong>3️⃣ Prompts（上下文模板）</strong></h4>
<ul>
<li>系统级规范</li>
<li>业务约束</li>
<li>任务模板</li>
</ul>
<hr />
<h2><strong>六、Function Calling vs MCP：本质对比</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>Function Calling</strong></th>
<th><strong>MCP</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>抽象层级</td>
<td>函数级</td>
<td>系统级</td>
</tr>
<tr>
<td>本质</td>
<td>模型生成调用参数</td>
<td>模型接入外部世界的协议</td>
</tr>
<tr>
<td>状态管理</td>
<td>无</td>
<td>支持（Server 维护）</td>
</tr>
<tr>
<td>工具规模</td>
<td>少量</td>
<td>大规模</td>
</tr>
<tr>
<td>协议标准</td>
<td>私有实现</td>
<td>开放标准</td>
</tr>
<tr>
<td>扩展性</td>
<td>差</td>
<td>极强</td>
</tr>
<tr>
<td>适用场景</td>
<td>Demo / 小工具</td>
<td>Agent / 企业系统</td>
</tr>
</tbody></table><hr />
<h2><strong>七、一个非常直观的类比</strong></h2>
<ul>
<li>
<p><strong>Function Calling</strong>：</p>
<p>像是你打电话让秘书临时帮你办一件事</p>
</li>
<li>
<p><strong>MCP</strong>：</p>
<p>像是你被授予了一整套办公系统的受控访问权限</p>
</li>
</ul>
<p>前者是“临时动作”，后者是“长期协作”。</p>
<hr />
<h2><strong>八、工程选型建议（非常重要）</strong></h2>
<h3><strong>什么时候用 Function Calling？</strong></h3>
<ul>
<li>工具数量少（&lt;10）</li>
<li>功能简单</li>
<li>单体应用</li>
<li>快速验证</li>
</ul>
<p>👉 <strong>90% 的聊天机器人 Demo</strong></p>
<hr />
<h3><strong>什么时候必须用 MCP？</strong></h3>
<ul>
<li>多工具、多数据源</li>
<li>AI Agent / Copilot</li>
<li>企业级系统</li>
<li>本地文件 / 内网服务</li>
<li>强安全与可扩展要求</li>
</ul>
<p>👉 <strong>真正可落地的 Agent 系统</strong></p>
<hr />
<h2><strong>九、总结一句话</strong></h2>
<blockquote>
<p>Function Calling 解决的是“模型怎么调用一个函数”，</p>
</blockquote>
<blockquote>
<p>MCP 解决的是“模型如何安全、可扩展地接入真实世界”。</p>
</blockquote>
<p>在 Agent 体系中：</p>
<ul>
<li>Function Calling 更像是“动作接口”</li>
<li>MCP 更接近“模型的操作系统层”</li>
</ul>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/2"/><category term="MCP"/><published>2025-12-13T06:24:38+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/1</id><title>从 RAG 到 GraphRAG，再到 LightRAG</title><updated>2025-12-16T03:28:16.710763+00:00</updated><content type="html"><![CDATA[<h2><strong>引言：当 RAG 不再只是“找得到”</strong></h2>
<p>检索增强生成（Retrieval-Augmented Generation, RAG）已经成为 LLM 应用的事实标准。</p>
<p>它通过“<strong>先检索、再生成</strong>”的方式，在一定程度上缓解了大模型的幻觉问题。</p>
<p>但随着应用场景从<strong>局部事实问答</strong>走向<strong>跨文档推理、全局总结与复杂关系理解</strong>，一个问题逐渐显现：</p>
<blockquote>
<p><strong>RAG 能检索到信息，但并不真正“理解结构”。</strong></p>
</blockquote>
<p>GraphRAG 与 LightRAG，正是在这一背景下被提出的两种结构化检索方案。</p>
<hr />
<h2><strong>一、传统 RAG 的结构性瓶颈</strong></h2>
<p>传统 RAG 的系统抽象非常清晰：</p>
<ul>
<li>文档 → Chunk</li>
<li>Chunk → 向量</li>
<li>Query → 相似度检索 → 上下文拼接</li>
</ul>
<p>这种设计在以下任务中表现良好：</p>
<ul>
<li>明确事实查询</li>
<li>定位具体段落</li>
<li>局部知识补全</li>
</ul>
<p>但它存在一个<strong>无法通过调参或模型升级解决的系统性限制</strong>：</p>
<blockquote>
<p><strong>Chunk 是扁平的，而知识本身是结构化的。</strong></p>
</blockquote>
<h3><strong>结构缺失带来的直接后果</strong></h3>
<ul>
<li>文档之间的关联关系被切断</li>
<li>实体之间的因果、从属、演化关系无法显式建模</li>
<li>模型只能“拼上下文”，而非“基于结构推理”</li>
</ul>
<p>这意味着，当问题本身涉及<strong>整体脉络、跨文档关系或宏观总结</strong>时，传统 RAG 的能力上限会迅速暴露。</p>
<hr />
<h2><strong>二、GraphRAG 是什么：第一次把“图结构”带入 RAG</strong></h2>
<p><strong>GraphRAG</strong> 是微软提出的一种 RAG 扩展框架，其核心目标并不是“检索更准的文本”，</p>
<p>而是：</p>
<blockquote>
<p><strong>让 RAG 具备显式建模和利用“知识结构”的能力。</strong></p>
</blockquote>
<p>在 GraphRAG 中，知识不再被视为一堆独立的文本片段，而是被重构为一个<strong>由实体与关系组成的图结构（Knowledge Graph）</strong>。</p>
<p>RAG 的检索对象，也从“文本 Chunk”升级为“图中的结构单元”。</p>
<hr />
<h2><strong>三、GraphRAG：显式引入“全局结构”的系统设计</strong></h2>
<h3><strong>1. GraphRAG 的核心设计思路</strong></h3>
<p>GraphRAG 的关键思想可以概括为一句话：</p>
<blockquote>
<p><strong>既然知识天然是图结构的，那就直接构建图，并在图的层面进行检索与摘要。</strong></p>
</blockquote>
<p>为此，GraphRAG 在 indexing 阶段引入了一套完整但代价不菲的流程：</p>
<ol>
<li>
<p><strong>实体与关系抽取</strong></p>
<p>使用 LLM 从原始文本中抽取实体（人物、概念、事件）及其关系。</p>
</li>
<li>
<p><strong>知识图谱构建</strong></p>
<p>将实体作为节点，关系作为边，形成全局图结构。</p>
</li>
<li>
<p><strong>社区发现（Community Detection）</strong></p>
<p>使用 Leiden 等图算法，将高度相关的子图聚合为“社区”。</p>
</li>
<li>
<p><strong>社区级摘要生成</strong></p>
<p>为每个社区预先生成结构化摘要，作为后续检索的高层语义单元。</p>
</li>
</ol>
<hr />
<h3><strong>2. GraphRAG 解决了什么本质问题？</strong></h3>
<p>GraphRAG 首次在 RAG 框架中引入了<strong>全局结构感知能力</strong>：</p>
<ul>
<li>不再只关注“相关段落”</li>
<li>而是能够回答“整体结构上发生了什么”</li>
</ul>
<p>例如：</p>
<ul>
<li>“整个语料中反复出现的核心主题是什么？”</li>
<li>“多个事件之间是否存在系统性关联？”</li>
</ul>
<p>这类问题，本质上需要<strong>跨文本、跨实体、跨关系的全局推理</strong>，</p>
<p>而这正是 GraphRAG 的优势所在。</p>
<hr />
<h3><strong>3. 但系统代价同样明确</strong></h3>
<p>从工程角度看，GraphRAG 的主要问题并不在效果，而在<strong>系统可持续性</strong>：</p>
<ul>
<li>索引阶段 Token 成本极高</li>
<li>社区结构对数据变化高度敏感</li>
<li>增量更新几乎不可行</li>
</ul>
<p>因此，GraphRAG 更适合<strong>离线分析型任务</strong>，而非高频更新的在线系统。</p>
<hr />
<h2><strong>四、LightRAG 是什么：对 GraphRAG 的工程化回应</strong></h2>
<p><strong>LightRAG</strong> 并不是对 GraphRAG 的否定，而是一种<strong>面向工程现实的重新设计</strong>。</p>
<p>它试图回答的问题是：</p>
<blockquote>
<p><strong>是否一定要构建“全局静态结构”，才能让 RAG 具备结构化推理能力？</strong></p>
</blockquote>
<p>LightRAG 的答案是：<strong>不一定。</strong></p>
<hr />
<h2><strong>五、LightRAG：把结构留到 Query 时再用</strong></h2>
<h3><strong>1. 设计立场的根本转变</strong></h3>
<p>与 GraphRAG 相比，LightRAG 的核心转变在于：</p>
<ul>
<li>不再预生成全局社区摘要</li>
<li>保留图结构，但不“冻结”结构语义</li>
<li>将结构的使用推迟到 Query 阶段</li>
</ul>
<p>可以将两者的差异理解为：</p>
<blockquote>
<p>GraphRAG：<strong>结构是提前计算好的</strong></p>
</blockquote>
<blockquote>
<p>LightRAG：<strong>结构是按需被查询和展开的</strong></p>
</blockquote>
<hr />
<h3><strong>2. 双层检索的工程意义</strong></h3>
<p>LightRAG 采用了一种<strong>图结构 + 向量检索的协同机制</strong>：</p>
<ul>
<li><strong>向量层</strong>：快速召回相关实体或节点</li>
<li><strong>图结构层</strong>：基于关系进行上下文扩展与约束</li>
</ul>
<p>这种设计带来的直接好处是：</p>
<ul>
<li>支持增量更新</li>
<li>检索路径随 Query 动态变化</li>
<li>成本与延迟显著降低</li>
</ul>
<hr />
<h2><strong>六、三种 RAG 方案的系统级对比</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>传统 RAG</strong></th>
<th><strong>GraphRAG</strong></th>
<th><strong>LightRAG</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>知识建模</td>
<td>扁平文本</td>
<td>显式全局图</td>
<td>局部图 + 向量</td>
</tr>
<tr>
<td>全局理解</td>
<td>❌</td>
<td>✅</td>
<td>部分支持</td>
</tr>
<tr>
<td>索引成本</td>
<td>低</td>
<td>极高</td>
<td>中等</td>
</tr>
<tr>
<td>增量更新</td>
<td>易</td>
<td>困难</td>
<td>易</td>
</tr>
<tr>
<td>工程可用性</td>
<td>高</td>
<td>低</td>
<td>高</td>
</tr>
</tbody></table><hr />
<h2><strong>七、结论：Graph 不是目的，结构化推理才是</strong></h2>
<p>GraphRAG 与 LightRAG 的分歧，并不在“用不用图”，而在于：</p>
<blockquote>
<p><strong>结构化能力应当放在“离线建模”还是“在线推理”阶段。</strong></p>
</blockquote>
<ul>
<li>GraphRAG 代表<strong>认知完整性优先</strong></li>
<li>LightRAG 代表<strong>工程可落地性优先</strong></li>
</ul>
<p>在可预见的未来，更可能的方向是：</p>
<blockquote>
<p><strong>图结构 × 向量检索 × 动态推理路径的融合系统</strong></p>
</blockquote>
<hr />
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/1"/><category term="RAG"/><published>2025-12-13T05:46:09+00:00</published></entry></feed>