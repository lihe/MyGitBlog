<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom"><id>https://github.com/lihe/MyGitBlog</id><title>RSS feed of lihe's MyGitBlog</title><updated>2025-12-13T16:26:30.509129+00:00</updated><link href="https://github.com/lihe/MyGitBlog"/><link href="https://raw.githubusercontent.com/lihe/MyGitBlog/master/feed.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><entry><id>https://github.com/lihe/MyGitBlog/issues/6</id><title>Transformer 中 FFN 是做什么的？——为什么注意力之外还需要前馈网络</title><updated>2025-12-13T16:26:30.999002+00:00</updated><content type="html"><![CDATA[<p>在 Transformer 架构中，自注意力机制（Self-Attention）往往更容易被关注，但真正决定模型表达能力上限的核心模块之一，是每一层中与注意力并列存在的前馈神经网络（Feed-Forward Network, FFN）。FFN 并非简单的附属组件，而是 Transformer 能够建模复杂语义与非线性关系的关键所在。</p>
<img width="1225" height="384" alt="Image" src="https://github.com/user-attachments/assets/ebfb108f-344b-4773-977e-d0045407e3be" />
<hr />
<h2><strong>注意力与 FFN 的职责分工</strong></h2>
<p>从信息流的角度来看，Transformer 的每一层可以拆分为两类互补的计算过程：</p>
<ul>
<li>
<p><strong>自注意力机制</strong></p>
<p>负责在不同 token 之间建立依赖关系，对上下文信息进行加权聚合，解决“从哪里获取信息”的问题。</p>
</li>
<li>
<p><strong>前馈网络（FFN）</strong></p>
<p>负责在单个 token 内部对已聚合的信息进行非线性变换和特征重组，解决“如何理解和加工信息”的问题。</p>
</li>
</ul>
<p>两者形成了清晰的分工：注意力负责信息路由，FFN 负责信息加工。</p>
<hr />
<h2><strong>FFN 的标准结构</strong></h2>
<p>在经典 Transformer 中，FFN 是一个逐位置（position-wise）的两层全连接网络，其基本形式为：</p>
<p>$\text{FFN}(x) = W_2 , \sigma(W_1 x + b_1) + b_2$</p>
<p>其中：</p>
<ul>
<li>$x \in \mathbb{R}^{d_{\text{model}}}$ 是单个 token 的表示；</li>
<li>$W_1$ 将特征维度从 $d_{\text{model}} $升维到 $d_{\text{ff}}$；</li>
<li>$\sigma$ 是非线性激活函数（ReLU、GELU、SwiGLU 等）；</li>
<li>$W_2$ 再将特征映射回模型维度。</li>
</ul>
<p>这一网络对序列中每个位置独立作用，参数在所有位置共享，不引入 token 间的交互。</p>
<hr />
<h2><strong>升维—非线性—降维的设计动机</strong></h2>
<p>FFN 中“先升维、再降维”的结构并非偶然，而是 Transformer 表达能力的核心来源。</p>
<ol>
<li>
<p><strong>扩展表示空间</strong></p>
<p>中间层维度 $d_{\text{ff}}$ 通常是 $d_{\text{model}} $的 4 倍左右。高维空间提供了更强的函数拟合能力，使模型能够表示复杂的特征组合。</p>
</li>
<li>
<p><strong>引入非线性建模能力</strong></p>
<p>自注意力本质上是线性加权求和，只能重组已有特征，无法产生新的语义结构。FFN 通过非线性激活，使模型能够学习条件关系和高阶特征。</p>
</li>
<li>
<p><strong>特征解耦与重组</strong></p>
<p>升维后的空间允许不同语义子特征被分离、激活和重新组合，随后再压缩为更抽象、更有判别力的表示。</p>
</li>
</ol>
<p>从这一角度看，FFN 更接近 Transformer 中真正的“计算单元”，而注意力更多承担结构性信息组织的角色。</p>
<hr />
<h2><strong>FFN 对语义理解的作用</strong></h2>
<p>在经过注意力计算后，每个 token 的表示已经融合了上下文信息，但这些信息仍然是线性叠加的结果。FFN 在此基础上完成语义层面的再解释。</p>
<p>例如，在上下文信息已被聚合后，FFN 能够根据特征组合激活特定语义模式，将模糊的上下文混合表示映射为更明确的高层语义状态。这种能力并非来自 token 间交互，而是来自对单个 token 表示的深度非线性变换。</p>
<hr />
<h2><strong>为什么 FFN 采用逐位置设计</strong></h2>
<p>FFN 以逐位置方式作用，原因在于 Transformer 的结构分层原则：</p>
<ul>
<li>token 之间的依赖关系已经由自注意力显式建模；</li>
<li>FFN 专注于 token 内部的语义加工；</li>
<li>职责分离使模型结构更清晰、训练更稳定。</li>
</ul>
<p>这种设计也使 FFN 在计算上高度并行，适合大规模模型训练。</p>
<hr />
<h2><strong>缺失 FFN 的影响</strong></h2>
<p>如果 Transformer 中仅保留注意力而移除 FFN，那么整个网络将退化为线性映射的堆叠。即便层数增加，模型仍无法表示复杂的非线性函数，表达能力会受到根本性限制。</p>
<p>在实际的大语言模型中，FFN 不仅承担主要的非线性建模任务，其参数量和计算量也通常占据模型总量的绝大部分。这一事实从工程层面进一步印证了 FFN 在 Transformer 中的核心地位。</p>
<hr />
<h2><strong>FFN 的结构演进</strong></h2>
<p>随着模型规模和任务复杂度的提升，FFN 的形式也在不断演化：</p>
<ul>
<li><strong>GELU FFN</strong>：常见于 BERT、早期 GPT 系列</li>
<li><strong>SwiGLU / GEGLU</strong>：引入门控机制，提高表达效率（LLaMA、PaLM、T5）</li>
<li><strong>MoE-FFN</strong>：通过专家路由进一步提升模型容量（Mixtral、DeepSeek-MoE）</li>
</ul>
<p>尽管实现形式不同，这些变体仍遵循同一基本思想：在高维空间中进行非线性特征加工，再映射回模型表示空间。</p>
<hr />
<h2><strong>总结</strong></h2>
<p>在 Transformer 架构中，前馈神经网络承担着对每个 token 表示进行深度语义加工的职责。自注意力机制负责跨 token 的信息聚合，而 FFN 通过升维、非线性激活与降维，使模型具备建模复杂语义关系的能力。</p>
<p>正是这种“注意力负责结构，FFN 负责计算”的分工，使 Transformer 成为一种既具全局建模能力、又具强表达能力的通用序列建模架构。</p>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/6"/><category term="Transformer"/><published>2025-12-13T16:20:47+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/5</id><title>P-Tuning 与 P-Tuning v2：参数高效 Prompt 微调方法解析</title><updated>2025-12-13T16:26:31.193407+00:00</updated><content type="html"><![CDATA[<h2><strong>一、方法提出背景</strong></h2>
<p>在大语言模型的下游任务适配中，Prompt 的构造方式对模型性能具有决定性影响。以 GPT-3 为代表的 In-Context Learning 依赖人工设计的离散模板，但这种方式存在明显局限：</p>
<ul>
<li>模板对措辞、顺序和示例极其敏感</li>
<li>轻微修改即可导致性能大幅波动</li>
<li>离散 token 的自动化搜索成本高，且难以保证稳定最优</li>
</ul>
<p>同时，全参数微调虽然效果稳定，但在大模型场景下带来了显著的显存、算力与维护成本，并伴随潜在的灾难性遗忘风险。</p>
<p>在这一背景下，P-Tuning 被提出，其核心目标是在<strong>冻结模型主体参数的前提下，通过可学习 Prompt 实现稳定、高效的任务适配</strong>。</p>
<hr />
<h2><strong>二、P-Tuning 的核心思想</strong></h2>
<p>P-Tuning 将传统 Prompt 中的离散 token 替换为一组<strong>连续可微的虚拟 token（Virtual Tokens）</strong>，并直接在 embedding 空间中进行优化。</p>
<p>核心特征包括：</p>
<ul>
<li>Prompt 不再是自然语言文本，而是可训练向量</li>
<li>Prompt 作为模型输入的一部分参与前向传播</li>
<li>模型主体参数保持冻结，仅优化 Prompt 相关参数</li>
</ul>
<p>这一设计使 Prompt 构造从“人工经验问题”转化为“连续优化问题”。</p>
<hr />
<h2><strong>三、P-Tuning 的技术原理</strong></h2>
<h3><strong>1. 连续 Prompt 表示</strong></h3>
<p>P-Tuning 将人工模板中的真实 token 替换为一组虚拟 token，这些 token 以 embedding 的形式插入输入序列中。</p>
<p>虚拟 token 的插入位置并不固定，可以是前缀，也可以位于输入序列的其他位置。</p>
<h3><strong>2. Prompt Encoder 设计</strong></h3>
<p>由于预训练语言模型的 embedding 空间高度离散，若直接随机初始化虚拟 token，优化过程容易陷入局部最优。</p>
<p>为此，P-Tuning 引入 Prompt Encoder，对虚拟 token 进行结构化建模：</p>
<ul>
<li>使用 LSTM + MLP 对 Prompt embedding 进行编码</li>
<li>显式建模 Prompt token 之间的相关性</li>
<li>提升收敛速度与优化稳定性</li>
</ul>
<p>实验结果表明，引入 Prompt Encoder 后，P-Tuning 在多个任务上可以达到甚至超过全参数微调的效果。</p>
<hr />
<h2><strong>四、P-Tuning 的实验现象与结论</strong></h2>
<p>在对比实验中，P-Tuning 展现出若干值得注意的现象：</p>
<ul>
<li>在相同参数规模下，全参数微调时 BERT 在 NLU 任务中通常优于 GPT</li>
<li>在 P-Tuning 设定下，GPT 在多个任务上可反超 BERT</li>
<li>P-Tuning 在部分任务中可达到与全参数微调一致的性能</li>
</ul>
<p>这些结果表明，<strong>模型能力并不完全由参数更新方式决定，输入空间的可学习结构同样具有重要影响</strong>。</p>
<hr />
<h2><strong>五、P-Tuning v2 的提出动机</strong></h2>
<p>尽管 P-Tuning 在多个任务中取得了良好效果，但其仍存在明显局限：</p>
<h3><strong>1. 规模泛化能力不足</strong></h3>
<ul>
<li>在大模型（&gt;10B）上效果接近全参微调</li>
<li>在中小模型（100M–1B）上性能明显下降</li>
</ul>
<h3><strong>2. 任务泛化能力不足</strong></h3>
<ul>
<li>在部分 NLU 分类任务中表现良好</li>
<li>在序列标注等结构化任务上效果不稳定</li>
</ul>
<h3><strong>3. 提示深度受限</strong></h3>
<ul>
<li>Prompt 仅作用于输入 embedding 层</li>
<li>对深层 Transformer 表征的影响较为间接</li>
</ul>
<p>为解决上述问题，P-Tuning v2 被提出。</p>
<hr />
<h2><strong>六、P-Tuning v2 的核心改进</strong></h2>
<p>P-Tuning v2（论文 <em>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</em>）引入了<strong>深度 Prompt 优化机制</strong>。</p>
<h3><strong>1. 多层 Prompt 注入</strong></h3>
<ul>
<li>在 Transformer 的每一层引入 Prompt token</li>
<li>Prompt 直接参与中间层表示计算</li>
<li>对模型预测产生更直接的影响</li>
</ul>
<h3><strong>2. 参数规模提升但仍高效</strong></h3>
<ul>
<li>可学习参数比例从约 0.01% 提升至 0.1%–3%</li>
<li>相较全参数微调仍具显著参数效率优势</li>
</ul>
<h3><strong>3. 结构设计上的简化</strong></h3>
<ul>
<li>移除 P-Tuning v1 中的重参数化 Prompt Encoder</li>
<li>直接将 Prompt 作为可学习参数</li>
<li>在小模型场景下表现更稳定</li>
</ul>
<hr />
<h2><strong>七、任务适配与训练策略</strong></h2>
<p>P-Tuning v2 在工程实现中引入了多项关键策略：</p>
<ul>
<li>
<p>针对不同任务设置不同 Prompt Length</p>
<ul>
<li>简单分类任务：较短 Prompt 即可</li>
<li>阅读理解、序列标注：需要更长 Prompt</li>
</ul>
</li>
<li>
<p>引入多任务 Prompt 预训练</p>
<ul>
<li>缓解 Prompt 随机初始化带来的优化困难</li>
<li>提升在复杂任务上的泛化能力</li>
</ul>
</li>
<li>
<p>回归传统分类头设计</p>
<ul>
<li>不再依赖 Label Word Verbalizer</li>
<li>使用标准 Classification Head</li>
<li>显著提升在序列标注任务中的适用性</li>
</ul>
</li>
</ul>
<hr />
<h2><strong>八、实验结果总结</strong></h2>
<p>实验结果显示：</p>
<ul>
<li>在 330M–10B 不同规模模型上，P-Tuning v2 均可达到与全参数微调相当的性能</li>
<li>在 RTE、NER、QA、SRL 等复杂序列任务上，P-Tuning v2 明显优于 Prompt Tuning 与 P-Tuning v1</li>
<li>Prompt Length 对性能影响显著，且随任务复杂度提升而增加</li>
</ul>
<p>这些结果表明，P-Tuning v2 在规模与任务维度上均具备良好的通用性。</p>
<hr />
<h2><strong>九、方法定位总结</strong></h2>
<p>从方法演进角度看：</p>
<ul>
<li><strong>P-Tuning</strong>：对 Prompt Tuning 的连续化与可微改进</li>
<li><strong>P-Tuning v2</strong>：将 Prefix Tuning 的深度思想引入 NLU 场景，并进行工程化优化</li>
</ul>
<p>P-Tuning v2 在多个模型规模与任务类型中展现出与全参数微调相媲美的性能，可作为参数高效微调方法的重要基线之一。</p>
<hr />
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/5"/><category term="微调"/><published>2025-12-13T11:33:50+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/4</id><title>GRPO 详解：DeepSeek-R1/DeepSeekMath 为什么用它替代 PPO？</title><updated>2025-12-13T16:26:31.408464+00:00</updated><content type="html"><![CDATA[<p>过去一年，“推理模型（Reasoning LLM）”的对齐路线几乎被反复验证了一件事：</p>
<p><strong>SFT 只是起点，真正把推理能力拉起来的，往往是强化学习阶段。</strong></p>
<p>而在 DeepSeek-R1 相关讨论里，GRPO（Group Relative Policy Optimization）频繁出现，原因并不玄学——它来自 DeepSeek 之前的工作《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》，并在推理/可验证任务上表现出很强的工程合理性。</p>
<p>这篇文章总结一条完整逻辑链：</p>
<ul>
<li>为什么从 PPO 走向 GRPO</li>
<li>GRPO 的目标函数到底在干嘛</li>
<li>结果监督 vs 过程监督的 GRPO</li>
<li>迭代 RL（reward model 跟不上怎么办）</li>
<li>DeepSeekMath 的一个很“硬核”的发现：<strong>RL 为什么有效</strong></li>
</ul>
<hr />
<h2><strong>1. 背景：LLM 对齐训练的常见路线</strong></h2>
<p>在大模型微调里，一个典型流程是：</p>
<p><strong>预训练 → SFT（监督微调）→ RL（强化学习对齐）</strong></p>
<p>SFT 让模型“会答题”，但 RL 往往能让模型“更稳定地答对题”。尤其在数学/代码/逻辑推理这种任务中，RL 的提升往往更明显。</p>
<p>问题是：传统 RL 对齐里最常用的 PPO，在 LLM 语境下有几处天然痛点。</p>
<hr />
<h2><strong>2. 从 PPO 到 GRPO：PPO 在大模型上的痛点是什么？</strong></h2>
<p>PPO（Proximal Policy Optimization）被广泛用在 RLHF/RLAIF 里。它的核心是最大化一个“带截断的替代目标”，并借助优势函数 A 做策略更新。</p>
<p>在 LLM 场景，PPO 常见训练形式还会加入 <strong>KL 约束</strong>，防止策略偏离参考模型（通常是 SFT 模型）太多。</p>
<h3><strong>PPO 的关键问题</strong></h3>
<p><strong>(1) 必须训练一个 Value/Critic（值函数）</strong></p>
<p>值函数通常规模接近策略模型，带来显著的内存与算力开销。更关键的是：critic 训练不稳定、和 policy 不匹配时，训练容易“崩”。</p>
<p><strong>(2) 工程复杂度高</strong></p>
<p>Actor + Critic + GAE + KL 控制 + clip，各组件耦合，调参成本高。</p>
<p><strong>(3) LLM 的奖励结构让 critic 更难学</strong></p>
<p>很多对齐设置里，奖励模型只在<strong>最后一个 token</strong>（答案结束）给一个整体分数。</p>
<p>这使得“token 级价值估计”在训练上更复杂，critic 的学习信号更弱、更不稳定。</p>
<p>于是一个很现实的想法出现了：</p>
<blockquote>
<p>很多时候我们并不需要“绝对奖励值”，只需要在同一个 prompt 下<strong>哪条回答更好</strong>。</p>
</blockquote>
<p>这就是 GRPO 的出发点。</p>
<hr />
<h2><strong>3. GRPO 的核心思想：用“组内相对优势”替代 Critic</strong></h2>
<h3><strong>3.1 一句话定义</strong></h3>
<p><strong>GRPO 是一种不需要 Critic 的强化学习对齐方法</strong>：</p>
<p>对同一 prompt 采样一组回答，用奖励模型打分，然后用<strong>组内相对差异</strong>构造优势，直接更新策略。</p>
<p>关键词：<strong>group-relative / no critic / 替代 PPO / 更稳定更省</strong></p>
<h3><strong>3.2 GRPO 的训练流程</strong></h3>
<p>给定一个问题（prompt）q，从旧策略采样一组输出（group）：</p>
<p>${o_1, o_2, \dots, o_G} \sim \pi_{\theta_{\text{old}}}(\cdot|q)$</p>
<p>用奖励模型（或规则验证器）对每个输出打分：</p>
<p>$r_i = R(q, o_i)$</p>
<p>接下来是 GRPO 的灵魂：构造<strong>组内相对优势</strong>（baseline 不再来自 value model，而来自组平均）：</p>
<p>一种常见形式是：</p>
<p>$A_i = r_i - \frac{1}{G}\sum_{j=1}^{G} r_j$</p>
<p>很多实现里还会做标准化（你材料里也提到）：</p>
<p>$\hat r_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)}$</p>
<p>然后把优势信号用于策略更新，形式上类似 PPO 的“比率 + clip”，但<strong>不需要 Critic/GAE</strong>。</p>
<hr />
<h2><strong>4. GRPO 的目标函数：它到底优化了什么？</strong></h2>
<p>GRPO 用如下目标对策略进行优化（直觉上是 PPO 风格的 surrogate objective + KL 正则）：</p>
<ul>
<li>PPO 里常见做法是把 KL 惩罚放进 reward 里（token-level reward shaping）</li>
<li>GRPO 的关键区别是：<strong>把 KL 作为损失函数里的正则项</strong>，而不是塞进 reward</li>
</ul>
<p>直觉很重要：</p>
<blockquote>
<p>PPO：奖励里加 KL 惩罚（reward shaping）</p>
</blockquote>
<blockquote>
<p>GRPO：损失里加 KL 正则（regularization）</p>
</blockquote>
<p>这让训练更干净，工程上更容易控制。</p>
<p>此外，GRPO 还使用一个无偏估计来计算 KL 散度，并强调该值为正，用于稳定训练。</p>
<hr />
<h2><strong>5. 结果监督 GRPO：只有“最终答案分”也能训起来</strong></h2>
<p>把 RL 分成了“结果监督”和“过程监督”，这是推理模型训练里非常关键的区分。</p>
<h3><strong>5.1 结果监督（Outcome Supervision）</strong></h3>
<p>对于同一个问题 q，采样 G 个输出 $o_i$。</p>
<p>奖励模型给每个完整输出一个分数 $r_i$，并进行组内标准化：</p>
<p>$\hat r_i = \frac{r_i - \text{mean}(r)}{\text{std}(r)}$</p>
<p>然后把这个标准化奖励作为该输出<strong>所有 token 的优势</strong>：</p>
<p>$\hat A_{i,t} = \hat r_i$</p>
<p>也就是说：</p>
<p><strong>整段输出的每个 token，都共享同一个优势信号。</strong></p>
<p>这在工程上很“粗”，但非常省事，且在很多任务上确实有效。</p>
<hr />
<h2><strong>6. 过程监督 GRPO：推理任务为什么需要 step-level reward？</strong></h2>
<p>结果监督的问题也很明显：</p>
<p>对于复杂数学推理，仅在最终答案给奖励，信号太稀疏。</p>
<h3><strong>6.1 过程监督（Process Supervision）</strong></h3>
<p>过程奖励模型会在推理步骤的结束 token（例如每个 step 的结束标记）给分：</p>
<p>对第 i 个输出，共有 $K_i$ 个步骤，步骤结束位置索引为 $\text{index}(j)$，得到奖励序列：</p>
<p>${r^i_{\text{index}(1)}, \dots, r^i_{\text{index}(K_i)}}$</p>
<p>同样做 mean/std 标准化得到 $\tilde r$。</p>
<p>然后每个 token 的优势定义为<strong>从当前 token 往后的 step reward 之和</strong>：</p>
<p>$\hat A_{i,t} = \sum_{\text{index}(j)\ge t} \tilde r^i_{\text{index}(j)}$</p>
<p>直觉上：</p>
<p>token 越早，能“背负”的后续推理步骤越多；</p>
<p>这使得过程监督能更有效地塑形推理轨迹，而不只是塑形最终答案。</p>
<hr />
<h2><strong>7. 迭代强化学习：reward model 跟不上 policy 怎么办？</strong></h2>
<p>一个非常现实的问题：</p>
<blockquote>
<p>随着策略模型不断变强，旧的奖励模型可能无法有效监督新的策略分布。</p>
</blockquote>
<p>于是 DeepSeekMath 引入了<strong>迭代 RL + GRPO</strong>：</p>
<ul>
<li>每次迭代，用当前策略生成数据</li>
<li>构建新的奖励模型训练集</li>
<li>用 replay 机制持续训练奖励模型（历史数据占比约 10%）</li>
</ul>
<p>这本质是在做一件事：</p>
<blockquote>
<p>让 RM 跟上 policy 的分布漂移，否则 RL 只会“优化一个过时的打分器”。</p>
</blockquote>
<p>这也是很多 RLHF 工程里不可避免的循环：<strong>policy 变强 → 分布变了 → RM 失效 → 必须续训 RM</strong>。</p>
<hr />
<h2><strong>8. DeepSeekMath 的一个关键发现：RL 为什么有效？</strong></h2>
<p>DeepSeekMath 对比了 SFT 与 RL 后模型的 <strong>Pass@K</strong> 与 <strong>Maj@K</strong>：</p>
<ul>
<li>RL <strong>显著提升 Maj@K</strong></li>
<li>RL <strong>没有提升 Pass@K</strong></li>
</ul>
<p>这说明：</p>
<blockquote>
<p>RL 的主要作用不是把“TopK 里的上限”推高（能力上限不变），</p>
</blockquote>
<blockquote>
<p>而是让输出分布更稳健，让正确答案更容易成为“多数/常见输出”。</p>
</blockquote>
<p>一句话翻译成大白话：</p>
<p><strong>RL 更像是在“调分布”，让你更常答对，而不是让你会更多。</strong></p>
<p>这也解释了为什么推理模型的 RL 往往让表现更“稳”、更“像会推理”。</p>
<hr />
<h2><strong>9. 统一范式：SFT / DPO / PPO / GRPO 本质上在优化什么？</strong></h2>
<p>所有方法的梯度可以写成一个统一形式，包含三个关键组成部分：</p>
<ol>
<li>数据源 D（训练数据从哪来）</li>
<li>奖励函数（训练信号来源，如 RM/verifier）</li>
<li>算法 A（如何把数据与奖励变成梯度系数）</li>
</ol>
<p>在这个范式下：</p>
<ul>
<li><strong>SFT</strong>：用人类挑选的数据监督学习</li>
<li><strong>RFT</strong>：对模型采样结果做过滤再监督</li>
<li><strong>DPO</strong>：对成对偏好做离线优化（偏监督）</li>
<li><strong>Online RFT</strong>：用实时 policy 采样 + 过滤</li>
<li><strong>PPO/GRPO</strong>：用实时 policy 采样 + 强化学习更新</li>
</ul>
<blockquote>
<p>方法很多，但核心变量其实就三个：数据从哪来、奖励怎么给、算法怎么用奖励更新模型。</p>
</blockquote>
<hr />
<h2><strong>10. GRPO vs PPO vs DPO：面试最容易问的三角对比</strong></h2>
<h3><strong>10.1 GRPO vs PPO</strong></h3>
<ul>
<li>PPO 的优势依赖 Critic / GAE</li>
<li>GRPO 用组内均值当 baseline，直接构造相对优势</li>
<li>GRPO 工程更简单、成本更低、稳定性更依赖 reward 排序质量</li>
</ul>
<p>一句话：</p>
<p><strong>PPO 学“我比期望好多少”，GRPO 学“我比同组其他答案好多少”。</strong></p>
<h3><strong>10.2 GRPO vs DPO</strong></h3>
<ul>
<li>DPO：离线偏好数据（chosen/rejected），偏监督学习</li>
<li>GRPO：在线采样 + reward 打分，是真 RL（但简化了 critic）</li>
</ul>
<p>一句话：</p>
<p><strong>DPO 用人类给的对比对；GRPO 用模型自己采样的一组答案做对比。</strong></p>
<hr />
<h2><strong>11. GRPO 特别适合哪些任务？</strong></h2>
<p>推理模型偏爱 GRPO 的原因，是它天然适合“可自动打分”的任务：</p>
<ul>
<li>数学（verifier 判对错）</li>
<li>代码（单测/编译/静态分析）</li>
<li>逻辑推理（规则检查）</li>
<li>任何能构建稳定 reward 的任务</li>
</ul>
<p>当 reward 足够可靠时，组内排序会非常稳定，GRPO 的优势就会被放大。</p>
<hr />
<h2><strong>12. 局限与工程注意点</strong></h2>
<p>GRPO 也不是银弹，也有风险：</p>
<ul>
<li>需要同 prompt 多采样（group size G）→ 成本来自采样</li>
<li>group 太小 → 方差大，排序不稳定</li>
<li>reward 噪声大 → 相对优势抖动，训练不稳</li>
</ul>
<p>工程上通常会做：</p>
<ul>
<li>合理的 G（group size）选择</li>
<li>KL 正则强度控制</li>
<li>clip 稳定训练</li>
<li>更可靠的 verifier / PRM 构建与迭代</li>
</ul>
<hr />
<h1><strong>总结：为什么 DeepSeek-R1/推理模型会偏爱 GRPO？</strong></h1>
<blockquote>
<p><strong>GRPO 通过“同 prompt 组内比较”构造相对优势，去掉 critic，极大降低 RL 对齐的工程复杂度与训练成本，并在可验证推理任务上更稳定、更有效。</strong></p>
</blockquote>
<blockquote>
<p>DeepSeekMath 的证据表明，RL 的提升往往来自“让正确答案更常出现”（Maj@K ↑），而不是把能力上限推高（Pass@K 不变）。</p>
</blockquote>
<blockquote>
<p>这也解释了为什么 GRPO 这类稳定分布的 RL 方法，在推理模型时代特别吃香。</p>
</blockquote>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/4"/><category term="PPO"/><published>2025-12-13T07:30:41+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/3</id><title>QLoRA 深度解析：用 4bit 量化，把 70B 大模型拉进单卡时代</title><updated>2025-12-13T16:26:31.638482+00:00</updated><content type="html"><![CDATA[<h2><strong>前言：QLoRA 为什么值得被认真理解？</strong></h2>
<p>QLoRA 的作者 <strong>Tim Dettmers</strong>，是模型量化领域的核心人物之一，同时也深度参与了 <strong>BLOOM</strong> 等大模型的工程化落地。</p>
<p>模型量化（Quantization）和参数高效微调（PEFT）看似属于两个方向，但它们背后有一个共同目标：</p>
<blockquote>
<p><strong>让大模型的训练与推理更快、更省、更可落地。</strong></p>
</blockquote>
<p>QLoRA 正是将“<strong>量化</strong>”与“<strong>LoRA 微调</strong>”这两条路线真正融合到了一起，首次实现了：</p>
<blockquote>
<p><strong>在 4bit 量化模型上稳定训练 LoRA，且效果几乎不损失。</strong></p>
</blockquote>
<p>这让 <strong>70B 级别模型的单卡微调</strong> 成为现实。</p>
<hr />
<h2><strong>一、为什么需要 QLoRA？</strong></h2>
<h3><strong>1. 传统微调的显存瓶颈</strong></h3>
<table>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>显存需求</strong></th>
<th><strong>问题</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>全参数微调</td>
<td>极高（65B &gt;300GB）</td>
<td>完全不可落地</td>
</tr>
<tr>
<td>LoRA（FP16）</td>
<td>较高（65B &gt;120GB）</td>
<td>模型仍需 FP16 加载</td>
</tr>
<tr>
<td>GPTQ / AWQ</td>
<td>低</td>
<td>只能推理，难以训练</td>
</tr>
</tbody></table><p>结论非常明确：</p>
<blockquote>
<p><strong>我们需要一种：既能量化、又能训练的方案。</strong></p>
</blockquote>
<p>QLoRA 就是这个答案。</p>
<hr />
<h2><strong>二、QLoRA 的一句话定义</strong></h2>
<blockquote>
<p><strong>QLoRA（Quantized LoRA）</strong></p>
</blockquote>
<blockquote>
<p>是一种在 <strong>4bit 量化模型上冻结主权重，仅训练 LoRA 低秩参数</strong> 的高效微调方法，在显存降低约 75% 的同时，训练效果接近甚至优于全参微调。</p>
</blockquote>
<hr />
<h2><strong>三、QLoRA 的整体技术框架</strong></h2>
<p>QLoRA 并不是单一技术，而是<strong>四项关键工程与算法设计的组合</strong>：</p>
<ol>
<li><strong>NF4（4bit Normal Float）量化</strong></li>
<li><strong>分块 + 分位数量化</strong></li>
<li><strong>双重量化（Double Quantization）</strong></li>
<li><strong>分页优化（Paged Optimizer / Paged Attention）</strong></li>
<li><strong>LoRA Adapter 训练</strong></li>
</ol>
<p>下面逐一拆解。</p>
<hr />
<h2><strong>四、模型量化基础（理解 QLoRA 的前提）</strong></h2>
<h3><strong>4.1 什么是模型量化？</strong></h3>
<p>模型量化的目标是：</p>
<blockquote>
<p><strong>用更低精度表示权重，在可接受的误差范围内减少模型体积和显存占用。</strong></p>
</blockquote>
<p>本质上，这是一个<strong>有损压缩问题</strong>。</p>
<p>常见量化方式包括：</p>
<ul>
<li>线性量化（absmax）</li>
<li>零点量化（zero-point）</li>
<li>非线性量化（分位数量化）</li>
</ul>
<hr />
<h3><strong>4.2 分位数量化（Quantile Quantization）</strong></h3>
<p>QLoRA 的关键观察是：</p>
<blockquote>
<p><strong>大模型权重通常近似服从正态分布。</strong></p>
</blockquote>
<p>因此，与其用线性区间均匀切分，不如：</p>
<ul>
<li>按 <strong>CDF 等概率切分</strong></li>
<li>让每个量化 bin 出现频率尽可能一致</li>
</ul>
<p>这就是 <strong>分位数量化</strong> 的核心思想。</p>
<hr />
<h2><strong>五、NF4：QLoRA 能在 4bit 下不掉点的根本原因</strong></h2>
<h3><strong>5.1 一句话理解 NF4</strong></h3>
<blockquote>
<p><strong>NF4 是一种专为大模型权重分布设计的 4bit 非均匀量化格式，通过将有限的 4bit 表示能力集中在高概率区域，从而在极低精度下最大化表达能力。</strong></p>
</blockquote>
<p>它不是“更激进的 INT4”，而是<strong>完全不同的量化思路</strong>。</p>
<hr />
<h3><strong>5.2 为什么传统 INT4 不适合大模型？</strong></h3>
<p>INT4 采用的是<strong>均匀量化</strong>：</p>
<ul>
<li>4bit → 16 个等距离散值</li>
<li>默认假设数值分布是均匀的</li>
</ul>
<p>但大模型（LLM）的权重分布具有非常明确的统计特征：</p>
<ul>
<li>近似 <strong>零均值</strong></li>
<li>近似 <strong>正态分布</strong></li>
<li><strong>大量权重集中在 0 附近</strong></li>
<li>极端大值数量极少，但幅度大</li>
</ul>
<p>这会直接导致 INT4 的问题：</p>
<ul>
<li>中心区域（最重要的权重区间）精度不足</li>
<li>量化桶大量浪费在几乎不会出现的区间</li>
<li>少量误差被放大为性能下降</li>
</ul>
<p><strong>换句话说：INT4 在“统计意义上”并不匹配 LLM。</strong></p>
<hr />
<h3><strong>5.3 NF4 的核心思想：非均匀量化</strong></h3>
<p>NF4 的设计目标非常明确：</p>
<blockquote>
<p><strong>把有限的 16 个量化值，用在最“值得用”的地方。</strong></p>
</blockquote>
<p>具体来说：</p>
<ul>
<li>在 <strong>0 附近分布更密</strong></li>
<li>在 <strong>两端极值区域分布更稀</strong></li>
</ul>
<p>这使得：</p>
<ul>
<li>常见的小权重 → 高精度表示</li>
<li>罕见的大权重 → 容忍更大误差</li>
</ul>
<p>这种设计与 LLM 权重的真实分布高度匹配。</p>
<hr />
<h3><strong>5.4 NF4 是怎么实现的？</strong></h3>
<h4><strong>1️⃣ 预定义 16 个“浮点量化值”</strong></h4>
<p>NF4 并不是简单的整数映射，而是<strong>预先定义好的 16 个浮点值</strong>，这些值近似来自：</p>
<ul>
<li>标准正态分布的分位点（quantiles）</li>
<li>经过归一化与截断处理</li>
</ul>
<p>你可以把它理解为：</p>
<pre><code>[-a, ..., -small, 0, +small, ..., +a]
</code></pre>
<p>但它们<strong>不是等距的</strong>，而是统计意义上更合理的分布。</p>
<hr />
<h4><strong>2️⃣ 权重映射到最近的 NF4 值</strong></h4>
<p>量化过程本质上是一个最近邻映射：</p>
<p>$q = \arg\min_{v \in \text{NF4 values}} |w - v|$</p>
<p>反量化时：</p>
<p>$\hat{w} = s \cdot q$</p>
<p>其中：</p>
<ul>
<li>q：4bit index</li>
<li>s：FP16（或经双重量化后的）scale</li>
</ul>
<p><strong>scale 的精度非常关键</strong>，它决定了整个 block 的动态范围。</p>
<hr />
<h3><strong>5.5 为什么 NF4 比 INT4 精度高？</strong></h3>
<h4><strong>（1）分布匹配</strong></h4>
<ul>
<li>LLM 权重 ≈ 正态分布</li>
<li>NF4 量化点 ≈ 正态分布分位点</li>
</ul>
<p>→ 在统计意义上是近似最优的 4bit 表示。</p>
<hr />
<h4><strong>（2）误差集中在“不重要区域”</strong></h4>
<ul>
<li>小权重：数量多、贡献大 → 高精度</li>
<li>大权重：数量少 → 容忍误差</li>
</ul>
<p>整体误差被有效控制。</p>
<hr />
<h4><strong>（3）论文实验结论</strong></h4>
<p>QLoRA 原论文明确指出：</p>
<blockquote>
<p><strong>NF4 在 4bit 条件下的量化误差，接近 FP16，显著优于 INT4。</strong></p>
</blockquote>
<p>这正是 QLoRA 能在 4bit 下“几乎不掉点”的关键原因。</p>
<hr />
<h3><strong>5.6 NF4 在 QLoRA 中的角色定位</strong></h3>
<p>在 QLoRA 框架中，各部分分工非常清晰：</p>
<table>
<thead>
<tr>
<th><strong>组件</strong></th>
<th><strong>精度</strong></th>
<th><strong>是否训练</strong></th>
<th><strong>作用</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Base 权重</td>
<td>NF4</td>
<td>❌ 冻结</td>
<td>压缩模型、降低显存</td>
</tr>
<tr>
<td>Scale</td>
<td>FP16 / 双量化</td>
<td>❌</td>
<td>恢复动态范围</td>
</tr>
<tr>
<td>LoRA A/B</td>
<td>FP16 / BF16</td>
<td>✅</td>
<td>学习新任务、补偿误差</td>
</tr>
</tbody></table><p>可以这样理解：</p>
<blockquote>
<p><strong>NF4 负责“把模型装进显存”，</strong></p>
</blockquote>
<blockquote>
<p><strong>LoRA 负责“把能力补回来”。</strong></p>
</blockquote>
<hr />
<h3>5.7 NF4 vs INT4</h3>
<table>
<thead>
<tr>
<th><strong>对比项</strong></th>
<th><strong>INT4</strong></th>
<th><strong>NF4</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>量化方式</td>
<td>均匀</td>
<td>非均匀</td>
</tr>
<tr>
<td>感知权重分布</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>中心区域精度</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>是否适合 LLM</td>
<td>一般</td>
<td>非常适合</td>
</tr>
<tr>
<td>QLoRA 默认</td>
<td>❌</td>
<td>✅</td>
</tr>
</tbody></table><p>一句话总结：</p>
<blockquote>
<p><strong>INT4 是通用压缩方案，NF4 是为大模型定制的 4bit 浮点格式。</strong></p>
</blockquote>
<hr />
<h3><strong>5.8 一个常被追问的点</strong></h3>
<p><strong>NF4 是浮点还是整数？</strong></p>
<ul>
<li><strong>逻辑上</strong>：4bit 浮点格式</li>
<li><strong>实现上</strong>：4bit index + FP16 scale</li>
</ul>
<p>它不是传统 IEEE 浮点，而是<strong>为神经网络权重定制的表示形式</strong>。</p>
<hr />
<h2><strong>小结</strong></h2>
<p>如果没有 NF4：</p>
<ul>
<li>4bit 量化 ≈ 性能大幅下降</li>
<li>LoRA 无法完全补偿量化误差</li>
</ul>
<p>而正是 NF4 的存在，才让下面这件事成立：</p>
<blockquote>
<p><strong>在 4bit 条件下冻结主权重，只训练少量 LoRA 参数，依然可以达到接近全参微调的效果。</strong></p>
</blockquote>
<p>这也是为什么——</p>
<p><strong>几乎所有严肃的 QLoRA 实现，都会默认使用 NF4。</strong></p>
<hr />
<h2><strong>六、双重量化：进一步压缩显存</strong></h2>
<p>在 4bit 量化中：</p>
<ul>
<li>权重 → 4bit</li>
<li>量化常数 c → FP32（显存杀手）</li>
</ul>
<p>QLoRA 的解决方案是：</p>
<blockquote>
<p><strong>对量化常数本身再做一次 8bit 量化。</strong></p>
</blockquote>
<p>即：</p>
<ul>
<li>第一层：权重量化</li>
<li>第二层：量化常数量化</li>
</ul>
<p>结果：</p>
<ul>
<li>量化常数显存占用从 <strong><del>1.6% → </del>0.37%</strong></li>
<li>几乎没有额外精度损失</li>
</ul>
<hr />
<h2><strong>七、Paged Optimizer：避免训练 OOM 的关键工程技巧</strong></h2>
<p>即使权重量化，训练中仍存在一个风险：</p>
<blockquote>
<p><strong>显存峰值（activation + gradient checkpoint）导致 OOM。</strong></p>
</blockquote>
<p>QLoRA 引入 <strong>分页优化（Paged Optimizer / Paged Attention）</strong>：</p>
<ul>
<li>将部分 KV cache / 激活页换出到 CPU 内存</li>
<li>类似操作系统的虚拟内存分页机制</li>
<li>在显存紧张时动态调度</li>
</ul>
<p>这使得：</p>
<blockquote>
<p><strong>13B–70B 模型在单卡训练成为可能。</strong></p>
</blockquote>
<hr />
<h2><strong>八、QLoRA 如何与 LoRA 结合？</strong></h2>
<p>核心公式仍然是 LoRA 的形式：</p>
<p>$W' = W + \Delta W,\quad \Delta W = BA$</p>
<p>区别在于：</p>
<ul>
<li>W：4bit NF4 量化权重（冻结）</li>
<li>A, B：FP16 / BF16 LoRA 参数（可训练）</li>
</ul>
<p>训练时：</p>
<ul>
<li>主权重仅反量化参与前向</li>
<li><strong>梯度只回传到 LoRA</strong></li>
</ul>
<p>这也是 QLoRA 能“量化 + 训练”共存的根本原因。</p>
<hr />
<h2><strong>九、QLoRA 的能力边界</strong></h2>
<h3><strong>支持的任务</strong></h3>
<table>
<thead>
<tr>
<th><strong>能力</strong></th>
<th><strong>是否支持</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>SFT</td>
<td>✅</td>
</tr>
<tr>
<td>DPO / ORPO</td>
<td>✅</td>
</tr>
<tr>
<td>多轮对话</td>
<td>✅</td>
</tr>
<tr>
<td>单卡 70B</td>
<td>✅</td>
</tr>
</tbody></table><hr />
<h3><strong>不适合的场景</strong></h3>
<ul>
<li>模型结构级改动（attention 重构）</li>
<li>LoRA rank 太小（易欠拟合）</li>
<li>极端高精度任务（数学 / 代码）</li>
</ul>
<hr />
<h2><strong>十、总结：QLoRA 的真正价值</strong></h2>
<p>QLoRA 的本质不是“LoRA 的改进”，而是：</p>
<blockquote>
<p><strong>把大模型训练，从“算力竞赛”拉回到“工程理性”。</strong></p>
</blockquote>
<p>它证明了一件事：</p>
<ul>
<li>量化 ≠ 只能推理</li>
<li>低精度 ≠ 低性能</li>
</ul>
<p>在今天，QLoRA 已经成为：</p>
<blockquote>
<p><strong>企业 SFT / DPO / 垂域微调的事实标准方案。</strong></p>
</blockquote>
<hr />
<h2><strong>写在最后</strong></h2>
<p>这篇文章既是对 QLoRA 的系统性梳理，也是一种个人补课记录。</p>
<p>如果其中存在理解偏差，欢迎讨论与指正。</p>
<hr />
<h3><strong>参考</strong></h3>
<ul>
<li>[1] QLoRA: Efficient Finetuning of Quantized LLMs</li>
<li>[2] LoRA: Low-Rank Adaptation of Large Language Models</li>
</ul>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/3"/><category term="LoRA"/><published>2025-12-13T06:57:38+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/2</id><title>从 Function Calling 到 MCP：大模型如何真正接入真实世界？</title><updated>2025-12-13T16:26:31.932884+00:00</updated><content type="html"><![CDATA[<blockquote>
<p>当我们谈论 AI Agent、Copilot、企业级智能系统时，真正的分水岭早已不在模型参数量，而在一个更底层的问题上：</p>
</blockquote>
<blockquote>
</blockquote>
<blockquote>
<p><strong>大模型，究竟是“会说话的百科全书”，还是“能动手做事的系统”？</strong></p>
</blockquote>
<p>Function Calling 和 MCP（Model Context Protocol），正是围绕这个问题展开的两代关键答案。</p>
<hr />
<h2><strong>一、没有工具的大模型，本质上是“被困在房间里的人”</strong></h2>
<p>早期的大语言模型，即便知识极其丰富，本质上仍然是一个<strong>封闭系统</strong>：</p>
<ul>
<li>无法访问实时数据</li>
<li>不能查询数据库</li>
<li>不能操作文件系统</li>
<li>不能调用外部 API</li>
</ul>
<p>它只能基于<strong>训练时学到的静态知识</strong>进行回答。</p>
<p>这在“解释概念”“生成文本”时问题不大，但一旦进入真实应用场景，就会立刻失效：</p>
<ul>
<li>查最新数据</li>
<li>进行精确计算</li>
<li>与业务系统交互</li>
<li>执行确定性任务</li>
</ul>
<p><strong>模型需要“手”和“工具”。</strong></p>
<hr />
<h2><strong>二、Function Calling：第一次让模型“会用工具”</strong></h2>
<h3><strong>1. Function Calling 是什么？</strong></h3>
<p>Function Calling 是 OpenAI 在 2023 年提出的一种能力扩展机制，其核心目标只有一个：</p>
<blockquote>
<p><strong>让模型在推理过程中，决定是否调用外部函数，并生成结构化参数。</strong></p>
</blockquote>
<p>需要特别强调的是：</p>
<ul>
<li>模型 <strong>不执行函数</strong></li>
<li>模型 <strong>只输出调用意图与参数</strong></li>
<li>真正的执行发生在宿主程序（Backend / Server）中</li>
</ul>
<p>换句话说，模型负责“想”，系统负责“做”。</p>
<hr />
<h3><strong>2. 典型工作流程</strong></h3>
<pre><code>用户输入
↓
LLM 推理
↓
生成 function_name + arguments（JSON）
↓
宿主系统执行函数
↓
结果返回给 LLM
↓
最终自然语言回答
</code></pre>
<p>示例（简化）：</p>
<pre><code>{
  &quot;name&quot;: &quot;get_weather&quot;,
  &quot;arguments&quot;: {
    &quot;city&quot;: &quot;北京&quot;,
    &quot;date&quot;: &quot;今天&quot;
  }
}
</code></pre>
<hr />
<h3><strong>3. 为什么说 Function Calling 是一次重要跃迁？</strong></h3>
<p>因为它第一次让大模型具备了：</p>
<ul>
<li>与真实系统交互的能力</li>
<li>执行确定性任务的可能性</li>
<li>构建 Agent 的基础动作接口</li>
</ul>
<p>我们今天在 Coze、Dify 等低代码 Agent 平台中看到的各种“插件”，本质上都是 <strong>Function Calling 的工程封装</strong>。</p>
<hr />
<h2><strong>三、Function Calling 的工程代价与瓶颈</strong></h2>
<p>Function Calling 虽然重要，但它从一开始就<strong>不是为大规模系统设计的标准</strong>。</p>
<h3><strong>1. 模型强依赖</strong></h3>
<ul>
<li>模型必须 <strong>原生支持 Function Calling</strong></li>
<li>甚至需要 <strong>专门的 Function Call 微调</strong></li>
</ul>
<p>例如在 ShareGPT 风格的数据集中，会出现专门的结构字段：</p>
<pre><code>{
  &quot;from&quot;: &quot;function_call&quot;,
  &quot;value&quot;: &quot;工具参数&quot;
}
</code></pre>
<p>这意味着：</p>
<blockquote>
<p><strong>不是所有模型天然就“会用工具”。</strong></p>
</blockquote>
<hr />
<h3><strong>2. 协议碎片化问题</strong></h3>
<p>不同模型、不同厂商的 Function Calling：</p>
<ul>
<li>Schema 不统一</li>
<li>调用格式不同</li>
<li>错误处理方式各异</li>
</ul>
<p>工程上，几乎不可避免地需要为每个模型维护一套适配层。</p>
<hr />
<h3><strong>3. 能力边界清晰但狭窄</strong></h3>
<p>Function Calling 更像是：</p>
<ul>
<li>单次</li>
<li>无状态</li>
<li>函数级别</li>
</ul>
<p>它非常适合：</p>
<ul>
<li>查天气</li>
<li>查订单</li>
<li>简单计算</li>
</ul>
<p>但一旦进入：</p>
<ul>
<li>多工具协作</li>
<li>状态管理</li>
<li>复杂资源访问</li>
</ul>
<p>就会迅速变得笨重。</p>
<hr />
<h2><strong>四、MCP 出现的背景：Function Calling 不够用了</strong></h2>
<h3><strong>1. MCP 是什么？</strong></h3>
<p><strong>MCP（Model Context Protocol）</strong> 是 Anthropic 提出的一种开放标准协议，其目标不是“再发明一种函数调用”，而是：</p>
<blockquote>
<p><strong>为大模型与外部世界之间，建立一个标准化、可控、可扩展的通信层。</strong></p>
</blockquote>
<p>一句话定义：</p>
<blockquote>
<p>MCP 是模型接入工具、数据和能力的“通用协议层”。</p>
</blockquote>
<hr />
<h3><strong>2. MCP 要解决的核心问题</strong></h3>
<p>Function Calling 无法优雅解决的问题包括：</p>
<ul>
<li>工具数量爆炸</li>
<li>数据源类型复杂（DB / 文件 / API / Git）</li>
<li>不同语言、不同部署环境</li>
<li>权限与安全边界不清晰</li>
<li>状态与上下文难以管理</li>
</ul>
<p>MCP 的目标非常明确：</p>
<blockquote>
<p><strong>把“外部能力”系统性地包装成模型可理解、可控制的上下文接口。</strong></p>
</blockquote>
<hr />
<h2><strong>五、MCP 的标准架构（关键）</strong></h2>
<p>MCP 采用典型的 <strong>Client–Server 架构</strong>：</p>
<pre><code>LLM（Client）
   ↕ MCP Protocol
MCP Server
   ├─ Tools
   ├─ Resources
   └─ Prompts
</code></pre>
<h3><strong>MCP Server 提供三类能力</strong></h3>
<h4><strong>1️⃣ Tools（可执行能力）</strong></h4>
<ul>
<li>标准化工具接口</li>
<li>明确输入 / 输出 schema</li>
<li>受控权限</li>
</ul>
<h4><strong>2️⃣ Resources（数据资源）</strong></h4>
<ul>
<li>文件系统</li>
<li>数据库</li>
<li>API 结果</li>
<li>Git 仓库</li>
<li>本地或远程状态</li>
</ul>
<p>模型可以“读资源”，而不是每次都靠 prompt 注入。</p>
<h4><strong>3️⃣ Prompts（上下文模板）</strong></h4>
<ul>
<li>系统级规范</li>
<li>业务约束</li>
<li>任务模板</li>
</ul>
<hr />
<h2><strong>六、Function Calling vs MCP：本质对比</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>Function Calling</strong></th>
<th><strong>MCP</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>抽象层级</td>
<td>函数级</td>
<td>系统级</td>
</tr>
<tr>
<td>本质</td>
<td>模型生成调用参数</td>
<td>模型接入外部世界的协议</td>
</tr>
<tr>
<td>状态管理</td>
<td>无</td>
<td>支持（Server 维护）</td>
</tr>
<tr>
<td>工具规模</td>
<td>少量</td>
<td>大规模</td>
</tr>
<tr>
<td>协议标准</td>
<td>私有实现</td>
<td>开放标准</td>
</tr>
<tr>
<td>扩展性</td>
<td>差</td>
<td>极强</td>
</tr>
<tr>
<td>适用场景</td>
<td>Demo / 小工具</td>
<td>Agent / 企业系统</td>
</tr>
</tbody></table><hr />
<h2><strong>七、一个非常直观的类比</strong></h2>
<ul>
<li>
<p><strong>Function Calling</strong>：</p>
<p>像是你打电话让秘书临时帮你办一件事</p>
</li>
<li>
<p><strong>MCP</strong>：</p>
<p>像是你被授予了一整套办公系统的受控访问权限</p>
</li>
</ul>
<p>前者是“临时动作”，后者是“长期协作”。</p>
<hr />
<h2><strong>八、工程选型建议（非常重要）</strong></h2>
<h3><strong>什么时候用 Function Calling？</strong></h3>
<ul>
<li>工具数量少（&lt;10）</li>
<li>功能简单</li>
<li>单体应用</li>
<li>快速验证</li>
</ul>
<p>👉 <strong>90% 的聊天机器人 Demo</strong></p>
<hr />
<h3><strong>什么时候必须用 MCP？</strong></h3>
<ul>
<li>多工具、多数据源</li>
<li>AI Agent / Copilot</li>
<li>企业级系统</li>
<li>本地文件 / 内网服务</li>
<li>强安全与可扩展要求</li>
</ul>
<p>👉 <strong>真正可落地的 Agent 系统</strong></p>
<hr />
<h2><strong>九、总结一句话</strong></h2>
<blockquote>
<p>Function Calling 解决的是“模型怎么调用一个函数”，</p>
</blockquote>
<blockquote>
<p>MCP 解决的是“模型如何安全、可扩展地接入真实世界”。</p>
</blockquote>
<p>在 Agent 体系中：</p>
<ul>
<li>Function Calling 更像是“动作接口”</li>
<li>MCP 更接近“模型的操作系统层”</li>
</ul>
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/2"/><category term="MCP"/><published>2025-12-13T06:24:38+00:00</published></entry><entry><id>https://github.com/lihe/MyGitBlog/issues/1</id><title>从 RAG 到 GraphRAG，再到 LightRAG</title><updated>2025-12-13T16:26:32.199116+00:00</updated><content type="html"><![CDATA[<h2><strong>引言：当 RAG 不再只是“找得到”</strong></h2>
<p>检索增强生成（Retrieval-Augmented Generation, RAG）已经成为 LLM 应用的事实标准。</p>
<p>它通过“<strong>先检索、再生成</strong>”的方式，在一定程度上缓解了大模型的幻觉问题。</p>
<p>但随着应用场景从<strong>局部事实问答</strong>走向<strong>跨文档推理、全局总结与复杂关系理解</strong>，一个问题逐渐显现：</p>
<blockquote>
<p><strong>RAG 能检索到信息，但并不真正“理解结构”。</strong></p>
</blockquote>
<p>GraphRAG 与 LightRAG，正是在这一背景下被提出的两种结构化检索方案。</p>
<hr />
<h2><strong>一、传统 RAG 的结构性瓶颈</strong></h2>
<p>传统 RAG 的系统抽象非常清晰：</p>
<ul>
<li>文档 → Chunk</li>
<li>Chunk → 向量</li>
<li>Query → 相似度检索 → 上下文拼接</li>
</ul>
<p>这种设计在以下任务中表现良好：</p>
<ul>
<li>明确事实查询</li>
<li>定位具体段落</li>
<li>局部知识补全</li>
</ul>
<p>但它存在一个<strong>无法通过调参或模型升级解决的系统性限制</strong>：</p>
<blockquote>
<p><strong>Chunk 是扁平的，而知识本身是结构化的。</strong></p>
</blockquote>
<h3><strong>结构缺失带来的直接后果</strong></h3>
<ul>
<li>文档之间的关联关系被切断</li>
<li>实体之间的因果、从属、演化关系无法显式建模</li>
<li>模型只能“拼上下文”，而非“基于结构推理”</li>
</ul>
<p>这意味着，当问题本身涉及<strong>整体脉络、跨文档关系或宏观总结</strong>时，传统 RAG 的能力上限会迅速暴露。</p>
<hr />
<h2><strong>二、GraphRAG 是什么：第一次把“图结构”带入 RAG</strong></h2>
<p><strong>GraphRAG</strong> 是微软提出的一种 RAG 扩展框架，其核心目标并不是“检索更准的文本”，</p>
<p>而是：</p>
<blockquote>
<p><strong>让 RAG 具备显式建模和利用“知识结构”的能力。</strong></p>
</blockquote>
<p>在 GraphRAG 中，知识不再被视为一堆独立的文本片段，而是被重构为一个<strong>由实体与关系组成的图结构（Knowledge Graph）</strong>。</p>
<p>RAG 的检索对象，也从“文本 Chunk”升级为“图中的结构单元”。</p>
<hr />
<h2><strong>三、GraphRAG：显式引入“全局结构”的系统设计</strong></h2>
<h3><strong>1. GraphRAG 的核心设计思路</strong></h3>
<p>GraphRAG 的关键思想可以概括为一句话：</p>
<blockquote>
<p><strong>既然知识天然是图结构的，那就直接构建图，并在图的层面进行检索与摘要。</strong></p>
</blockquote>
<p>为此，GraphRAG 在 indexing 阶段引入了一套完整但代价不菲的流程：</p>
<ol>
<li>
<p><strong>实体与关系抽取</strong></p>
<p>使用 LLM 从原始文本中抽取实体（人物、概念、事件）及其关系。</p>
</li>
<li>
<p><strong>知识图谱构建</strong></p>
<p>将实体作为节点，关系作为边，形成全局图结构。</p>
</li>
<li>
<p><strong>社区发现（Community Detection）</strong></p>
<p>使用 Leiden 等图算法，将高度相关的子图聚合为“社区”。</p>
</li>
<li>
<p><strong>社区级摘要生成</strong></p>
<p>为每个社区预先生成结构化摘要，作为后续检索的高层语义单元。</p>
</li>
</ol>
<hr />
<h3><strong>2. GraphRAG 解决了什么本质问题？</strong></h3>
<p>GraphRAG 首次在 RAG 框架中引入了<strong>全局结构感知能力</strong>：</p>
<ul>
<li>不再只关注“相关段落”</li>
<li>而是能够回答“整体结构上发生了什么”</li>
</ul>
<p>例如：</p>
<ul>
<li>“整个语料中反复出现的核心主题是什么？”</li>
<li>“多个事件之间是否存在系统性关联？”</li>
</ul>
<p>这类问题，本质上需要<strong>跨文本、跨实体、跨关系的全局推理</strong>，</p>
<p>而这正是 GraphRAG 的优势所在。</p>
<hr />
<h3><strong>3. 但系统代价同样明确</strong></h3>
<p>从工程角度看，GraphRAG 的主要问题并不在效果，而在<strong>系统可持续性</strong>：</p>
<ul>
<li>索引阶段 Token 成本极高</li>
<li>社区结构对数据变化高度敏感</li>
<li>增量更新几乎不可行</li>
</ul>
<p>因此，GraphRAG 更适合<strong>离线分析型任务</strong>，而非高频更新的在线系统。</p>
<hr />
<h2><strong>四、LightRAG 是什么：对 GraphRAG 的工程化回应</strong></h2>
<p><strong>LightRAG</strong> 并不是对 GraphRAG 的否定，而是一种<strong>面向工程现实的重新设计</strong>。</p>
<p>它试图回答的问题是：</p>
<blockquote>
<p><strong>是否一定要构建“全局静态结构”，才能让 RAG 具备结构化推理能力？</strong></p>
</blockquote>
<p>LightRAG 的答案是：<strong>不一定。</strong></p>
<hr />
<h2><strong>五、LightRAG：把结构留到 Query 时再用</strong></h2>
<h3><strong>1. 设计立场的根本转变</strong></h3>
<p>与 GraphRAG 相比，LightRAG 的核心转变在于：</p>
<ul>
<li>不再预生成全局社区摘要</li>
<li>保留图结构，但不“冻结”结构语义</li>
<li>将结构的使用推迟到 Query 阶段</li>
</ul>
<p>可以将两者的差异理解为：</p>
<blockquote>
<p>GraphRAG：<strong>结构是提前计算好的</strong></p>
</blockquote>
<blockquote>
<p>LightRAG：<strong>结构是按需被查询和展开的</strong></p>
</blockquote>
<hr />
<h3><strong>2. 双层检索的工程意义</strong></h3>
<p>LightRAG 采用了一种<strong>图结构 + 向量检索的协同机制</strong>：</p>
<ul>
<li><strong>向量层</strong>：快速召回相关实体或节点</li>
<li><strong>图结构层</strong>：基于关系进行上下文扩展与约束</li>
</ul>
<p>这种设计带来的直接好处是：</p>
<ul>
<li>支持增量更新</li>
<li>检索路径随 Query 动态变化</li>
<li>成本与延迟显著降低</li>
</ul>
<hr />
<h2><strong>六、三种 RAG 方案的系统级对比</strong></h2>
<table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>传统 RAG</strong></th>
<th><strong>GraphRAG</strong></th>
<th><strong>LightRAG</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>知识建模</td>
<td>扁平文本</td>
<td>显式全局图</td>
<td>局部图 + 向量</td>
</tr>
<tr>
<td>全局理解</td>
<td>❌</td>
<td>✅</td>
<td>部分支持</td>
</tr>
<tr>
<td>索引成本</td>
<td>低</td>
<td>极高</td>
<td>中等</td>
</tr>
<tr>
<td>增量更新</td>
<td>易</td>
<td>困难</td>
<td>易</td>
</tr>
<tr>
<td>工程可用性</td>
<td>高</td>
<td>低</td>
<td>高</td>
</tr>
</tbody></table><hr />
<h2><strong>七、结论：Graph 不是目的，结构化推理才是</strong></h2>
<p>GraphRAG 与 LightRAG 的分歧，并不在“用不用图”，而在于：</p>
<blockquote>
<p><strong>结构化能力应当放在“离线建模”还是“在线推理”阶段。</strong></p>
</blockquote>
<ul>
<li>GraphRAG 代表<strong>认知完整性优先</strong></li>
<li>LightRAG 代表<strong>工程可落地性优先</strong></li>
</ul>
<p>在可预见的未来，更可能的方向是：</p>
<blockquote>
<p><strong>图结构 × 向量检索 × 动态推理路径的融合系统</strong></p>
</blockquote>
<hr />
]]></content><link href="https://github.com/lihe/MyGitBlog/issues/1"/><category term="RAG"/><published>2025-12-13T05:46:09+00:00</published></entry></feed>